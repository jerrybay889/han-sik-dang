{"file_contents":{"data-hub/src/workflows/sync.py":{"content":"\"\"\"\nSync workflow to í•œì‹ë‹¹\nì •ì œëœ ë°ì´í„°ë¥¼ í•œì‹ë‹¹ External APIë¡œ ì „ì†¡\n\"\"\"\nimport asyncio\nimport httpx\nimport uuid\nfrom datetime import datetime\nfrom typing import List, Dict, Any\nfrom loguru import logger\n\nfrom src.database.connection import db_session\nfrom src.database.models import ProcessedRestaurant, SyncLog\nfrom config import settings\n\n\nclass SyncWorkflow:\n    \"\"\"í•œì‹ë‹¹ ë™ê¸°í™” ì›Œí¬í”Œë¡œìš°\"\"\"\n    \n    def __init__(self):\n        self.api_url = settings.hansikdang_api_url\n        self.api_key = settings.data_collection_api_key\n        self.batch_size = settings.batch_size\n        self.logger = logger.bind(workflow=\"sync\")\n    \n    async def sync_to_hansikdang(self):\n        \"\"\"í•œì‹ë‹¹ìœ¼ë¡œ ë°ì´í„° ë™ê¸°í™”\"\"\"\n        self.logger.info(\"Starting sync to í•œì‹ë‹¹\")\n        \n        with db_session() as db:\n            # ë™ê¸°í™” ëŒ€ê¸° ì¤‘ì¸ ë ˆìŠ¤í† ë‘ (í’ˆì§ˆ ì ìˆ˜ 70 ì´ìƒ)\n            restaurants = db.query(ProcessedRestaurant).filter(\n                ProcessedRestaurant.sync_status == 'pending',\n                ProcessedRestaurant.quality_score >= settings.quality_threshold\n            ).limit(self.batch_size).all()\n            \n            if not restaurants:\n                self.logger.info(\"No restaurants to sync\")\n                return\n            \n            # ë™ê¸°í™” ë¡œê·¸ ìƒì„±\n            log_id = str(uuid.uuid4())\n            log = SyncLog(\n                id=log_id,\n                status='running',\n                total_sent=len(restaurants)\n            )\n            db.add(log)\n            db.commit()\n            \n            try:\n                # í•œì‹ë‹¹ External APIë¡œ ì „ì†¡\n                restaurant_list = [\n                    self._format_for_hansikdang(r)\n                    for r in restaurants\n                ]\n                \n                payload = {\"restaurants\": restaurant_list}\n                \n                async with httpx.AsyncClient() as client:\n                    response = await client.post(\n                        f\"{self.api_url}/api/external/restaurants\",\n                        json=payload,\n                        headers={\n                            \"X-API-Key\": self.api_key,\n                            \"Content-Type\": \"application/json\"\n                        },\n                        timeout=60.0\n                    )\n                    \n                    if response.status_code != 200:\n                        self.logger.error(f\"Sync failed with status {response.status_code}: {response.text}\")\n                    \n                    response.raise_for_status()\n                    result = response.json()\n                \n                # ì„±ê³µí•œ ë ˆìŠ¤í† ë‘ ì—…ë°ì´íŠ¸\n                success_count = result.get(\"success\", 0)  # ë©”ì¸ ì•± ì‘ë‹µ í˜•ì‹: {\"success\": N, \"failed\": M}\n                \n                for restaurant in restaurants:\n                    restaurant.sync_status = 'synced'\n                    restaurant.synced_to_hansikdang = True\n                    restaurant.synced_at = datetime.now()\n                \n                # ë¡œê·¸ ì—…ë°ì´íŠ¸\n                log.completed_at = datetime.now()\n                log.status = 'completed'\n                log.success_count = success_count\n                log.error_count = len(restaurants) - success_count\n                \n                db.commit()\n                \n                self.logger.info(f\"Synced {success_count}/{len(restaurants)} restaurants\")\n                \n            except Exception as e:\n                self.logger.error(f\"Sync failed: {e}\")\n                log.status = 'failed'\n                log.error_details = {\"error\": str(e)}\n                db.commit()\n    \n    def _format_for_hansikdang(self, restaurant: ProcessedRestaurant) -> Dict[str, Any]:\n        \"\"\"í•œì‹ë‹¹ External API í˜•ì‹ìœ¼ë¡œ ë³€í™˜\"\"\"\n        # priceRange ìˆ«ì ë³€í™˜ (ë¬¸ìì—´ â†’ ìˆ«ì)\n        price_range_map = {\n            \"ì €ë ´\": 1,\n            \"ë³´í†µ\": 2,\n            \"ë¹„ìŒˆ\": 3,\n            \"ë§¤ìš°ë¹„ìŒˆ\": 4,\n            \"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4\n        }\n        price_range_value = price_range_map.get(str(restaurant.price_range), 2)\n        \n        return {\n            \"name\": restaurant.name,\n            \"nameEn\": restaurant.name_en,\n            \"category\": restaurant.category,\n            \"cuisine\": restaurant.cuisine,\n            \"district\": restaurant.district,\n            \"address\": restaurant.address,\n            \"description\": restaurant.description,\n            \"descriptionEn\": restaurant.description_en,\n            \"priceRange\": price_range_value,\n            \"imageUrl\": restaurant.image_url or \"https://via.placeholder.com/400x300?text=Restaurant\",\n            \"openHours\": restaurant.open_hours or \"ì •ë³´ ì—†ìŒ\",\n            \"phone\": restaurant.phone,\n            \"latitude\": restaurant.latitude,\n            \"longitude\": restaurant.longitude,\n        }\n\n\nasync def main():\n    \"\"\"ë™ê¸°í™” ì‹¤í–‰\"\"\"\n    workflow = SyncWorkflow()\n    await workflow.sync_to_hansikdang()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n","size_bytes":5111},"data-hub/start_server.sh":{"content":"#!/bin/bash\n# Data Hub API Server Startup Script\n\necho \"ğŸš€ Starting Restaurant Data Hub API Server...\"\necho \"\"\n\ncd \"$(dirname \"$0\")\"\n\n# í™˜ê²½ ë³€ìˆ˜ í™•ì¸\nif [ ! -f .env ]; then\n    echo \"âš ï¸  .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\"\n    cp .env.example .env\n    echo \"âœ… .env íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\"\nfi\n\n# DB ì´ˆê¸°í™” (ì²˜ìŒ í•œ ë²ˆë§Œ)\nif [ ! -f .db_initialized ]; then\n    echo \"ğŸ“¦ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...\"\n    python3 cli.py init\n    touch .db_initialized\n    echo \"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ\"\n    echo \"\"\nfi\n\n# API ì„œë²„ ì‹œì‘\necho \"ğŸŒ API ì„œë²„ ì‹œì‘ (í¬íŠ¸ 8000)...\"\necho \"ğŸ“ ì ‘ì†: http://localhost:8000\"\necho \"ğŸ“š API ë¬¸ì„œ: http://localhost:8000/docs\"\necho \"\"\n\nexec python3 -m uvicorn src.api.main:app \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --reload \\\n    --log-level info\n","size_bytes":837},"data-hub/src/database/models.py":{"content":"\"\"\"\nSQLAlchemy Database Models for Restaurant Data Hub\n\"\"\"\nfrom datetime import datetime\nfrom sqlalchemy import (\n    Column, String, Integer, Float, DateTime, Text, Boolean, JSON, Index\n)\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.sql import func\n\nBase = declarative_base()\n\n\nclass RawRestaurantData(Base):\n    \"\"\"ì›ë³¸ ìŠ¤í¬ë˜í•‘ ë°ì´í„° (ì •ì œ ì „) - Phase 1 ì—…ê·¸ë ˆì´ë“œ\"\"\"\n    __tablename__ = \"raw_restaurant_data\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    source = Column(String, nullable=False)  # 'naver' or 'google'\n    source_id = Column(String, nullable=False)  # ë„¤ì´ë²„/êµ¬ê¸€ ID\n    source_url = Column(String)\n    place_id = Column(String)  # âœ… Phase 1: PlaceID ì¶”ê°€\n    \n    # ê¸°ë³¸ ì •ë³´ (JSONìœ¼ë¡œ ì €ì¥)\n    raw_data = Column(JSON, nullable=False)\n    \n    # ë©”íƒ€ë°ì´í„°\n    scraped_at = Column(DateTime(timezone=True), server_default=func.now())\n    status = Column(String, default='pending')  # pending, processing, processed, failed\n    error_message = Column(Text)\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_raw_source_id', 'source', 'source_id'),\n        Index('idx_raw_status', 'status'),\n        Index('idx_raw_scraped_at', 'scraped_at'),\n        Index('idx_raw_place_id', 'place_id'),  # âœ… Phase 1: PlaceID ì¸ë±ìŠ¤\n    )\n\n\nclass RestaurantMapping(Base):\n    \"\"\"ë„¤ì´ë²„ â†” êµ¬ê¸€ ID ë§¤í•‘ í…Œì´ë¸”\"\"\"\n    __tablename__ = \"restaurant_mapping\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    naver_id = Column(String, unique=True)\n    naver_url = Column(String)\n    google_place_id = Column(String, unique=True)\n    google_url = Column(String)\n    \n    # ë§¤ì¹­ ì‹ ë¢°ë„\n    confidence_score = Column(Float, default=0.0)  # 0-1\n    matched_by = Column(String)  # 'ai', 'manual', 'exact'\n    \n    # ë©”íƒ€ë°ì´í„°\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_mapping_naver', 'naver_id'),\n        Index('idx_mapping_google', 'google_place_id'),\n    )\n\n\nclass ProcessedRestaurant(Base):\n    \"\"\"ì •ì œëœ ë ˆìŠ¤í† ë‘ ë°ì´í„° (í•œì‹ë‹¹ í˜•ì‹) - Phase 1 ì—…ê·¸ë ˆì´ë“œ\"\"\"\n    __tablename__ = \"processed_restaurants\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    mapping_id = Column(String)  # RestaurantMapping FK\n    naver_place_id = Column(String)  # âœ… Phase 1: ë„¤ì´ë²„ PlaceID\n    google_place_id = Column(String)  # âœ… Phase 1: êµ¬ê¸€ PlaceID\n    \n    # ê¸°ë³¸ ì •ë³´\n    name = Column(String, nullable=False)\n    name_en = Column(String)\n    category = Column(String, default='í•œì‹')\n    cuisine = Column(String)  # ìœ¡ë¥˜, ëƒ‰ë©´, ë¶„ì‹ ë“±\n    \n    # ìœ„ì¹˜ ì •ë³´\n    district = Column(String)  # ê°•ë‚¨êµ¬, ì¢…ë¡œêµ¬ ë“±\n    address = Column(String)\n    address_en = Column(String)\n    latitude = Column(Float)\n    longitude = Column(Float)\n    \n    # ìƒì„¸ ì •ë³´\n    description = Column(Text)\n    description_en = Column(Text)\n    price_range = Column(String)  # ì €ë ´, ë³´í†µ, ë¹„ìŒˆ\n    phone = Column(String)\n    website = Column(String)\n    \n    # ìš´ì˜ ì •ë³´\n    open_hours = Column(JSON)  # {\"mon\": \"09:00-22:00\", ...}\n    last_order = Column(String)\n    break_time = Column(String)\n    closed_days = Column(JSON)  # [\"ì¼ìš”ì¼\"]\n    parking = Column(Boolean)\n    \n    # í‰ê°€ ì •ë³´\n    rating = Column(Float)\n    review_count = Column(Integer, default=0)\n    naver_rating = Column(Float)\n    naver_review_count = Column(Integer, default=0)\n    google_rating = Column(Float)\n    google_review_count = Column(Integer, default=0)\n    \n    # ì¸ê¸°ì§€ìˆ˜ (Phase 2: ì¸ê¸°ì§€ìˆ˜ ì‹œìŠ¤í…œ)\n    popularity_score = Column(Float, default=0.0)  # 0-100\n    popularity_tier = Column(String, default='new_or_limited')  # top_rated, highly_popular, popular, average, new_or_limited\n    \n    # ì´ë¯¸ì§€\n    image_url = Column(String)\n    image_urls = Column(JSON)  # ì¶”ê°€ ì´ë¯¸ì§€ë“¤\n    \n    # ë©”ë‰´ (ê°„ë‹¨í•œ ëª©ë¡)\n    menu_summary = Column(JSON)  # [{\"name\": \"ëƒ‰ë©´\", \"price\": \"9000\"}]\n    \n    # âœ… Phase 1: ì‹¤ì œ ë¦¬ë·° ë°ì´í„°\n    reviews = Column(JSON)  # [{\"author\": \"ê¹€**\", \"rating\": 5, \"comment\": \"ë§›ìˆì–´ìš”\"}]\n    \n    # í’ˆì§ˆ ì ìˆ˜\n    quality_score = Column(Integer, default=0)  # 0-100\n    quality_details = Column(JSON)  # ì ìˆ˜ ì„¸ë¶€ ë‚´ì—­\n    \n    # ìƒíƒœ\n    sync_status = Column(String, default='pending')  # pending, synced, failed\n    synced_to_hansikdang = Column(Boolean, default=False)\n    synced_at = Column(DateTime(timezone=True))\n    \n    # ë©”íƒ€ë°ì´í„°\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_processed_district', 'district'),\n        Index('idx_processed_cuisine', 'cuisine'),\n        Index('idx_processed_quality', 'quality_score'),\n        Index('idx_processed_sync', 'sync_status'),\n        Index('idx_processed_naver_place_id', 'naver_place_id'),  # âœ… Phase 1\n        Index('idx_processed_google_place_id', 'google_place_id'),  # âœ… Phase 1\n        Index('idx_processed_popularity', 'popularity_score'),  # âœ… Phase 2: ì¸ê¸°ì§€ìˆ˜\n        Index('idx_processed_popularity_tier', 'popularity_tier'),  # âœ… Phase 2: ì¸ê¸°ë“±ê¸‰\n    )\n\n\nclass ScrapingTarget(Base):\n    \"\"\"ìŠ¤í¬ë˜í•‘ íƒ€ê²Ÿ (í‚¤ì›Œë“œ, ì§€ì—­ ë“±)\"\"\"\n    __tablename__ = \"scraping_targets\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    \n    # íƒ€ê²Ÿ ì •ë³´\n    keyword = Column(String, nullable=False)  # \"ê°•ë‚¨ ëƒ‰ë©´\"\n    region = Column(String)  # \"ê°•ë‚¨êµ¬\"\n    cuisine_type = Column(String)  # \"ëƒ‰ë©´\"\n    \n    # ì˜µì…˜\n    min_rating = Column(Float)\n    min_reviews = Column(Integer)\n    \n    # ìš°ì„ ìˆœìœ„\n    priority = Column(Integer, default=5)  # 1-10\n    \n    # ìƒíƒœ\n    status = Column(String, default='active')  # active, paused, completed\n    last_scraped = Column(DateTime(timezone=True))\n    total_found = Column(Integer, default=0)\n    \n    # ìƒì„± ë°©ë²•\n    created_by = Column(String)  # 'ai', 'manual', 'auto'\n    \n    # ë©”íƒ€ë°ì´í„°\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_target_status', 'status'),\n        Index('idx_target_priority', 'priority'),\n    )\n\n\nclass ScrapingLog(Base):\n    \"\"\"ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ë¡œê·¸\"\"\"\n    __tablename__ = \"scraping_logs\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    target_id = Column(String)\n    \n    # ì‹¤í–‰ ì •ë³´\n    started_at = Column(DateTime(timezone=True), server_default=func.now())\n    completed_at = Column(DateTime(timezone=True))\n    status = Column(String)  # running, completed, failed\n    \n    # ê²°ê³¼\n    total_scraped = Column(Integer, default=0)\n    success_count = Column(Integer, default=0)\n    error_count = Column(Integer, default=0)\n    \n    # ìƒì„¸\n    error_details = Column(JSON)\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_log_started', 'started_at'),\n        Index('idx_log_status', 'status'),\n    )\n\n\nclass SyncLog(Base):\n    \"\"\"í•œì‹ë‹¹ ë™ê¸°í™” ë¡œê·¸\"\"\"\n    __tablename__ = \"sync_logs\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    \n    # ë™ê¸°í™” ì •ë³´\n    started_at = Column(DateTime(timezone=True), server_default=func.now())\n    completed_at = Column(DateTime(timezone=True))\n    status = Column(String)  # running, completed, failed\n    \n    # ê²°ê³¼\n    total_sent = Column(Integer, default=0)\n    success_count = Column(Integer, default=0)\n    error_count = Column(Integer, default=0)\n    \n    # ìƒì„¸\n    error_details = Column(JSON)\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_sync_started', 'started_at'),\n        Index('idx_sync_status', 'status'),\n    )\n\n\n# DuplicateGroup moved to end of file (Stage A MVP version)\n\n\nclass MergeHistory(Base):\n    \"\"\"ë ˆìŠ¤í† ë‘ ë³‘í•© ì´ë ¥\"\"\"\n    __tablename__ = \"merge_history\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    \n    # ë³‘í•© ì •ë³´\n    duplicate_group_id = Column(String, nullable=False)  # DuplicateGroup FK\n    master_id = Column(String, nullable=False)  # ìµœì¢… ëŒ€í‘œ ë ˆìŠ¤í† ë‘\n    merged_ids = Column(JSON, nullable=False)  # ë³‘í•©ëœ ë ˆìŠ¤í† ë‘ ID ëª©ë¡\n    \n    # ë³‘í•© ìƒì„¸\n    merge_reason = Column(String)  # ë³‘í•© ì‚¬ìœ \n    similarity_details = Column(JSON)  # ìœ ì‚¬ë„ ìƒì„¸ ì •ë³´\n    \n    # ë³‘í•© ì „ ë°ì´í„° ë°±ì—…\n    merged_data = Column(JSON)  # ë³‘í•©ëœ ë ˆìŠ¤í† ë‘ì˜ ì›ë³¸ ë°ì´í„°\n    \n    # ìë™/ìˆ˜ë™ ë³‘í•©\n    merge_type = Column(String, default='auto')  # auto, manual\n    merged_by = Column(String)  # 'system', 'admin', 'api'\n    \n    # ë©”íƒ€ë°ì´í„°\n    merged_at = Column(DateTime(timezone=True), server_default=func.now())\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_merge_master', 'master_id'),\n        Index('idx_merge_group', 'duplicate_group_id'),\n        Index('idx_merge_date', 'merged_at'),\n        Index('idx_merge_type', 'merge_type'),\n    )\n\n\nclass QualityMetrics(Base):\n    \"\"\"ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­\"\"\"\n    __tablename__ = \"quality_metrics\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    \n    # ëŒ€ìƒ ì •ë³´\n    restaurant_id = Column(String)  # ProcessedRestaurant FK\n    data_type = Column(String, nullable=False)  # 'restaurant', 'batch', 'system'\n    \n    # 7ê°€ì§€ í’ˆì§ˆ ì§€í‘œ\n    completeness_score = Column(Float, default=0.0)  # ì™„ì „ì„± (0-100)\n    accuracy_score = Column(Float, default=0.0)  # ì •í™•ì„± (0-100)\n    consistency_score = Column(Float, default=0.0)  # ì¼ê´€ì„± (0-100)\n    timeliness_score = Column(Float, default=0.0)  # ì ì‹œì„± (0-100)\n    validity_score = Column(Float, default=0.0)  # ìœ íš¨ì„± (0-100)\n    uniqueness_score = Column(Float, default=0.0)  # ê³ ìœ ì„± (0-100)\n    relevance_score = Column(Float, default=0.0)  # ê´€ë ¨ì„± (0-100)\n    \n    # ì¢…í•© í’ˆì§ˆ ì ìˆ˜\n    overall_quality_score = Column(Float, default=0.0)  # 0-100\n    quality_grade = Column(String)  # A, B, C, D, F\n    \n    # ìƒì„¸ ì •ë³´\n    issues = Column(JSON)  # ë°œê²¬ëœ ë¬¸ì œ ëª©ë¡\n    recommendations = Column(JSON)  # ê°œì„  ê¶Œì¥ì‚¬í•­\n    \n    # ë©”íƒ€ë°ì´í„°\n    measured_at = Column(DateTime(timezone=True), server_default=func.now())\n    measured_by = Column(String, default='system')\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_quality_restaurant', 'restaurant_id'),\n        Index('idx_quality_type', 'data_type'),\n        Index('idx_quality_score', 'overall_quality_score'),\n        Index('idx_quality_measured', 'measured_at'),\n    )\n\n\nclass DataLineage(Base):\n    \"\"\"ë°ì´í„° ê³„ë³´ ì¶”ì \"\"\"\n    __tablename__ = \"data_lineage\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    \n    # ë°ì´í„° ì‹ë³„\n    entity_id = Column(String, nullable=False)  # Restaurant ID\n    entity_type = Column(String, nullable=False)  # 'restaurant'\n    \n    # ë³€í™˜ ì •ë³´\n    operation = Column(String, nullable=False)  # 'scraped', 'processed', 'merged', 'synced'\n    operation_status = Column(String)  # 'success', 'failed', 'partial'\n    \n    # ì†ŒìŠ¤ ì¶”ì \n    source_system = Column(String)  # 'naver', 'google', 'gemini', 'system'\n    source_id = Column(String)  # ì›ë³¸ ë°ì´í„° ID\n    \n    # ë³€í™˜ ìƒì„¸\n    input_data = Column(JSON)  # ë³€í™˜ ì „ ë°ì´í„° (ìƒ˜í”Œ)\n    output_data = Column(JSON)  # ë³€í™˜ í›„ ë°ì´í„° (ìƒ˜í”Œ)\n    transformation_rules = Column(JSON)  # ì ìš©ëœ ê·œì¹™\n    \n    # í’ˆì§ˆ ì˜í–¥\n    quality_before = Column(Float)  # ë³€í™˜ ì „ í’ˆì§ˆ ì ìˆ˜\n    quality_after = Column(Float)  # ë³€í™˜ í›„ í’ˆì§ˆ ì ìˆ˜\n    quality_delta = Column(Float)  # í’ˆì§ˆ ë³€í™”ëŸ‰\n    \n    # ë©”íƒ€ë°ì´í„°\n    executed_at = Column(DateTime(timezone=True), server_default=func.now())\n    executed_by = Column(String)  # 'system', 'scheduler', 'api'\n    execution_time_ms = Column(Integer)  # ì‹¤í–‰ ì‹œê°„ (ë°€ë¦¬ì´ˆ)\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_lineage_entity', 'entity_id'),\n        Index('idx_lineage_operation', 'operation'),\n        Index('idx_lineage_source', 'source_system'),\n        Index('idx_lineage_executed', 'executed_at'),\n    )\n\n\nclass SystemHealth(Base):\n    \"\"\"ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ\"\"\"\n    __tablename__ = \"system_health\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    \n    # ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ\n    component = Column(String, nullable=False)  # 'scraper', 'processor', 'sync', 'api', 'database'\n    component_status = Column(String, nullable=False)  # 'healthy', 'degraded', 'down'\n    \n    # ì„±ëŠ¥ ë©”íŠ¸ë¦­\n    response_time_ms = Column(Integer)  # ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)\n    throughput = Column(Integer)  # ì²˜ë¦¬ëŸ‰ (ê°œ/ì‹œê°„)\n    error_rate = Column(Float)  # ì˜¤ë¥˜ìœ¨ (%)\n    success_rate = Column(Float)  # ì„±ê³µë¥  (%)\n    \n    # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©\n    cpu_usage = Column(Float)  # CPU ì‚¬ìš©ë¥  (%)\n    memory_usage = Column(Float)  # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (%)\n    disk_usage = Column(Float)  # ë””ìŠ¤í¬ ì‚¬ìš©ë¥  (%)\n    \n    # ì‘ì—… í†µê³„\n    total_operations = Column(Integer, default=0)\n    successful_operations = Column(Integer, default=0)\n    failed_operations = Column(Integer, default=0)\n    \n    # ì•Œë¦¼ ì •ë³´\n    alerts = Column(JSON)  # ë°œìƒí•œ ì•Œë¦¼ ëª©ë¡\n    last_alert_at = Column(DateTime(timezone=True))\n    \n    # ìƒì„¸ ì •ë³´\n    details = Column(JSON)  # ì¶”ê°€ ìƒì„¸ ì •ë³´\n    \n    # ë©”íƒ€ë°ì´í„°\n    measured_at = Column(DateTime(timezone=True), server_default=func.now())\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_health_component', 'component'),\n        Index('idx_health_status', 'component_status'),\n        Index('idx_health_measured', 'measured_at'),\n    )\n\n\nclass BackupHistory(Base):\n    \"\"\"Google Drive ë°±ì—… ì´ë ¥\"\"\"\n    __tablename__ = \"backup_history\"\n    \n    id = Column(String, primary_key=True)  # UUID\n    \n    # ë°±ì—… ì •ë³´\n    backup_date = Column(String, nullable=False)  # YYYY-MM-DD\n    backup_type = Column(String, nullable=False)  # 'daily', 'weekly', 'manual'\n    \n    # íŒŒì¼ ì •ë³´\n    file_name = Column(String, nullable=False)  # 07-collection.csv\n    file_path = Column(String)  # /hansikdang-data/2025-11/07-collection.csv\n    drive_file_id = Column(String)  # Google Drive íŒŒì¼ ID\n    file_size_bytes = Column(Integer)  # íŒŒì¼ í¬ê¸° (ë°”ì´íŠ¸)\n    \n    # ë°ì´í„° í†µê³„\n    total_records = Column(Integer, default=0)  # ë°±ì—…ëœ ë ˆìŠ¤í† ë‘ ìˆ˜\n    new_records = Column(Integer, default=0)  # ë‹¹ì¼ ì‹ ê·œ ìˆ˜ì§‘\n    duplicate_removed = Column(Integer, default=0)  # ì¤‘ë³µ ì œê±°ëœ ìˆ˜\n    average_quality_score = Column(Float)  # í‰ê·  í’ˆì§ˆ ì ìˆ˜\n    \n    # ë°±ì—… ìƒíƒœ\n    status = Column(String, nullable=False)  # 'success', 'failed', 'partial'\n    error_message = Column(String)  # ì—ëŸ¬ ë©”ì‹œì§€\n    retry_count = Column(Integer, default=0)  # ì¬ì‹œë„ íšŸìˆ˜\n    \n    # ë©”íƒ€ë°ì´í„°\n    started_at = Column(DateTime(timezone=True))\n    completed_at = Column(DateTime(timezone=True))\n    execution_time_seconds = Column(Integer)  # ì‹¤í–‰ ì‹œê°„ (ì´ˆ)\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_backup_date', 'backup_date'),\n        Index('idx_backup_type', 'backup_type'),\n        Index('idx_backup_status', 'status'),\n        Index('idx_backup_completed', 'completed_at'),\n    )\n\n\nclass CollectionConfig(Base):\n    \"\"\"ìˆ˜ì§‘ ì„¤ì • ê´€ë¦¬ - Stage A MVP\"\"\"\n    __tablename__ = \"collection_configs\"\n    \n    id = Column(Integer, primary_key=True, autoincrement=True)\n    name = Column(String, nullable=False)  # ì„¤ì •ëª…\n    regions = Column(JSON)  # ë‹¤ì¤‘ ì§€ì—­ [{\"sido\": \"ì„œìš¸\", \"gugun\": \"ê°•ë‚¨êµ¬\", \"dong\": \"ì—­ì‚¼ë™\"}]\n    keywords = Column(JSON)  # ë‹¤ì¤‘ í‚¤ì›Œë“œ [\"ê°ˆë¹„\", \"í•œìš°\", \"ìˆ¯ë¶ˆ\"]\n    source = Column(String, default='both')  # google/naver/both\n    status = Column(String, default='active')  # active/paused/pending/rejected\n    monthly_cost = Column(Float, default=0.0)  # ì›”ê°„ ì˜ˆìƒ ë¹„ìš©\n    is_active = Column(Boolean, default=True)  # í™œì„±í™” ì—¬ë¶€\n    \n    # ë©”íƒ€ë°ì´í„°\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_collection_status', 'status'),\n        Index('idx_collection_active', 'is_active'),\n    )\n\n\nclass DuplicateGroup(Base):\n    \"\"\"ì¤‘ë³µ ê·¸ë£¹ ê´€ë¦¬ - Stage A MVP\"\"\"\n    __tablename__ = \"duplicate_groups\"\n    \n    id = Column(Integer, primary_key=True, autoincrement=True)\n    restaurant_ids = Column(JSON)  # ì¤‘ë³µëœ ë ˆìŠ¤í† ë‘ ID ëª©ë¡ [\"id1\", \"id2\"]\n    match_type = Column(String, default='exact')  # exact/fuzzy/geo\n    similarity_score = Column(Float, default=100.0)  # ìœ ì‚¬ë„ ì ìˆ˜ (0-100)\n    status = Column(String, default='pending')  # pending/merged/separated/ignored\n    resolved_by = Column(String)  # ì²˜ë¦¬í•œ ì‚¬ëŒ\n    \n    # ë©”íƒ€ë°ì´í„°\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    resolved_at = Column(DateTime(timezone=True))\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_duplicate_status', 'status'),\n        Index('idx_duplicate_type', 'match_type'),\n    )\n\n\nclass QualityScore(Base):\n    \"\"\"ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ - Stage A MVP\"\"\"\n    __tablename__ = \"quality_scores\"\n    \n    id = Column(Integer, primary_key=True, autoincrement=True)\n    restaurant_id = Column(String, nullable=False)  # FK to ProcessedRestaurant\n    \n    # í’ˆì§ˆ ì§€í‘œ\n    completeness_score = Column(Float, default=0.0)  # í•„ìˆ˜ì •ë³´ ì™„ì„±ë„ (0-100)\n    phone_valid = Column(Boolean, default=False)  # ì „í™”ë²ˆí˜¸ ìœ íš¨ì„±\n    address_complete = Column(Boolean, default=False)  # ì£¼ì†Œ ì™„ì„±ë„\n    coordinates_valid = Column(Boolean, default=False)  # ì¢Œí‘œ ì •í™•ë„\n    total_score = Column(Float, default=0.0)  # ì¢…í•© ì ìˆ˜ (0-100)\n    \n    # ë©”íƒ€ë°ì´í„°\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    \n    # ì¸ë±ìŠ¤\n    __table_args__ = (\n        Index('idx_quality_restaurant', 'restaurant_id'),\n        Index('idx_quality_total', 'total_score'),\n    )\n","size_bytes":18211},"data-hub/test_api.py":{"content":"\"\"\"\nQuick API test script\n\"\"\"\nimport httpx\nimport asyncio\nfrom pprint import pprint\n\n\nasync def test_api():\n    \"\"\"Test Data Hub API endpoints\"\"\"\n    base_url = \"http://localhost:8000\"\n    \n    async with httpx.AsyncClient() as client:\n        print(\"ğŸ” Testing Restaurant Data Hub API...\\n\")\n        \n        # 1. Health check\n        print(\"1ï¸âƒ£ Health Check\")\n        response = await client.get(f\"{base_url}/\")\n        print(f\"   Status: {response.status_code}\")\n        pprint(response.json())\n        print()\n        \n        # 2. Stats\n        print(\"2ï¸âƒ£ System Stats\")\n        response = await client.get(f\"{base_url}/api/stats\")\n        print(f\"   Status: {response.status_code}\")\n        pprint(response.json())\n        print()\n        \n        # 3. Targets\n        print(\"3ï¸âƒ£ Scraping Targets\")\n        response = await client.get(f\"{base_url}/api/targets\")\n        print(f\"   Status: {response.status_code}\")\n        targets = response.json()\n        print(f\"   Found {len(targets)} targets:\")\n        for target in targets:\n            print(f\"     - {target['keyword']} ({target['region']}) [priority: {target['priority']}]\")\n        print()\n        \n        # 4. Raw restaurants\n        print(\"4ï¸âƒ£ Raw Restaurant Data\")\n        response = await client.get(f\"{base_url}/api/restaurants/raw?limit=10\")\n        print(f\"   Status: {response.status_code}\")\n        raw_data = response.json()\n        print(f\"   Found {len(raw_data)} raw records\")\n        print()\n        \n        # 5. Scraping logs\n        print(\"5ï¸âƒ£ Scraping Logs\")\n        response = await client.get(f\"{base_url}/api/logs/scraping?limit=10\")\n        print(f\"   Status: {response.status_code}\")\n        logs = response.json()\n        print(f\"   Found {len(logs)} log entries\")\n        print()\n        \n        print(\"âœ… All API tests passed!\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(test_api())\n","size_bytes":1902},"data-hub/src/__init__.py":{"content":"\"\"\"Restaurant Data Hub\"\"\"\n__version__ = \"0.1.0\"\n","size_bytes":48},"data-hub/cron_schedule.py":{"content":"\"\"\"\nCron Schedule for Restaurant Data Hub\në§¤ì¼ ìë™ ì‹¤í–‰ ìŠ¤ì¼€ì¤„\n\"\"\"\nimport schedule\nimport time\nimport asyncio\nfrom loguru import logger\n\nfrom src.workflows.scraping import ScrapingWorkflow\nfrom src.workflows.sync import SyncWorkflow\n\n\nasync def daily_scraping_job():\n    \"\"\"ë§¤ì¼ ì˜¤í›„ 2ì‹œ: ìŠ¤í¬ë˜í•‘\"\"\"\n    logger.info(\"Running daily scraping job\")\n    workflow = ScrapingWorkflow()\n    await workflow.run_daily_scraping()\n    logger.info(\"Daily scraping job completed\")\n\n\nasync def daily_processing_job():\n    \"\"\"ë§¤ì¼ ì˜¤í›„ 4ì‹œ: ë°ì´í„° ì²˜ë¦¬\"\"\"\n    logger.info(\"Running daily processing job\")\n    workflow = ScrapingWorkflow()\n    await workflow.process_raw_data(batch_size=500)\n    logger.info(\"Daily processing job completed\")\n\n\nasync def daily_sync_job():\n    \"\"\"ë§¤ì¼ ìƒˆë²½ 3ì‹œ: í•œì‹ë‹¹ ë™ê¸°í™”\"\"\"\n    logger.info(\"Running daily sync job\")\n    workflow = SyncWorkflow()\n    await workflow.sync_to_hansikdang()\n    logger.info(\"Daily sync job completed\")\n\n\ndef run_async_job(coro):\n    \"\"\"ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰ wrapper\"\"\"\n    asyncio.run(coro)\n\n\n# ìŠ¤ì¼€ì¤„ ì„¤ì •\nschedule.every().day.at(\"14:00\").do(lambda: run_async_job(daily_scraping_job()))\nschedule.every().day.at(\"16:00\").do(lambda: run_async_job(daily_processing_job()))\nschedule.every().day.at(\"03:00\").do(lambda: run_async_job(daily_sync_job()))\n\n\nif __name__ == \"__main__\":\n    logger.info(\"Starting cron scheduler...\")\n    logger.info(\"Schedules:\")\n    logger.info(\"  - 14:00: Daily scraping\")\n    logger.info(\"  - 16:00: Data processing\")\n    logger.info(\"  - 03:00: Sync to í•œì‹ë‹¹\")\n    \n    while True:\n        schedule.run_pending()\n        time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬\n","size_bytes":1684},"data-hub/src/workflows/scraping.py":{"content":"\"\"\"\nMain scraping workflow\ní•˜ë£¨ 333ê°œ ë ˆìŠ¤í† ë‘ ìˆ˜ì§‘ ëª©í‘œ\n\"\"\"\nimport asyncio\nimport uuid\nfrom datetime import datetime\nfrom typing import List\nfrom loguru import logger\n\nfrom src.database.connection import db_session\nfrom src.database.models import (\n    RawRestaurantData, ScrapingTarget, ScrapingLog\n)\nfrom src.scrapers.naver import NaverPlaceScraper\nfrom src.scrapers.google import GoogleMapsScraper\nfrom src.processors.gemini import GeminiProcessor\nfrom config import settings\n\n\nclass ScrapingWorkflow:\n    \"\"\"ìŠ¤í¬ë˜í•‘ ì›Œí¬í”Œë¡œìš°\"\"\"\n    \n    def __init__(self):\n        self.naver_scraper = NaverPlaceScraper()\n        self.google_scraper = GoogleMapsScraper()\n        self.gemini = GeminiProcessor()\n        self.logger = logger.bind(workflow=\"scraping\")\n    \n    async def run_daily_scraping(self):\n        \"\"\"ì¼ì¼ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰\"\"\"\n        self.logger.info(f\"Starting daily scraping (target: {settings.daily_target})\")\n        \n        with db_session() as db:\n            # í™œì„± íƒ€ê²Ÿ ê°€ì ¸ì˜¤ê¸°\n            targets = db.query(ScrapingTarget).filter(\n                ScrapingTarget.status == 'active'\n            ).order_by(\n                ScrapingTarget.priority.desc()\n            ).limit(10).all()\n            \n            if not targets:\n                self.logger.warning(\"No active targets found\")\n                return\n            \n            total_scraped = 0\n            \n            for target in targets:\n                if total_scraped >= settings.daily_target:\n                    break\n                \n                log_id = str(uuid.uuid4())\n                log = ScrapingLog(\n                    id=log_id,\n                    target_id=target.id,\n                    status='running'\n                )\n                db.add(log)\n                db.commit()\n                \n                try:\n                    # ë„¤ì´ë²„ ìŠ¤í¬ë˜í•‘\n                    naver_results = await self.naver_scraper.search(\n                        keyword=target.keyword,\n                        region=target.region,\n                        limit=50\n                    )\n                    \n                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n                    success_count = 0\n                    for result in naver_results:\n                        try:\n                            raw_data = RawRestaurantData(\n                                id=str(uuid.uuid4()),\n                                source='naver',\n                                source_id=result.source_id,\n                                source_url=result.source_url,\n                                raw_data=result.raw_data,\n                                status='pending'\n                            )\n                            db.add(raw_data)\n                            success_count += 1\n                            total_scraped += 1\n                            \n                            if total_scraped >= settings.daily_target:\n                                break\n                                \n                        except Exception as e:\n                            self.logger.error(f\"Failed to save: {e}\")\n                    \n                    # ë¡œê·¸ ì—…ë°ì´íŠ¸\n                    log.completed_at = datetime.now()\n                    log.status = 'completed'\n                    log.total_scraped = len(naver_results)\n                    log.success_count = success_count\n                    \n                    # íƒ€ê²Ÿ ì—…ë°ì´íŠ¸\n                    target.last_scraped = datetime.now()\n                    target.total_found += success_count\n                    \n                    db.commit()\n                    \n                    self.logger.info(\n                        f\"Target '{target.keyword}': {success_count} restaurants\"\n                    )\n                    \n                    # Rate limiting\n                    await asyncio.sleep(5)\n                    \n                except Exception as e:\n                    self.logger.error(f\"Scraping failed for '{target.keyword}': {e}\")\n                    log.status = 'failed'\n                    log.error_details = {\"error\": str(e)}\n                    db.commit()\n            \n            self.logger.info(f\"Daily scraping completed: {total_scraped} restaurants\")\n    \n    async def process_raw_data(self, batch_size: int = 50):\n        \"\"\"ì›ë³¸ ë°ì´í„° ì²˜ë¦¬ (Gemini AI)\"\"\"\n        self.logger.info(\"Processing raw data with Gemini AI\")\n        \n        with db_session() as db:\n            # ë¯¸ì²˜ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n            raw_items = db.query(RawRestaurantData).filter(\n                RawRestaurantData.status == 'pending'\n            ).limit(batch_size).all()\n            \n            for item in raw_items:\n                try:\n                    item.status = 'processing'\n                    db.commit()\n                    \n                    # Geminië¡œ ë°ì´í„° ì •ì œ\n                    refined = await self.gemini.refine_restaurant_data(item.raw_data)\n                    \n                    # ProcessedRestaurantì— ì €ì¥\n                    # (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ êµ¬í˜„)\n                    \n                    item.status = 'processed'\n                    db.commit()\n                    \n                    self.logger.info(f\"Processed: {refined.get('name')}\")\n                    \n                except Exception as e:\n                    self.logger.error(f\"Processing failed: {e}\")\n                    item.status = 'failed'\n                    item.error_message = str(e)\n                    db.commit()\n\n\nasync def main():\n    \"\"\"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\"\"\"\n    workflow = ScrapingWorkflow()\n    \n    # ì¼ì¼ ìŠ¤í¬ë˜í•‘\n    await workflow.run_daily_scraping()\n    \n    # ë°ì´í„° ì²˜ë¦¬\n    await workflow.process_raw_data()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n","size_bytes":5868},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"asyncpg>=0.30.0\",\n    \"fastapi>=0.120.1\",\n    \"fuzzywuzzy>=0.18.0\",\n    \"geopy>=2.4.1\",\n    \"google-api-python-client>=2.187.0\",\n    \"google-auth>=2.43.0\",\n    \"google-auth-oauthlib>=1.2.2\",\n    \"psutil>=7.1.3\",\n    \"pydantic>=2.12.3\",\n    \"python-levenshtein>=0.27.3\",\n    \"python-multipart>=0.0.20\",\n    \"pytrends>=4.9.2\",\n    \"schedule>=1.2.2\",\n    \"uvicorn[standard]>=0.38.0\",\n]\n","size_bytes":530},"data-hub/DEPLOYMENT_TO_NEW_REPL.md":{"content":"# ğŸš€ Data Hub ìƒˆ Repl ë°°í¬ ê°€ì´ë“œ\n\n## ğŸ“‹ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (Replit Secrets)\n\nhansikdang-data-hub Replì—ì„œ ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:\n\n### **í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜**\n\n1. **Tools ì•„ì´ì½˜ (ğŸ”§) â†’ Secrets í´ë¦­**\n2. ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ í•˜ë‚˜ì”© ì¶”ê°€:\n\n```bash\n# 1. PostgreSQL Database (Supabase)\nDATABASE_URL=postgresql://postgres.[PROJECT]:[PASSWORD]@aws-0-ap-northeast-2.pooler.supabase.com:6543/postgres\n\n# 2. Google Gemini AI\nGEMINI_API_KEY=AIzaSy...\n\n# 3. Apify (Naver Place ìŠ¤í¬ë˜í•‘)\nAPIFY_API_TOKEN=apify_api_...\n\n# 4. í•œì‹ë‹¹ External API ì¸ì¦\nDATA_COLLECTION_API_KEY=...\n\n# 5. Session Secret (32ì ëœë¤ ë¬¸ìì—´)\nSESSION_SECRET=...\n```\n\n### **ì„ íƒ í™˜ê²½ ë³€ìˆ˜**\n\n```bash\n# Google Maps ìŠ¤í¬ë˜í•‘ (ì„ íƒì‚¬í•­)\nOUTSCRAPER_API_KEY=...\n```\n\n---\n\n## ğŸ“¦ íŒŒì¼ ë³µì‚¬ ë°©ë²•\n\n### **ë°©ë²• 1: Replit íŒŒì¼ ì—…ë¡œë“œ (ê¶Œì¥)**\n\n1. í˜„ì¬ hansikdang Replì—ì„œ `data-hub/` í´ë” ì „ì²´ë¥¼ ë‹¤ìš´ë¡œë“œ\n2. hansikdang-data-hub Replì—ì„œ ì—…ë¡œë“œ\n\n### **ë°©ë²• 2: Git ì‚¬ìš©**\n\n```bash\n# í˜„ì¬ hansikdang Replì—ì„œ\ncd data-hub\ngit init\ngit add .\ngit commit -m \"Initial Data Hub commit\"\ngit remote add origin <your-repo-url>\ngit push -u origin main\n\n# hansikdang-data-hub Replì—ì„œ\ngit clone <your-repo-url> .\n```\n\n### **ë°©ë²• 3: ìˆ˜ë™ ë³µì‚¬ (ëª¨ë“  íŒŒì¼)**\n\në‹¤ìŒ í´ë”/íŒŒì¼ë“¤ì„ ë³µì‚¬:\n\n```\ndata-hub/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ api/\nâ”‚   â”‚   â””â”€â”€ main.py                 # FastAPI ë©”ì¸ ì•±\nâ”‚   â”œâ”€â”€ database/\nâ”‚   â”‚   â”œâ”€â”€ connection.py           # DB ì—°ê²°\nâ”‚   â”‚   â””â”€â”€ models.py               # SQLAlchemy ëª¨ë¸ (7ê°œ í…Œì´ë¸”)\nâ”‚   â”œâ”€â”€ scrapers/\nâ”‚   â”‚   â”œâ”€â”€ apify_scraper.py        # Apify (Naver Place)\nâ”‚   â”‚   â””â”€â”€ outscraper_scraper.py   # Outscraper (Google Maps)\nâ”‚   â”œâ”€â”€ processors/\nâ”‚   â”‚   â””â”€â”€ gemini.py               # Gemini AI í”„ë¡œì„¸ì„œ\nâ”‚   â””â”€â”€ workflows/\nâ”‚       â”œâ”€â”€ scraping.py             # ìŠ¤í¬ë˜í•‘ ì›Œí¬í”Œë¡œìš°\nâ”‚       â”œâ”€â”€ processing.py           # AI ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°\nâ”‚       â””â”€â”€ sync.py                 # í•œì‹ë‹¹ ë™ê¸°í™”\nâ”œâ”€â”€ cli.py                          # CLI ë„êµ¬\nâ”œâ”€â”€ cron_schedule.py                # í¬ë¡  ìŠ¤ì¼€ì¤„ëŸ¬\nâ”œâ”€â”€ config.py                       # ì„¤ì •\nâ”œâ”€â”€ requirements.txt                # Python ì˜ì¡´ì„±\nâ”œâ”€â”€ Dockerfile                      # Docker ì„¤ì •\nâ”œâ”€â”€ .env.example                    # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì œ\nâ”œâ”€â”€ START_HERE.md                   # ì‹œì‘ ê°€ì´ë“œ\nâ”œâ”€â”€ QUICK_START.md                  # ë¹ ë¥¸ ì‹œì‘\nâ””â”€â”€ DEPLOYMENT_GUIDE.md             # ë°°í¬ ê°€ì´ë“œ\n```\n\n---\n\n## ğŸ”§ ì„¤ì¹˜ ë° ì´ˆê¸°í™”\n\n### **1. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜**\n\n```bash\n# hansikdang-data-hub Repl Shellì—ì„œ\npip install -r requirements.txt\n```\n\n### **2. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”**\n\n```bash\n# 7ê°œ í…Œì´ë¸” ìƒì„±\npython3 cli.py init\n\n# í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ì¶”ê°€\npython3 cli.py add-target \"ê°•ë‚¨ ëƒ‰ë©´\" --region ê°•ë‚¨êµ¬ --priority 10\npython3 cli.py add-target \"ì´íƒœì› í•œì •ì‹\" --region ìš©ì‚°êµ¬ --priority 8\n```\n\n### **3. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸**\n\n```bash\npython3 cli.py\n```\n\nì¶œë ¥ ì˜ˆìƒ:\n```\n============================================================\n  ğŸª Restaurant Data Hub - System Status\n============================================================\n\nğŸ“ Scraping Targets: 2 total, 2 active\nğŸ“¦ Raw Data: 0 total, 0 pending\nâœ¨ Processed Data: 0 total, 0 synced\nğŸ”— ID Mappings: 0 naverâ†”google pairs\nğŸ“‹ Logs: 0 scraping, 0 sync\n```\n\n---\n\n## ğŸŒ API ì„œë²„ ì‹¤í–‰\n\n```bash\npython3 -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload\n```\n\n**ì ‘ì†:**\n- API: `https://[your-repl].replit.dev:8000`\n- ë¬¸ì„œ: `https://[your-repl].replit.dev:8000/docs`\n\n---\n\n## ğŸ§ª í…ŒìŠ¤íŠ¸\n\n### **1. Health Check**\n```bash\ncurl https://[your-repl].replit.dev:8000/\n```\n\n### **2. ì‹œìŠ¤í…œ í†µê³„**\n```bash\ncurl https://[your-repl].replit.dev:8000/api/stats\n```\n\n### **3. ì²« ìŠ¤í¬ë˜í•‘ (Apify)**\n```bash\npython3 cli.py scrape\n```\n\n---\n\n## ğŸ“¤ ë°°í¬ (Publish)\n\n1. **Replit Console â†’ Publish ë²„íŠ¼ í´ë¦­**\n2. **Deployment Type ì„ íƒ: Autoscale**\n3. **Deploy í´ë¦­**\n4. **Settings â†’ Link a domain**\n5. **ì…ë ¥: `data-hub.hansikdang.net`**\n6. **Replitì´ ì œê³µí•˜ëŠ” IP ì£¼ì†Œ ë³µì‚¬**\n\n---\n\n## ğŸŒ DNS ì„¤ì •\n\nhansikdang.net DNS ê´€ë¦¬ í˜ì´ì§€ì—ì„œ:\n\n```\nType:  A\nName:  data-hub\nValue: [Replitì´ ì œê³µí•œ IP ì£¼ì†Œ]\nTTL:   3600\n```\n\n**ì „íŒŒ ëŒ€ê¸°:** 5~30ë¶„ (ìµœëŒ€ 48ì‹œê°„)\n\n**í™•ì¸:**\n```bash\n# í„°ë¯¸ë„ì—ì„œ\nnslookup data-hub.hansikdang.net\n\n# ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸\nhttps://dnschecker.org\n```\n\n---\n\n## âœ… ìµœì¢… í™•ì¸\n\n```bash\n# í•˜ìœ„ ë„ë©”ì¸ ì ‘ì†\nhttps://data-hub.hansikdang.net/\n\n# API ë¬¸ì„œ\nhttps://data-hub.hansikdang.net/docs\n\n# ì‹¤ì‹œê°„ í†µê³„\nhttps://data-hub.hansikdang.net/api/stats\n```\n\n---\n\n## ğŸ¤– 24/7 ìë™í™” í™œì„±í™”\n\n```bash\n# í¬ë¡  ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)\nnohup python3 cron_schedule.py &\n```\n\n**ìë™ ì‘ì—…:**\n- ë§¤ì¼ 00:00 - AI íƒ€ê²Ÿ ìƒì„±\n- ë§¤ì¼ 02:00 - ìŠ¤í¬ë˜í•‘ ì‹¤í–‰\n- ë§¤ì¼ 04:00 - AI ì²˜ë¦¬\n- ë§¤ì¼ 06:00 - í•œì‹ë‹¹ ë™ê¸°í™”\n\n---\n\n## ğŸ“Š ëª¨ë‹ˆí„°ë§\n\n**ë§¤ì¼ í™•ì¸í•  URL:**\n```\nhttps://data-hub.hansikdang.net/docs\n```\n\n**ì£¼ìš” ì§€í‘œ:**\n- `/api/stats` - ì „ì²´ ì‹œìŠ¤í…œ í†µê³„\n- `/api/targets` - ìŠ¤í¬ë˜í•‘ íƒ€ê²Ÿ ëª©ë¡\n- `/api/logs/scraping` - ìŠ¤í¬ë˜í•‘ ë¡œê·¸\n- `/api/restaurants/raw` - ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„°\n\n---\n\n## ğŸ†˜ ë¬¸ì œ í•´ê²°\n\n**API ì„œë²„ê°€ ì•ˆ ëœ¨ë©´:**\n```bash\n# ë¡œê·¸ í™•ì¸\ncat /tmp/logs/*.log\n\n# DB ì—°ê²° í…ŒìŠ¤íŠ¸\npython3 -c \"from src.database.connection import db_session; print('DB OK')\"\n```\n\n**ìŠ¤í¬ë˜í•‘ì´ ì•ˆ ë˜ë©´:**\n```bash\n# Apify í¬ë ˆë”§ í™•ì¸\nhttps://console.apify.com/account/plan\n\n# í™˜ê²½ ë³€ìˆ˜ í™•ì¸\npython3 -c \"from config import APIFY_API_TOKEN; print('Token OK' if APIFY_API_TOKEN else 'Missing')\"\n```\n\n---\n\n**ë°°í¬ ì™„ë£Œ í›„ ì´ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë§¤ì¼ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”!** ğŸ‰\n","size_bytes":5973},"data-hub/src/database/__init__.py":{"content":"\"\"\"Database module\"\"\"\n","size_bytes":22},"data-hub/DEPLOYMENT_GUIDE.md":{"content":"# Restaurant Data Hub ë°°í¬ ê°€ì´ë“œ\n\n## ğŸ¯ ëª©í‘œ\n1ê°œì›”ì— 1ë§Œ ê°œ ë ˆìŠ¤í† ë‘ ë°ì´í„° ìë™ ìˆ˜ì§‘ ë° ê´€ë¦¬\n\n## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„\n\n### 1. API í‚¤ ë°œê¸‰\n\n#### Apify (ë„¤ì´ë²„í”Œë ˆì´ìŠ¤ ìŠ¤í¬ë˜í•‘)\n1. https://apify.com ê°€ì…\n2. í”Œëœ: Starter ($49/ì›”)\n3. API Token ë³µì‚¬\n\n#### Outscraper (êµ¬ê¸€ë§µìŠ¤ ìŠ¤í¬ë˜í•‘)\n1. https://outscraper.com ê°€ì…\n2. í¬ë ˆë”§ êµ¬ë§¤ ($10 = 1,000ê°œ)\n3. API Key ë³µì‚¬\n\n#### Bright Data (ì„ íƒ - í”„ë¡ì‹œ)\n1. https://brightdata.com ê°€ì…\n2. Residential Proxies ì„ íƒ\n3. í•œêµ­ IP í”„ë¡ì‹œ ì„¤ì •\n\n#### Gemini API (ë¬´ë£Œ!)\n- ì´ë¯¸ ì„¤ì •ëœ GEMINI_API_KEY ì¬ì‚¬ìš© ê°€ëŠ¥\n\n---\n\n## ğŸš€ Replitì—ì„œ ì‹¤í–‰ (ê°œë°œ/í…ŒìŠ¤íŠ¸)\n\n### 1. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜\n```bash\ncd data-hub\npip install -r requirements.txt\n```\n\n### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\nReplit Secretsì— ì¶”ê°€:\n```\nDATA_HUB_DATABASE_URL=postgresql://...  # ë³„ë„ Supabase ì¸ìŠ¤í„´ìŠ¤\nGEMINI_API_KEY=your_key\nAPIFY_API_TOKEN=your_token\nOUTSCRAPER_API_KEY=your_key\nHANSIKDANG_API_URL=http://localhost:5000\nDATA_COLLECTION_API_KEY=your_existing_key\n```\n\n### 3. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”\n```bash\ncd data-hub\npython cli.py init\n```\n\n### 4. íƒ€ê²Ÿ í‚¤ì›Œë“œ ìƒì„± (AI)\n```bash\npython cli.py generate-targets --region ê°•ë‚¨êµ¬ --count 50\n```\n\n### 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n```bash\n# ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ (ì†ŒëŸ‰)\npython cli.py scrape\n\n# ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\npython cli.py process\n\n# í•œì‹ë‹¹ ë™ê¸°í™” í…ŒìŠ¤íŠ¸\npython cli.py sync\n```\n\n---\n\n## â˜ï¸ Google Cloud Run ë°°í¬ (í”„ë¡œë•ì…˜)\n\n### ì™œ Cloud Run?\n- âœ… ì‚¬ìš©í•œ ë§Œí¼ë§Œ ê³¼ê¸ˆ (ì‹¤í–‰ ì•ˆ í•˜ë©´ $0)\n- âœ… Auto-scaling (ë¶€í•˜ ì¦ê°€ ì‹œ ìë™ í™•ì¥)\n- âœ… í•œì‹ë‹¹ê³¼ ê°™ì€ Google Cloud ìƒíƒœê³„\n- âœ… í¬ë¡  ì¡ ë‚´ì¥ (Cloud Scheduler)\n\n### 1. Google Cloud ì„¤ì •\n\n```bash\n# gcloud CLI ì„¤ì¹˜\ncurl https://sdk.cloud.google.com | bash\n\n# ë¡œê·¸ì¸\ngcloud auth login\n\n# í”„ë¡œì íŠ¸ ì„¤ì •\ngcloud config set project YOUR_PROJECT_ID\n```\n\n### 2. Docker ì´ë¯¸ì§€ ë¹Œë“œ & í‘¸ì‹œ\n\n```bash\ncd data-hub\n\n# ì´ë¯¸ì§€ ë¹Œë“œ\ndocker build -t gcr.io/YOUR_PROJECT_ID/data-hub .\n\n# Google Container Registryì— í‘¸ì‹œ\ndocker push gcr.io/YOUR_PROJECT_ID/data-hub\n```\n\n### 3. Cloud Run ë°°í¬\n\n```bash\ngcloud run deploy data-hub \\\n  --image gcr.io/YOUR_PROJECT_ID/data-hub \\\n  --platform managed \\\n  --region asia-northeast3 \\\n  --allow-unauthenticated \\\n  --set-env-vars DATA_HUB_DATABASE_URL=postgresql://... \\\n  --set-env-vars GEMINI_API_KEY=... \\\n  --set-env-vars APIFY_API_TOKEN=... \\\n  --set-env-vars OUTSCRAPER_API_KEY=... \\\n  --set-env-vars HANSIKDANG_API_URL=... \\\n  --set-env-vars DATA_COLLECTION_API_KEY=... \\\n  --memory 2Gi \\\n  --cpu 2 \\\n  --timeout 3600 \\\n  --max-instances 5\n```\n\n### 4. Cloud Scheduler (í¬ë¡  ì¡) ì„¤ì •\n\n```bash\n# ë§¤ì¼ ì˜¤í›„ 2ì‹œ: ìŠ¤í¬ë˜í•‘\ngcloud scheduler jobs create http daily-scraping \\\n  --schedule=\"0 14 * * *\" \\\n  --uri=\"https://YOUR_CLOUD_RUN_URL/api/scrape/start\" \\\n  --http-method=POST \\\n  --time-zone=\"Asia/Seoul\"\n\n# ë§¤ì¼ ì˜¤í›„ 4ì‹œ: ë°ì´í„° ì²˜ë¦¬\ngcloud scheduler jobs create http daily-processing \\\n  --schedule=\"0 16 * * *\" \\\n  --uri=\"https://YOUR_CLOUD_RUN_URL/api/process/start\" \\\n  --http-method=POST \\\n  --time-zone=\"Asia/Seoul\"\n\n# ë§¤ì¼ ìƒˆë²½ 3ì‹œ: í•œì‹ë‹¹ ë™ê¸°í™”\ngcloud scheduler jobs create http daily-sync \\\n  --schedule=\"0 3 * * *\" \\\n  --uri=\"https://YOUR_CLOUD_RUN_URL/api/sync/start\" \\\n  --http-method=POST \\\n  --time-zone=\"Asia/Seoul\"\n```\n\n---\n\n## ğŸ“Š ëª¨ë‹ˆí„°ë§\n\n### Grafana Cloud (ë¬´ë£Œ)\n\n1. https://grafana.com ê°€ì…\n2. Data Source ì¶”ê°€: PostgreSQL (Supabase)\n3. Dashboard ìƒì„±:\n   - ì¼ì¼ ìˆ˜ì§‘ í˜„í™©\n   - ì—ëŸ¬ìœ¨\n   - í’ˆì§ˆ ì ìˆ˜ ë¶„í¬\n   - ë™ê¸°í™” ìƒíƒœ\n\n### Better Stack (ë¬´ë£Œ)\n\n1. https://betterstack.com ê°€ì…\n2. Uptime Monitoring ì„¤ì •\n3. ì•Œë¦¼: ì´ë©”ì¼/SMS\n\n---\n\n## ğŸ’° ì˜ˆìƒ ë¹„ìš© (ì›”)\n\n```\nApify Starter:          $49\nOutscraper:            $150 (15,000ê°œ Ã— $0.01)\nBright Data:            $50 (ì„ íƒ)\nGoogle Cloud Run:       $30 (ì»¨í…Œì´ë„ˆ)\nGemini API:             $50 (ë°ì´í„° ì •ì œ)\nSupabase Pro:           $25 (ë³„ë„ DB)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nì´ ë¹„ìš©:              $354/ì›”\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nì—…ì²´ë‹¹ ë¹„ìš©:          $0.035 (3.5ì„¼íŠ¸)\n```\n\n### ë¹„ìš© ì ˆê° íŒ\n- Apify ëŒ€ì‹  Playwright ì§ì ‘ êµ¬í˜„ ($49 ì ˆê°)\n- ìˆ˜ì§‘ ì†ë„ ì¡°ì ˆ (í•˜ë£¨ 200ê°œ â†’ ì›” $200)\n- Gemini í”„ë¡¬í”„íŠ¸ ìµœì í™” (í† í° ì‚¬ìš© ìµœì†Œí™”)\n\n---\n\n## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…\n\n### ìŠ¤í¬ë˜í•‘ ì°¨ë‹¨\n- Bright Data í”„ë¡ì‹œ ì‚¬ìš©\n- User-Agent ëœë¤í™”\n- ë”œë ˆì´ ì¶”ê°€ (3-5ì´ˆ)\n\n### Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼\n- ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸° (50 â†’ 20)\n- í”„ë¡¬í”„íŠ¸ ìµœì í™”\n- Gemini 1.5 Flash ì‚¬ìš© (ë” ì €ë ´)\n\n### ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜\n- Supabase Connection Pooler ì‚¬ìš©\n- SQLAlchemy NullPool ì„¤ì •\n- íƒ€ì„ì•„ì›ƒ ì¦ê°€ (30s â†’ 60s)\n\n---\n\n## ğŸ“ˆ í™•ì¥ ê³„íš\n\n### Phase 1 (í˜„ì¬)\n- 1ê°œì›” 1ë§Œ ê°œ ìˆ˜ì§‘\n- ë„¤ì´ë²„ + êµ¬ê¸€ í†µí•©\n- í’ˆì§ˆ ì ìˆ˜ 70 ì´ìƒë§Œ ë™ê¸°í™”\n\n### Phase 2 (3ê°œì›”)\n- ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘\n- ë©”ë‰´íŒ OCR (Google Vision)\n- ì‚¬ì§„ í¬ë¡¤ë§\n\n### Phase 3 (6ê°œì›”)\n- ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ê°ì§€\n- íì—… ì—…ì²´ ìë™ ì œê±°\n- ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜\n\n---\n\n## ğŸ¯ ì„±ê³µ ì§€í‘œ\n\n### ë°ì´í„° í’ˆì§ˆ\n- í‰ê·  í’ˆì§ˆ ì ìˆ˜ > 80\n- GPS ì •í™•ë„ > 95%\n- ì¤‘ë³µë¥  < 5%\n\n### ìš´ì˜ íš¨ìœ¨\n- ìŠ¤í¬ë˜í•‘ ì„±ê³µë¥  > 90%\n- ë™ê¸°í™” ì„±ê³µë¥  > 95%\n- ì¼ì¼ ë‹¤ìš´íƒ€ì„ < 10ë¶„\n\n### ë¹„ìš© íš¨ìœ¨\n- ì—…ì²´ë‹¹ ë¹„ìš© < $0.05\n- ì›” ì´ ë¹„ìš© < $500\n- ROI (ê´‘ê³  ìˆ˜ìµ ëŒ€ë¹„) > 300%\n\n---\n\n## ğŸ“ ì§€ì›\n\në¬¸ì œ ë°œìƒ ì‹œ:\n1. ë¡œê·¸ í™•ì¸: `python cli.py logs`\n2. ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ: API `/api/stats`\n3. GitHub Issues ë“±ë¡\n","size_bytes":5724},"data-hub/src/scrapers/naver.py":{"content":"\"\"\"\nNaver Place Scraper using Apify\n\"\"\"\nfrom typing import List, Optional, Dict, Any\nimport uuid\nfrom apify_client import ApifyClient\nfrom loguru import logger\n\nfrom src.scrapers.base import BaseScraper, ScrapedRestaurant\nfrom config import settings\n\n\nclass NaverPlaceScraper(BaseScraper):\n    \"\"\"ë„¤ì´ë²„í”Œë ˆì´ìŠ¤ ìŠ¤í¬ë˜í¼ (Apify ì‚¬ìš©)\"\"\"\n    \n    def __init__(self):\n        super().__init__(api_key=settings.apify_api_token)\n        if not self.api_key:\n            raise ValueError(\"APIFY_API_TOKEN not set\")\n        \n        self.client = ApifyClient(self.api_key)\n        \n        # Apify Actor ID (ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ìŠ¤í¬ë˜í¼)\n        # delicious_zebu/naver-map-search-results-scraper (2025, 99% success rate)\n        self.actor_id = \"delicious_zebu/naver-map-search-results-scraper\"\n    \n    async def search(\n        self,\n        keyword: str,\n        region: Optional[str] = None,\n        limit: int = 50\n    ) -> List[ScrapedRestaurant]:\n        \"\"\"ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ê²€ìƒ‰\"\"\"\n        try:\n            search_query = f\"{region} {keyword}\" if region else keyword\n            \n            self.logger.info(f\"Searching Naver: {search_query}\")\n            \n            # Apify Actor ì‹¤í–‰\n            run_input = {\n                \"keywords\": [search_query]\n            }\n            \n            run = self.client.actor(self.actor_id).call(run_input=run_input)\n            \n            # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n            results = []\n            for item in self.client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n                restaurant = self._parse_naver_item(item)\n                if restaurant:\n                    results.append(restaurant)\n            \n            self.logger.info(f\"Found {len(results)} restaurants from Naver\")\n            return results\n            \n        except Exception as e:\n            self.logger.error(f\"Naver search failed: {e}\")\n            return []\n    \n    async def get_details(self, source_id: str) -> ScrapedRestaurant:\n        \"\"\"ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ìƒì„¸ ì •ë³´\"\"\"\n        try:\n            # Naver Place IDë¡œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n            run_input = {\n                \"placeIds\": [source_id],\n                \"includeReviews\": True,\n                \"maxReviews\": 20,\n                \"includeImages\": True,\n            }\n            \n            run = self.client.actor(self.actor_id).call(run_input=run_input)\n            \n            for item in self.client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n                return self._parse_naver_item(item)\n            \n            raise ValueError(f\"No details found for {source_id}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to get Naver details: {e}\")\n            raise\n    \n    def _parse_naver_item(self, item: Dict[str, Any]) -> ScrapedRestaurant:\n        \"\"\"Apify ì‘ë‹µì„ ScrapedRestaurantë¡œ ë³€í™˜\"\"\"\n        try:\n            source_id = item.get(\"id\") or item.get(\"placeId\") or str(uuid.uuid4())\n            \n            return ScrapedRestaurant(\n                source=\"naver\",\n                source_id=source_id,\n                source_url=item.get(\"url\", \"\"),\n                raw_data=item,\n                name=item.get(\"title\") or item.get(\"name\"),\n                address=item.get(\"address\"),\n                phone=item.get(\"phone\") or item.get(\"tel\"),\n                rating=float(item.get(\"rating\", 0)) if item.get(\"rating\") else None,\n                review_count=int(item.get(\"reviewCount\", 0)) if item.get(\"reviewCount\") else None,\n                latitude=float(item.get(\"lat\")) if item.get(\"lat\") else None,\n                longitude=float(item.get(\"lng\")) if item.get(\"lng\") else None,\n            )\n        except Exception as e:\n            self.logger.error(f\"Failed to parse Naver item: {e}\")\n            return None\n\n\n# ìˆ˜ë™ ìŠ¤í¬ë˜í•‘ ë°±ì—… (Apify ì—†ì´)\nclass NaverPlaceManualScraper(BaseScraper):\n    \"\"\"ìˆ˜ë™ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ìŠ¤í¬ë˜í¼ (Playwright ì‚¬ìš©)\"\"\"\n    \n    async def search(self, keyword: str, region: Optional[str] = None, limit: int = 50) -> List[ScrapedRestaurant]:\n        \"\"\"\n        TODO: Playwrightë¡œ ì§ì ‘ ìŠ¤í¬ë˜í•‘\n        - Apifyê°€ ë¹„ì‹¸ê±°ë‚˜ ì°¨ë‹¨ë  ê²½ìš° ëŒ€ë¹„\n        - playwrightë¡œ ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ ìë™í™”\n        \"\"\"\n        self.logger.warning(\"Manual scraping not implemented yet\")\n        return []\n    \n    async def get_details(self, source_id: str) -> ScrapedRestaurant:\n        \"\"\"TODO: êµ¬í˜„ í•„ìš”\"\"\"\n        raise NotImplementedError()\n","size_bytes":4546},"data-hub/README.md":{"content":"# Restaurant Data Hub\n\n## ëª©ì \ní•œì‹ë‹¹ í”Œë«í¼ì„ ìœ„í•œ ëŒ€ê·œëª¨ ë ˆìŠ¤í† ë‘ ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë¦¬ ì‹œìŠ¤í…œ\n\n## ëª©í‘œ\n- **1ê°œì›”ì— 1ë§Œ ê°œ ë ˆìŠ¤í† ë‘** ë°ì´í„° ìˆ˜ì§‘\n- ë„¤ì´ë²„í”Œë ˆì´ìŠ¤ + êµ¬ê¸€ë§µìŠ¤ í†µí•© ìŠ¤í¬ë˜í•‘\n- Gemini AI ê¸°ë°˜ ë°ì´í„° ì •ì œ ë° ë²ˆì—­\n- í•œì‹ë‹¹ ë©”ì¸ í”Œë«í¼ê³¼ API ì—°ë™\n\n## ì•„í‚¤í…ì²˜\n\n```\ndata-hub/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ scrapers/          # ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ\nâ”‚   â”‚   â”œâ”€â”€ naver.py       # ë„¤ì´ë²„í”Œë ˆì´ìŠ¤ ìŠ¤í¬ë˜í¼\nâ”‚   â”‚   â”œâ”€â”€ google.py      # êµ¬ê¸€ë§µìŠ¤ ìŠ¤í¬ë˜í¼\nâ”‚   â”‚   â””â”€â”€ base.py        # ê³µí†µ ìŠ¤í¬ë˜í¼ ì¸í„°í˜ì´ìŠ¤\nâ”‚   â”œâ”€â”€ processors/        # ë°ì´í„° ì²˜ë¦¬\nâ”‚   â”‚   â”œâ”€â”€ gemini.py      # Gemini AI ì •ì œ\nâ”‚   â”‚   â”œâ”€â”€ translator.py  # ë‹¤êµ­ì–´ ë²ˆì—­\nâ”‚   â”‚   â””â”€â”€ validator.py   # í’ˆì§ˆ ê²€ì¦\nâ”‚   â”œâ”€â”€ database/          # ë°ì´í„°ë² ì´ìŠ¤\nâ”‚   â”‚   â”œâ”€â”€ models.py      # SQLAlchemy ëª¨ë¸\nâ”‚   â”‚   â”œâ”€â”€ connection.py  # DB ì—°ê²°\nâ”‚   â”‚   â””â”€â”€ migrations/    # ë§ˆì´ê·¸ë ˆì´ì…˜\nâ”‚   â”œâ”€â”€ workflows/         # ì›Œí¬í”Œë¡œìš°\nâ”‚   â”‚   â”œâ”€â”€ discovery.py   # íƒ€ê²Ÿ ë°œê²¬\nâ”‚   â”‚   â”œâ”€â”€ scraping.py    # ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš°\nâ”‚   â”‚   â””â”€â”€ sync.py        # í•œì‹ë‹¹ ë™ê¸°í™”\nâ”‚   â””â”€â”€ api/               # FastAPI ì„œë²„\nâ”‚       â”œâ”€â”€ main.py        # API ì—”íŠ¸ë¦¬í¬ì¸íŠ¸\nâ”‚       â””â”€â”€ routes/        # API ë¼ìš°íŠ¸\nâ”œâ”€â”€ dashboard/             # Admin Dashboard\nâ”‚   â”œâ”€â”€ src/\nâ”‚   â”‚   â”œâ”€â”€ pages/\nâ”‚   â”‚   â””â”€â”€ components/\nâ”‚   â””â”€â”€ package.json\nâ”œâ”€â”€ requirements.txt       # Python ì˜ì¡´ì„±\nâ”œâ”€â”€ Dockerfile            # Cloud Run ë°°í¬ìš©\nâ””â”€â”€ config.py             # ì„¤ì • ê´€ë¦¬\n```\n\n## ê¸°ìˆ  ìŠ¤íƒ\n- **ì–¸ì–´**: Python 3.11+\n- **í”„ë ˆì„ì›Œí¬**: FastAPI, SQLAlchemy\n- **AI**: Google Gemini 2.0 Flash\n- **ìŠ¤í¬ë˜í•‘**: Apify, Outscraper, httpx\n- **ë°ì´í„°ë² ì´ìŠ¤**: PostgreSQL (Supabase)\n- **ì›Œí¬í”Œë¡œìš°**: Python asyncio + schedule\n\n## í™˜ê²½ ë³€ìˆ˜\n```bash\n# Supabase (ë³„ë„ ì¸ìŠ¤í„´ìŠ¤)\nDATA_HUB_DATABASE_URL=\n\n# API Keys\nGEMINI_API_KEY=\nAPIFY_API_TOKEN=\nOUTSCRAPER_API_KEY=\nBRIGHT_DATA_PROXY_URL=\n\n# í•œì‹ë‹¹ ì—°ë™\nHANSIKDANG_API_URL=\nDATA_COLLECTION_API_KEY=\n```\n\n## ì‹¤í–‰ ë°©ë²•\n```bash\n# ì˜ì¡´ì„± ì„¤ì¹˜\npip install -r requirements.txt\n\n# ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜\npython -m src.database.migrations.migrate\n\n# API ì„œë²„ ì‹¤í–‰\npython -m src.api.main\n\n# ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (í¬ë¡ )\npython -m src.workflows.scraping\n```\n\n## ë°°í¬\nGoogle Cloud Runìœ¼ë¡œ ë°°í¬ ì˜ˆì •\n","size_bytes":2636},"data-hub/START_HERE.md":{"content":"# ğŸš€ Restaurant Data Hub - ì‹œì‘ ê°€ì´ë“œ\n\n## âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n\në°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆê³ , í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n---\n\n## ğŸ“¡ API ì„œë²„ ì‹¤í–‰ ë°©ë²•\n\n### ë°©ë²• 1: ì§ì ‘ ì‹¤í–‰ (ê¶Œì¥)\n\n```bash\ncd data-hub\npython3 -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload\n```\n\n**ì ‘ì† URL:**\n- API ì„œë²„: `http://localhost:8000`\n- ìë™ ë¬¸ì„œ: `http://localhost:8000/docs` (Swagger UI)\n- ëŒ€ì²´ ë¬¸ì„œ: `http://localhost:8000/redoc`\n\n### ë°©ë²• 2: ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰\n\n```bash\ncd data-hub\n./start_server.sh\n```\n\n---\n\n## ğŸ” API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸\n\n### 1. Health Check\n```bash\ncurl http://localhost:8000/\n```\n\n### 2. ì‹œìŠ¤í…œ í†µê³„\n```bash\ncurl http://localhost:8000/api/stats\n```\n\n### 3. ìŠ¤í¬ë˜í•‘ íƒ€ê²Ÿ ëª©ë¡\n```bash\ncurl http://localhost:8000/api/targets\n```\n\n### 4. ì›ë³¸ ë ˆìŠ¤í† ë‘ ë°ì´í„°\n```bash\ncurl http://localhost:8000/api/restaurants/raw?limit=10\n```\n\n### 5. ìŠ¤í¬ë˜í•‘ ë¡œê·¸\n```bash\ncurl http://localhost:8000/api/logs/scraping?limit=10\n```\n\n---\n\n## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„\n\n### 1. AIë¡œ íƒ€ê²Ÿ í‚¤ì›Œë“œ ìë™ ìƒì„±\n```bash\ncd data-hub\npython3 cli.py generate-targets --region ê°•ë‚¨êµ¬ --count 50\n```\n\n### 2. ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ (Apify API í•„ìš”)\n```bash\n# .env íŒŒì¼ì— APIFY_API_TOKEN ì¶”ê°€ í›„\npython3 cli.py scrape\n```\n\n### 3. ë°ì´í„° ì²˜ë¦¬ (Gemini AI)\n```bash\npython3 cli.py process\n```\n\n### 4. í•œì‹ë‹¹ ë™ê¸°í™”\n```bash\npython3 cli.py sync\n```\n\n### 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n```bash\npython3 cli.py full-pipeline\n```\n\n---\n\n## ğŸŒ Replit ì›¹ë·°ì—ì„œ ì ‘ì†í•˜ê¸°\n\nReplitì—ì„œ API ì„œë²„ë¥¼ ì‹¤í–‰í•˜ë©´, Webview íƒ­ì—ì„œ ìë™ìœ¼ë¡œ ì ‘ì† ê°€ëŠ¥í•©ë‹ˆë‹¤:\n\n1. **í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰:**\n   ```bash\n   cd data-hub && python3 -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000\n   ```\n\n2. **Webview íƒ­ í´ë¦­** ë˜ëŠ” URL ë’¤ì— í¬íŠ¸ ì¶”ê°€:\n   ```\n   https://your-repl-url.replit.dev:8000\n   ```\n\n3. **API ë¬¸ì„œ ìë™ ìƒì„±:**\n   ```\n   https://your-repl-url.replit.dev:8000/docs\n   ```\n\n---\n\n## ğŸ“Š í˜„ì¬ ìƒíƒœ\n\n- âœ… ë°ì´í„°ë² ì´ìŠ¤: 7ê°œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ\n- âœ… í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ: 2ê°œ ì¶”ê°€ (ê°•ë‚¨ ëƒ‰ë©´, ì´íƒœì› í•œì •ì‹)\n- âœ… API ì„œë²„: ì¤€ë¹„ ì™„ë£Œ\n- â³ ìŠ¤í¬ë˜í•‘: API í‚¤ ì„¤ì • í•„ìš”\n- â³ AI ì²˜ë¦¬: Gemini API ì‚¬ìš© ê°€ëŠ¥\n- â³ í•œì‹ë‹¹ ì—°ë™: External API ì¤€ë¹„ ì™„ë£Œ\n\n---\n\n## ğŸ’¡ ìœ ìš©í•œ ëª…ë ¹ì–´\n\n```bash\n# ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸\npython3 cli.py\n\n# íƒ€ê²Ÿ ì¶”ê°€\npython3 cli.py add-target \"ëª…ë™ í•œì •ì‹\" --region ì¤‘êµ¬\n\n# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (ì£¼ì˜!)\npython3 cli.py init\n\n# API ì„œë²„ + ìë™ ì¬ì‹œì‘\npython3 -m uvicorn src.api.main:app --reload --port 8000\n```\n\n---\n\n## ğŸ“š ë¬¸ì„œ\n\n- **QUICK_START.md**: 5ë¶„ ë¹ ë¥¸ ì‹œì‘\n- **DEPLOYMENT_GUIDE.md**: Google Cloud Run ë°°í¬\n- **README.md**: ì „ì²´ ì•„í‚¤í…ì²˜\n- **API Docs**: http://localhost:8000/docs (ì„œë²„ ì‹¤í–‰ í›„)\n\n---\n\n**ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ì‹ ê°€ìš”?**  \n`python3 cli.py --help` ëª…ë ¹ì–´ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª…ë ¹ì–´ë¥¼ í™•ì¸í•˜ì„¸ìš”!\n","size_bytes":3021},"data-hub/src/scrapers/__init__.py":{"content":"\"\"\"Scraping module\"\"\"\n","size_bytes":22},"data-hub/COPY_TO_NEW_REPL_CHECKLIST.md":{"content":"# âœ… Data Hub â†’ ìƒˆ Repl ë³µì‚¬ ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n## ğŸ¯ ëª©í‘œ\n`hansikdang` Replì˜ `data-hub/` í´ë”ë¥¼ â†’ `hansikdang-data-hub` Replë¡œ ì™„ì „íˆ ì´ë™\n\n---\n\n## ğŸ“¦ ë°©ë²• 1: Replit íŒŒì¼ ì‹œìŠ¤í…œ ì‚¬ìš© (ê°€ì¥ ì‰¬ì›€)\n\n### **ë‹¨ê³„ë³„ ê°€ì´ë“œ:**\n\n1. **í˜„ì¬ Repl (hansikdang)ì—ì„œ:**\n   - ì™¼ìª½ Files íŒ¨ë„ì—ì„œ `data-hub` í´ë” ì°¾ê¸°\n   - `data-hub` í´ë” ìš°í´ë¦­ â†’ **Download**\n   - `data-hub.zip` íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œë¨\n\n2. **ìƒˆ Repl (hansikdang-data-hub)ë¡œ ì´ë™:**\n   - hansikdang-data-hub Repl ì—´ê¸°\n   - Files íŒ¨ë„ì—ì„œ ë¹ˆ ê³µê°„ ìš°í´ë¦­ â†’ **Upload files** ë˜ëŠ” **Upload folder**\n   - ë‹¤ìš´ë¡œë“œí•œ `data-hub.zip` ì—…ë¡œë“œ ë˜ëŠ” ì••ì¶• í•´ì œí•œ í´ë” ì—…ë¡œë“œ\n\n3. **í´ë” êµ¬ì¡° í™•ì¸:**\n   ```\n   hansikdang-data-hub/\n   â”œâ”€â”€ src/\n   â”‚   â”œâ”€â”€ api/\n   â”‚   â”œâ”€â”€ database/\n   â”‚   â”œâ”€â”€ scrapers/\n   â”‚   â”œâ”€â”€ processors/\n   â”‚   â””â”€â”€ workflows/\n   â”œâ”€â”€ cli.py\n   â”œâ”€â”€ config.py\n   â”œâ”€â”€ requirements.txt\n   â””â”€â”€ ...\n   ```\n\n---\n\n## ğŸ“¦ ë°©ë²• 2: í„°ë¯¸ë„ ë³µì‚¬ (ê³ ê¸‰)\n\n### **hansikdang Repl Shell:**\n\n```bash\n# data-hub í´ë” ì••ì¶•\ncd /home/runner/workspace\ntar -czf data-hub-backup.tar.gz data-hub/\n\n# ë‹¤ìš´ë¡œë“œ (Webviewì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)\ncp data-hub-backup.tar.gz ~/workspace/\n```\n\n### **hansikdang-data-hub Repl Shell:**\n\n```bash\n# ì••ì¶• íŒŒì¼ ì—…ë¡œë“œ í›„\ntar -xzf data-hub-backup.tar.gz\nmv data-hub/* .\nrm -rf data-hub\n```\n\n---\n\n## ğŸ“‹ ë³µì‚¬í•´ì•¼ í•  í•„ìˆ˜ íŒŒì¼ ëª©ë¡\n\n### **âœ… ë£¨íŠ¸ íŒŒì¼ë“¤**\n- [ ] `cli.py` - CLI ë„êµ¬\n- [ ] `config.py` - ì„¤ì •\n- [ ] `cron_schedule.py` - í¬ë¡  ìŠ¤ì¼€ì¤„ëŸ¬\n- [ ] `requirements.txt` - Python ì˜ì¡´ì„±\n- [ ] `.env.example` - í™˜ê²½ ë³€ìˆ˜ ì˜ˆì œ\n- [ ] `setup_new_repl.sh` - ìë™ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸\n- [ ] `DEPLOYMENT_TO_NEW_REPL.md` - ë°°í¬ ê°€ì´ë“œ\n- [ ] `START_HERE.md` - ì‹œì‘ ê°€ì´ë“œ\n- [ ] `QUICK_START.md` - ë¹ ë¥¸ ì‹œì‘\n- [ ] `DEPLOYMENT_GUIDE.md` - Cloud Run ê°€ì´ë“œ\n- [ ] `README.md` - ì „ì²´ ë¬¸ì„œ\n\n### **âœ… src/ í´ë”**\n- [ ] `src/__init__.py`\n- [ ] `src/api/__init__.py`\n- [ ] `src/api/main.py` - FastAPI ë©”ì¸\n- [ ] `src/database/__init__.py`\n- [ ] `src/database/connection.py` - DB ì—°ê²°\n- [ ] `src/database/models.py` - 7ê°œ í…Œì´ë¸” ëª¨ë¸\n- [ ] `src/scrapers/__init__.py`\n- [ ] `src/scrapers/base.py` - ìŠ¤í¬ë˜í¼ ë² ì´ìŠ¤\n- [ ] `src/scrapers/naver.py` - Naver Place (Apify)\n- [ ] `src/scrapers/google.py` - Google Maps (Outscraper)\n- [ ] `src/processors/__init__.py`\n- [ ] `src/processors/gemini.py` - Gemini AI í”„ë¡œì„¸ì„œ\n- [ ] `src/workflows/__init__.py`\n- [ ] `src/workflows/scraping.py` - ìŠ¤í¬ë˜í•‘ ì›Œí¬í”Œë¡œìš°\n- [ ] `src/workflows/sync.py` - í•œì‹ë‹¹ ë™ê¸°í™”\n\n### **âš ï¸ ë³µì‚¬í•˜ì§€ ë§ ê²ƒ**\n- âŒ `__pycache__/` í´ë”ë“¤\n- âŒ `.pyc` íŒŒì¼ë“¤\n- âŒ `venv/` ë˜ëŠ” `.venv/`\n- âŒ `.db_initialized` (ìë™ ìƒì„±ë¨)\n- âŒ `*.log` íŒŒì¼ë“¤\n- âŒ `api_server.pid`\n\n---\n\n## ğŸ”§ ë³µì‚¬ í›„ ì„¤ì •\n\n### **1. Replit Secrets ì„¤ì •**\n\nhansikdang-data-hub Replì—ì„œ:\n\n```\nTools (ğŸ”§) â†’ Secrets â†’ Add new secret\n```\n\n**í•„ìˆ˜:**\n- `DATABASE_URL` - Supabase PostgreSQL URL\n- `GEMINI_API_KEY` - Google Gemini API í‚¤\n- `APIFY_API_TOKEN` - Apify API í† í°\n- `DATA_COLLECTION_API_KEY` - í•œì‹ë‹¹ External API í‚¤\n\n**ì„ íƒ:**\n- `OUTSCRAPER_API_KEY` - Google Maps ìŠ¤í¬ë˜í•‘ìš© (ì„ íƒ)\n- `SESSION_SECRET` - 32ì ëœë¤ ë¬¸ìì—´\n\n### **2. ìë™ ì„¤ì • ì‹¤í–‰**\n\n```bash\n# hansikdang-data-hub Repl Shellì—ì„œ\nchmod +x setup_new_repl.sh\n./setup_new_repl.sh\n```\n\n**ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ ì‹¤í–‰:**\n1. âœ… í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n2. âœ… Python íŒ¨í‚¤ì§€ ì„¤ì¹˜\n3. âœ… .env íŒŒì¼ ìƒì„±\n4. âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (7ê°œ í…Œì´ë¸”)\n5. âœ… í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ì¶”ê°€ (2ê°œ)\n6. âœ… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸\n\n### **3. API ì„œë²„ ì‹¤í–‰**\n\n```bash\npython3 -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload\n```\n\n---\n\n## âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n### **íŒŒì¼ êµ¬ì¡° í™•ì¸**\n```bash\nls -la\n# ì¶œë ¥ì— cli.py, config.py, requirements.txt, src/ ë“±ì´ ë³´ì—¬ì•¼ í•¨\n```\n\n### **Python íŒ¨í‚¤ì§€ í™•ì¸**\n```bash\npip list | grep -E \"(fastapi|sqlalchemy|apify|google-generativeai)\"\n```\n\n### **DB ì—°ê²° í™•ì¸**\n```bash\npython3 -c \"from src.database.connection import db_session; print('âœ… DB ì—°ê²° ì„±ê³µ')\"\n```\n\n### **API ì„œë²„ í™•ì¸**\n```bash\ncurl http://localhost:8000/ | python3 -m json.tool\n```\n\n---\n\n## ğŸš€ ì™„ë£Œ í›„\n\n1. **Publish** ë²„íŠ¼ í´ë¦­\n2. **Deployment Type**: Autoscale ì„ íƒ\n3. **Deploy** ì‹¤í–‰\n4. **Settings â†’ Link a domain**\n5. **ì…ë ¥**: `data-hub.hansikdang.net`\n6. **DNS ì„¤ì •**: Replitì´ ì œê³µí•œ IP ì£¼ì†Œë¡œ A ë ˆì½”ë“œ ì¶”ê°€\n\n---\n\n## ğŸ†˜ ë¬¸ì œ í•´ê²°\n\n**íŒŒì¼ì´ ë„ˆë¬´ ë§ì•„ì„œ ì—…ë¡œë“œê°€ ì•ˆ ë˜ë©´:**\nâ†’ ë°©ë²• 2 (í„°ë¯¸ë„ ë³µì‚¬) ì‚¬ìš©\n\n**Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ê°€ ì•ˆ ë˜ë©´:**\n```bash\npip install --upgrade pip\npip install -r requirements.txt --force-reinstall\n```\n\n**DB ì—°ê²° ì˜¤ë¥˜:**\nâ†’ Secretsì— `DATABASE_URL`ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸\n\n---\n\n**ë‹¤ìŒ ë¬¸ì„œ:** `DEPLOYMENT_TO_NEW_REPL.md` ì°¸ê³ \n","size_bytes":5049},"data-hub/src/processors/__init__.py":{"content":"\"\"\"Data processing module\"\"\"\n","size_bytes":29},"replit.md":{"content":"# Hansikdang Data Hub - ë ˆìŠ¤í† ë‘ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ\n\n## Overview\nHansikdang Data Hub is a large-scale restaurant data collection and management system for the Hansikdang platform. Its primary goal is to automatically collect approximately 10,000 restaurant data entries monthly from Naver Place and Google Maps, refine them using Gemini AI, and synchronize them with the Hansikdang platform. The project aims to provide comprehensive restaurant data, enhance data quality, and offer robust data governance and monitoring capabilities.\n\n## User Preferences\nNone specified yet.\n\n## System Architecture\n\n### UI/UX Decisions\n-   **Dashboard**: Interactive API documentation via Swagger UI, and a web-based operations dashboard (Vue 3 + Chart.js) for system health, statistics, alerts, and operational controls.\n-   **Input Forms**: Single-scroll forms with clear section headers and compact layouts for menu and link inputs. Autocomplete features for regional inputs.\n-   **Table Enhancements**: Improved collection results table with pagination, action buttons (view, edit, delete), and enhanced statistical cards.\n-   **Design Consistency**: Reusable UI components and consistent styling across different pages (e.g., unified-editor.html, restaurant-detail.html).\n\n### Technical Implementations\n-   **Framework**: FastAPI 0.109.0 with Python 3.11, served by Uvicorn.\n-   **Data Model**: Each entry includes an auto-generated `id`, `timestamp`, a flexible `data` JSON object, and optional `source` and `tags`.\n-   **Automated Scheduling**: Orchestrates data collection, AI processing, and platform synchronization daily and weekly, including smart targeting, duplicate detection, Gemini AI processing, Google rating enrichment, and platform synchronization.\n-   **Data Quality**: Gemini AI refines restaurant descriptions (200-300 characters) and prevents hallucinations.\n-   **Popularity Scoring**: Calculates a comprehensive popularity score (0-100) and tier.\n-   **Rate Limiting**: Robust rate limiting and retry mechanisms for external APIs.\n-   **Scalability**: Designed to collect 33 restaurants daily from Naver.\n\n### Feature Specifications\n-   **Automated 24/7 Scheduler**: Orchestrates data collection, AI processing, and platform synchronization.\n-   **Smart Targeting**: Dynamic query generation based on Google Trends.\n-   **Duplicate Detection**: Fuzzy matching with GPS distance calculation, auto-merging before AI processing.\n-   **Data Governance & Monitoring**: Includes 7 quality indicators, data lineage tracking, system health monitoring, alert management, and comprehensive dashboard APIs.\n-   **Google Drive Backup**: Daily automated CSV backups.\n-   **API Endpoints**: Comprehensive endpoints for core operations, collection & targeting, governance & quality, monitoring & alerts, dashboard, and data management.\n-   **Data Management Systems**: Modules for collection settings, duplicate detection, quality management, fuzzy matching, and batch synchronization.\n\n### System Design Choices\n-   **Project Structure**: Organized into `src` subdirectories for `api`, `scrapers`, `processors`, `workflows`, and `database`.\n-   **Cache Control**: Middleware implemented to prevent browser caching (`Cache-Control: no-cache, no-store, must-revalidate`).\n-   **Error Handling**: Validation logic for inputs like administrative regions with specific error messages.\n\n## External Dependencies\n\n-   **PostgreSQL**: Primary database for persistent storage.\n-   **Apify**: Used for web scraping, particularly for Naver data and weekly full data updates.\n-   **Google Gemini API**: Utilized for AI-powered data refinement, description generation, and hallucination prevention.\n-   **Google Places API**: Integrated for augmenting restaurant data with ratings, review counts, and images.\n-   **Google Drive**: Permanent backup storage for daily CSV backups.\n-   **Hansikdang Main Platform API**: For synchronizing processed restaurant data.\n\n## Recent Changes\n\n### 2025-11-11: Phase 1 ì§ì ‘ ì…ë ¥ í¼ UX ê°œì„  ì™„ë£Œ âœ…\n\n**Phase 1 ì™„ë£Œ ì‚¬í•­:**\n1. **ì§€ì—­ ì…ë ¥ Autocomplete** - ì„œìš¸ 25ê°œêµ¬, ë¶€ì‚° 16ê°œêµ¬, ê²½ê¸° ì£¼ìš” 10ê°œì‹œ (ì´ 50ê°œ)\n2. **ë©”ë‰´ ì…ë ¥ ì»´íŒ©íŠ¸í™”** - í…Œì´ë¸” í˜•íƒœ, ë†’ì´ 50% ì¶•ì†Œ\n3. **ë§í¬ ì…ë ¥ í•œ ì¤„ë¡œ** - í”Œë«í¼ + URL + ID í˜•íƒœ\n4. **ì €ì¥ ì™„ë£Œ íŒì—…** - ì„±ê³µ/ì‹¤íŒ¨ ëª¨ë‹¬, 2ì´ˆ í›„ ìë™ ì´ë™\n\n**ë¹„ìš©:** $2.5-3 (ì˜ˆìƒ ëŒ€ë¹„ 40% ì ˆê°)\n\n### 2025-11-11: Phase 1.5 ì¶”ê°€ì •ë³´ ì„¹ì…˜ ì™„ë£Œ âœ…\n\n**ì™„ë£Œ ì‚¬í•­:**\n- âœ… ğŸŒ ì¶”ê°€ì •ë³´ ì„¹ì…˜ ì¶”ê°€\n- âœ… ë§í¬ íƒ€ì… ë“œë¡­ë‹¤ìš´: í™ˆí˜ì´ì§€, ë¸”ë¡œê·¸, ì¸ìŠ¤íƒ€ê·¸ë¨, í˜ì´ìŠ¤ë¶, ìœ íŠœë¸Œ, ê¸°íƒ€\n- âœ… URL ì…ë ¥ (75% ë„ˆë¹„) + ì‚­ì œ ë²„íŠ¼ (5% ë„ˆë¹„)\n- âœ… \"+ ë§í¬ ì¶”ê°€\" ë²„íŠ¼ìœ¼ë¡œ ë¬´ì œí•œ ì¶”ê°€ ê°€ëŠ¥\n- âœ… additionalInfo ë°°ì—´ â†’ additional_info ê°ì²´ ë³€í™˜\n- âœ… API payloadì— additional_info í•„ë“œ ì¶”ê°€\n\n**ë¹„ìš©:** $0.5-1 (ì•½ 20ë¶„ ì‘ì—…)\n\n**ì´ Phase 1 + 1.5 ë¹„ìš©:** ~$3-3.5 (ì˜ˆìƒ $5-7 ëŒ€ë¹„ 40% ì ˆê°)","size_bytes":4996},"data-hub/config.py":{"content":"\"\"\"\nConfiguration management for Restaurant Data Hub\n\"\"\"\nimport os\nfrom typing import Optional\nfrom pydantic_settings import BaseSettings\n\n\nclass Settings(BaseSettings):\n    \"\"\"Application settings\"\"\"\n    \n    # Database\n    data_hub_database_url: str = os.getenv(\n        \"DATA_HUB_DATABASE_URL\",\n        \"postgresql://user:pass@localhost:5432/datahub\"\n    )\n    \n    # API Keys\n    gemini_api_key: Optional[str] = os.getenv(\"GEMINI_API_KEY\")\n    apify_api_token: Optional[str] = os.getenv(\"APIFY_API_TOKEN\")\n    outscraper_api_key: Optional[str] = os.getenv(\"OUTSCRAPER_API_KEY\")\n    bright_data_proxy_url: Optional[str] = os.getenv(\"BRIGHT_DATA_PROXY_URL\")\n    naver_client_id: Optional[str] = os.getenv(\"NAVER_CLIENT_ID\")\n    naver_client_secret: Optional[str] = os.getenv(\"NAVER_CLIENT_SECRET\")\n    \n    # í•œì‹ë‹¹ ì—°ë™\n    hansikdang_api_url: str = os.getenv(\n        \"HANSIKDANG_API_URL\",\n        \"http://localhost:5000\"\n    )\n    data_collection_api_key: Optional[str] = os.getenv(\"DATA_COLLECTION_API_KEY\")\n    \n    # Scraping Settings\n    daily_target: int = 33  # í•˜ë£¨ ëª©í‘œ ìˆ˜ì§‘ ìˆ˜ (ë„¤ì´ë²„ë§Œ ìš´ì˜)\n    quality_threshold: int = 20  # í’ˆì§ˆ ì ìˆ˜ ì„ê³„ê°’ (ì¡°ì •: 75 â†’ 20, ëª¨ë“  ë ˆìŠ¤í† ë‘ ë™ê¸°í™” ê°€ëŠ¥)\n    batch_size: int = 10  # í•œì‹ë‹¹ ë™ê¸°í™” ë°°ì¹˜ ì‚¬ì´ì¦ˆ (413 ì—ëŸ¬ ë°©ì§€)\n    \n    # Quality Standards\n    min_images: int = 5  # ìµœì†Œ ì´ë¯¸ì§€ ìˆ˜\n    min_reviews: int = 10  # ìµœì†Œ ë¦¬ë·° ìˆ˜\n    \n    # Retry Settings\n    max_retries: int = 3\n    retry_delay: int = 5  # seconds\n    \n    class Config:\n        env_file = \".env\"\n        case_sensitive = False\n        extra = \"ignore\"\n\n\nsettings = Settings()\n","size_bytes":1667},"data-hub/src/scrapers/base.py":{"content":"\"\"\"\nBase scraper interface\n\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom loguru import logger\n\n\n@dataclass\nclass ScrapedRestaurant:\n    \"\"\"ìŠ¤í¬ë˜í•‘ëœ ë ˆìŠ¤í† ë‘ ë°ì´í„°\"\"\"\n    source: str  # 'naver' or 'google'\n    source_id: str\n    source_url: str\n    raw_data: Dict[str, Any]\n    \n    # ê¸°ë³¸ ì •ë³´ (íŒŒì‹±ëœ)\n    name: Optional[str] = None\n    address: Optional[str] = None\n    phone: Optional[str] = None\n    rating: Optional[float] = None\n    review_count: Optional[int] = None\n    latitude: Optional[float] = None\n    longitude: Optional[float] = None\n\n\nclass BaseScraper(ABC):\n    \"\"\"ìŠ¤í¬ë˜í¼ ë² ì´ìŠ¤ í´ë˜ìŠ¤\"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        self.api_key = api_key\n        self.logger = logger.bind(scraper=self.__class__.__name__)\n    \n    @abstractmethod\n    async def search(\n        self,\n        keyword: str,\n        region: Optional[str] = None,\n        limit: int = 50\n    ) -> List[ScrapedRestaurant]:\n        \"\"\"í‚¤ì›Œë“œë¡œ ë ˆìŠ¤í† ë‘ ê²€ìƒ‰\"\"\"\n        pass\n    \n    @abstractmethod\n    async def get_details(self, source_id: str) -> ScrapedRestaurant:\n        \"\"\"ë ˆìŠ¤í† ë‘ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\"\"\"\n        pass\n    \n    def _sanitize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"ë°ì´í„° ì •ì œ (ê³µí†µ)\"\"\"\n        return {k: v for k, v in data.items() if v is not None}\n","size_bytes":1456},"data-hub/cli.py":{"content":"\"\"\"\nCLI Tool for Restaurant Data Hub\nëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤\n\"\"\"\nimport asyncio\nimport click\nfrom loguru import logger\n\nfrom src.database.connection import init_db\nfrom src.workflows.scraping import ScrapingWorkflow\nfrom src.workflows.sync import SyncWorkflow\nfrom src.processors.gemini import GeminiProcessor\nfrom src.database.connection import db_session\nfrom src.database.models import ScrapingTarget\nimport uuid\n\n\n@click.group()\ndef cli():\n    \"\"\"Restaurant Data Hub CLI\"\"\"\n    pass\n\n\n@cli.command()\ndef init():\n    \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”\"\"\"\n    click.echo(\"Initializing database...\")\n    init_db()\n    click.echo(\"âœ… Database initialized!\")\n\n\n@cli.command()\ndef scrape():\n    \"\"\"ìŠ¤í¬ë˜í•‘ ì‹¤í–‰\"\"\"\n    click.echo(\"Starting scraping workflow...\")\n    \n    async def run():\n        workflow = ScrapingWorkflow()\n        await workflow.run_daily_scraping()\n    \n    asyncio.run(run())\n    click.echo(\"âœ… Scraping completed!\")\n\n\n@cli.command()\ndef process():\n    \"\"\"ì›ë³¸ ë°ì´í„° ì²˜ë¦¬\"\"\"\n    click.echo(\"Processing raw data with Gemini AI...\")\n    \n    async def run():\n        from src.database.models import RawRestaurantData, ProcessedRestaurant\n        \n        with db_session() as db:\n            # pending ìƒíƒœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n            raw_data = db.query(RawRestaurantData).filter(\n                RawRestaurantData.status == 'pending'\n            ).limit(100).all()\n            \n            click.echo(f\"Found {len(raw_data)} pending records\")\n            \n            if not raw_data:\n                click.echo(\"No pending data to process\")\n                return\n            \n            gemini = GeminiProcessor()\n            processed_count = 0\n            \n            for idx, raw in enumerate(raw_data):\n                try:\n                    # Rate Limit ë°©ì§€ë¥¼ ìœ„í•œ delay (ë¶„ë‹¹ 10ê°œ ì œí•œ â†’ 8ê°œ/60ì´ˆë¡œ ì•ˆì „í•˜ê²Œ)\n                    if idx > 0 and idx % 8 == 0:\n                        click.echo(f\"  â†’ Rate limit protection: waiting 60 seconds... ({idx}/{len(raw_data)} processed)\")\n                        await asyncio.sleep(60)\n                    \n                    # Geminië¡œ ì •ì œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n                    max_retries = 3\n                    retry_count = 0\n                    refined = None\n                    \n                    while retry_count < max_retries and refined is None:\n                        try:\n                            refined = await gemini.refine_restaurant_data(raw.raw_data)\n                        except Exception as retry_error:\n                            error_msg = str(retry_error)\n                            if \"429\" in error_msg or \"quota\" in error_msg.lower():\n                                retry_count += 1\n                                if retry_count < max_retries:\n                                    wait_time = 60 * retry_count\n                                    click.echo(f\"  âš ï¸ Rate limit hit! Retrying in {wait_time}s... (attempt {retry_count}/{max_retries})\")\n                                    await asyncio.sleep(wait_time)\n                                else:\n                                    click.echo(f\"  âŒ Max retries exceeded for: {raw.source_id}\")\n                                    raise\n                            else:\n                                raise\n                    \n                    if refined is None:\n                        raise Exception(\"Failed to refine data after retries\")\n                    \n                    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n                    quality = await gemini.calculate_quality_score(raw.raw_data)\n                    \n                    # DBì— ì €ì¥\n                    new_restaurant = ProcessedRestaurant(\n                        id=str(uuid.uuid4()),\n                        mapping_id=raw.id,\n                        name=refined.get('name', ''),\n                        name_en=refined.get('nameEn', ''),\n                        category=refined.get('category', 'í•œì‹'),\n                        cuisine=refined.get('cuisine', ''),\n                        district=refined.get('district', ''),\n                        address=refined.get('address', ''),\n                        address_en=refined.get('addressEn', ''),\n                        latitude=raw.raw_data.get('lat') or raw.raw_data.get('geometry', {}).get('location', {}).get('lat'),\n                        longitude=raw.raw_data.get('lng') or raw.raw_data.get('geometry', {}).get('location', {}).get('lng'),\n                        description=refined.get('description', ''),\n                        description_en=refined.get('descriptionEn', ''),\n                        price_range=str(refined.get('priceRange', 2)),\n                        phone=refined.get('phone', ''),\n                        rating=raw.raw_data.get('rating'),\n                        review_count=raw.raw_data.get('reviewCount') or raw.raw_data.get('user_ratings_total', 0),\n                        image_url=refined.get('imageUrl', 'https://via.placeholder.com/400x300?text=Restaurant'),\n                        open_hours=refined.get('openHours'),\n                        quality_score=quality.get('quality_score', 0),\n                        quality_details=quality.get('quality_details', {}),\n                        sync_status='pending'\n                    )\n                    db.add(new_restaurant)\n                    \n                    # raw ë°ì´í„° ìƒíƒœ ì—…ë°ì´íŠ¸\n                    raw.status = 'processed'\n                    \n                    processed_count += 1\n                    click.echo(f\"âœ“ Processed: {refined.get('name', 'Unknown')}\")\n                    \n                except Exception as e:\n                    raw.status = 'failed'\n                    raw.error_message = str(e)\n                    logger.error(f\"Failed to process {raw.id}: {e}\")\n                    click.echo(f\"âœ— Failed: {raw.source_id}\")\n            \n            db.commit()\n            click.echo(f\"\\nâœ… Processed {processed_count}/{len(raw_data)} records\")\n    \n    asyncio.run(run())\n    click.echo(\"âœ… Processing completed!\")\n\n\n@cli.command()\ndef sync():\n    \"\"\"í•œì‹ë‹¹ ë™ê¸°í™”\"\"\"\n    click.echo(\"Syncing to í•œì‹ë‹¹...\")\n    \n    async def run():\n        workflow = SyncWorkflow()\n        await workflow.sync_to_hansikdang()\n    \n    asyncio.run(run())\n    click.echo(\"âœ… Sync completed!\")\n\n\n@cli.command()\n@click.argument('keyword')\n@click.option('--region', default=None, help='ì§€ì—­ (ì˜ˆ: ê°•ë‚¨êµ¬)')\n@click.option('--priority', default=5, help='ìš°ì„ ìˆœìœ„ (1-10)')\ndef add_target(keyword, region, priority):\n    \"\"\"ìŠ¤í¬ë˜í•‘ íƒ€ê²Ÿ ì¶”ê°€\"\"\"\n    with db_session() as db:\n        target = ScrapingTarget(\n            id=str(uuid.uuid4()),\n            keyword=keyword,\n            region=region,\n            priority=priority,\n            status='active',\n            created_by='cli'\n        )\n        db.add(target)\n        db.commit()\n    \n    click.echo(f\"âœ… Target added: {keyword} ({region})\")\n\n\n@cli.command()\n@click.option('--region', default='ê°•ë‚¨êµ¬', help='ì§€ì—­')\n@click.option('--count', default=50, help='ìƒì„±í•  í‚¤ì›Œë“œ ìˆ˜')\ndef generate_targets(region, count):\n    \"\"\"AIë¡œ íƒ€ê²Ÿ í‚¤ì›Œë“œ ìë™ ìƒì„±\"\"\"\n    click.echo(f\"Generating {count} target keywords for {region}...\")\n    \n    async def run():\n        gemini = GeminiProcessor()\n        keywords = await gemini.generate_target_keywords(region, count)\n        \n        with db_session() as db:\n            for keyword in keywords:\n                target = ScrapingTarget(\n                    id=str(uuid.uuid4()),\n                    keyword=keyword,\n                    region=region,\n                    priority=5,\n                    status='active',\n                    created_by='ai'\n                )\n                db.add(target)\n            db.commit()\n        \n        click.echo(f\"âœ… Generated {len(keywords)} targets\")\n    \n    asyncio.run(run())\n\n\n@cli.command()\ndef full_pipeline():\n    \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìŠ¤í¬ë˜í•‘ â†’ ì²˜ë¦¬ â†’ ë™ê¸°í™”)\"\"\"\n    click.echo(\"Running full pipeline...\")\n    \n    async def run():\n        # 1. ìŠ¤í¬ë˜í•‘\n        click.echo(\"Step 1/3: Scraping...\")\n        scrape_workflow = ScrapingWorkflow()\n        await scrape_workflow.run_daily_scraping()\n        \n        # 2. ì²˜ë¦¬\n        click.echo(\"Step 2/3: Processing...\")\n        await scrape_workflow.process_raw_data(batch_size=100)\n        \n        # 3. ë™ê¸°í™”\n        click.echo(\"Step 3/3: Syncing...\")\n        sync_workflow = SyncWorkflow()\n        await sync_workflow.sync_to_hansikdang()\n    \n    asyncio.run(run())\n    click.echo(\"âœ… Full pipeline completed!\")\n\n\n@cli.command()\n@click.argument('json_file')\n@click.option('--source', default='google_places', help='ë°ì´í„° ì†ŒìŠ¤')\ndef import_json(json_file, source):\n    \"\"\"JSON íŒŒì¼ì—ì„œ ë ˆìŠ¤í† ë‘ ë°ì´í„° ì„í¬íŠ¸\"\"\"\n    import json\n    from src.database.models import RawRestaurantData\n    \n    click.echo(f\"Importing from {json_file}...\")\n    \n    with open(json_file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    click.echo(f\"Found {len(data)} restaurants\")\n    \n    with db_session() as db:\n        count = 0\n        for item in data:\n            try:\n                # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ\n                location = item.get('location', {})\n                lat = location.get('lat')\n                lng = location.get('lng')\n                \n                # raw_dataì— lat/lng ì¶”ê°€\n                raw_data = {**item, 'lat': lat, 'lng': lng}\n                \n                raw = RawRestaurantData(\n                    id=str(uuid.uuid4()),\n                    source=source,\n                    source_id=item.get('placeId', item.get('id', str(uuid.uuid4()))),\n                    raw_data=raw_data,\n                    status='pending'\n                )\n                db.add(raw)\n                count += 1\n            except Exception as e:\n                logger.error(f\"Failed to import: {e}\")\n        \n        db.commit()\n        click.echo(f\"âœ… Imported {count}/{len(data)} restaurants\")\n\n\n@cli.command()\n@click.option('--query', default='í™ëŒ€ í•œì‹', help='ê²€ìƒ‰ ì¿¼ë¦¬')\n@click.option('--limit', default=100, help='ìˆ˜ì§‘í•  ìµœëŒ€ ê°œìˆ˜')\ndef scrape_naver(query, limit):\n    \"\"\"ë„¤ì´ë²„ Maps APIë¡œ ë ˆìŠ¤í† ë‘ ë°ì´í„° ìˆ˜ì§‘\"\"\"\n    click.echo(f\"ğŸ” Naver Maps API: {query} (limit={limit})\")\n    \n    async def run():\n        from src.scrapers.naver_maps_api import NaverMapsScraper\n        \n        scraper = NaverMapsScraper()\n        result = await scraper.scrape(query=query, limit=limit)\n        \n        click.echo(f\"\\nâœ… ë„¤ì´ë²„ Maps API ìˆ˜ì§‘ ì™„ë£Œ!\")\n        click.echo(f\"  - ê²€ìƒ‰ ì¿¼ë¦¬: {result['query']}\")\n        click.echo(f\"  - ë°œê²¬: {result['total_found']}ê°œ\")\n        click.echo(f\"  - ì €ì¥: {result['saved_count']}ê°œ\")\n        click.echo(f\"  - ì¤‘ë³µ: {result['duplicate_count']}ê°œ\")\n    \n    asyncio.run(run())\n\n\n@cli.command()\ndef stats():\n    \"\"\"ì „ì²´ í†µê³„ í™•ì¸\"\"\"\n    from src.database.models import RawRestaurantData, ProcessedRestaurant\n    \n    with db_session() as db:\n        total_raw = db.query(RawRestaurantData).count()\n        total_processed = db.query(ProcessedRestaurant).count()\n        total_synced = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.synced_to_hansikdang == True\n        ).count()\n        \n        raw_pending = db.query(RawRestaurantData).filter(\n            RawRestaurantData.status == 'pending'\n        ).count()\n        \n        # ì†ŒìŠ¤ë³„ í†µê³„\n        naver_count = db.query(RawRestaurantData).filter(\n            RawRestaurantData.source == 'naver'\n        ).count()\n        google_count = db.query(RawRestaurantData).filter(\n            RawRestaurantData.source == 'google_places'\n        ).count()\n        \n        click.echo(\"\\nğŸ“Š Data Hub í†µê³„\")\n        click.echo(\"=\" * 50)\n        click.echo(f\"Total raw: {total_raw}\")\n        click.echo(f\"  - Naver: {naver_count}\")\n        click.echo(f\"  - Google: {google_count}\")\n        click.echo(f\"Total processed: {total_processed}\")\n        click.echo(f\"Total synced: {total_synced}\")\n        click.echo(f\"Pending processing: {raw_pending}\")\n        click.echo(f\"Daily target: 333\")\n        click.echo(\"=\" * 50)\n\n\nif __name__ == '__main__':\n    cli()\n","size_bytes":12355},"data-hub/src/api/main.py":{"content":"\"\"\"\nFastAPI Server for Restaurant Data Hub\n\"\"\"\nfrom fastapi import FastAPI, Depends, HTTPException, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import FileResponse, RedirectResponse, Response\nfrom sqlalchemy.orm import Session\nfrom typing import List, Dict, Any, Optional\nimport uuid\nfrom datetime import datetime\nimport os\n\nfrom src.database.connection import get_db, init_db\nfrom src.database.models import (\n    RawRestaurantData, ProcessedRestaurant, ScrapingTarget,\n    ScrapingLog, SyncLog, CollectionConfig, DuplicateGroup, QualityScore\n)\nfrom src.workflows.scraping import ScrapingWorkflow\nfrom src.api.targeting_routes import router as targeting_router\nfrom src.api.deduplication_routes import router as deduplication_router\nfrom src.api.governance_routes import router as governance_router\nfrom src.api.monitoring_routes import router as monitoring_router\nfrom src.api.dashboard_routes import router as dashboard_router\nfrom src.api.sync_routes import router as sync_router\nfrom src.api.restaurant_routes import router as restaurant_router\nfrom src.api.jobs_routes import router as jobs_router\nfrom src.api.collection_routes import router as collection_router\nfrom src.api.duplicate_routes import router as duplicate_router\nfrom src.api.batch_sync_routes import router as batch_sync_router\nfrom src.api.collection_request_routes import router as collection_request_router\nfrom src.api.collection_result_routes import router as collection_result_router\nfrom src.api.manual_input_routes import router as manual_input_router\nfrom src.api.unified_editor_routes import router as unified_editor_router\nfrom src.api.deployment_routes import router as deployment_router\nfrom config import settings\n\napp = FastAPI(\n    title=\"Restaurant Data Hub API\",\n    description=\"ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë¦¬ ì‹œìŠ¤í…œ\",\n    version=\"0.1.0\"\n)\n\napp.include_router(targeting_router)\napp.include_router(deduplication_router)\napp.include_router(governance_router)\napp.include_router(monitoring_router)\napp.include_router(dashboard_router)\napp.include_router(sync_router)\napp.include_router(restaurant_router)\napp.include_router(jobs_router)\napp.include_router(collection_router)\napp.include_router(duplicate_router)\napp.include_router(batch_sync_router)\napp.include_router(collection_request_router)\napp.include_router(collection_result_router)\napp.include_router(manual_input_router)\napp.include_router(unified_editor_router)\napp.include_router(deployment_router)\n\n# CORS ì„¤ì •\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ í•„ìš”\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Cache-Control ë¯¸ë“¤ì›¨ì–´ (ë¸Œë¼ìš°ì € ìºì‹± ë°©ì§€)\n@app.middleware(\"http\")\nasync def add_cache_control_header(request: Request, call_next):\n    response = await call_next(request)\n    response.headers[\"Cache-Control\"] = \"no-cache, no-store, must-revalidate\"\n    response.headers[\"Pragma\"] = \"no-cache\"\n    response.headers[\"Expires\"] = \"0\"\n    return response\n\n# Static files\nstatic_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), \"static\")\nif os.path.exists(static_dir):\n    app.mount(\"/static\", StaticFiles(directory=static_dir), name=\"static\")\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"ì„œë²„ ì‹œì‘ ì‹œ DB ì´ˆê¸°í™”\"\"\"\n    init_db()\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"ë©”ì¸ í˜ì´ì§€ - ëŒ€ì‹œë³´ë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸\"\"\"\n    return RedirectResponse(url=\"/dashboard\")\n\n\n@app.get(\"/health\")\nasync def health_check(db: Session = Depends(get_db)):\n    \"\"\"ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬\"\"\"\n    try:\n        total_records = db.query(RawRestaurantData).count()\n        return {\n            \"status\": \"healthy\",\n            \"database\": \"connected\",\n            \"total_records\": total_records,\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n    except Exception as e:\n        return {\n            \"status\": \"unhealthy\",\n            \"database\": \"error\",\n            \"error\": str(e),\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n\n\n@app.get(\"/api/stats\")\nasync def get_stats(db: Session = Depends(get_db)):\n    \"\"\"ì „ì²´ í†µê³„\"\"\"\n    total_raw = db.query(RawRestaurantData).count()\n    total_processed = db.query(ProcessedRestaurant).count()\n    total_synced = db.query(ProcessedRestaurant).filter(\n        ProcessedRestaurant.synced_to_hansikdang == True\n    ).count()\n    \n    pending_raw = db.query(RawRestaurantData).filter(\n        RawRestaurantData.status == 'pending'\n    ).count()\n    \n    return {\n        \"total_raw\": total_raw,\n        \"total_processed\": total_processed,\n        \"total_synced\": total_synced,\n        \"pending_processing\": pending_raw,\n        \"daily_target\": settings.daily_target,\n    }\n\n\n@app.get(\"/api/targets\")\nasync def get_targets(db: Session = Depends(get_db)):\n    \"\"\"ìŠ¤í¬ë˜í•‘ íƒ€ê²Ÿ ëª©ë¡\"\"\"\n    targets = db.query(ScrapingTarget).order_by(\n        ScrapingTarget.priority.desc()\n    ).limit(100).all()\n    \n    return [\n        {\n            \"id\": t.id,\n            \"keyword\": t.keyword,\n            \"region\": t.region,\n            \"priority\": t.priority,\n            \"status\": t.status,\n            \"total_found\": t.total_found,\n            \"last_scraped\": t.last_scraped.isoformat() if t.last_scraped else None,\n        }\n        for t in targets\n    ]\n\n\n@app.post(\"/api/targets\")\nasync def create_target(\n    keyword: str,\n    region: Optional[str] = None,\n    priority: int = 5,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìƒˆ ìŠ¤í¬ë˜í•‘ íƒ€ê²Ÿ ì¶”ê°€\"\"\"\n    target = ScrapingTarget(\n        id=str(uuid.uuid4()),\n        keyword=keyword,\n        region=region,\n        priority=priority,\n        status='active',\n        created_by='manual'\n    )\n    db.add(target)\n    db.commit()\n    \n    return {\"id\": target.id, \"keyword\": keyword}\n\n\n@app.post(\"/api/scrape/start\")\nasync def start_scraping(db: Session = Depends(get_db)):\n    \"\"\"ìŠ¤í¬ë˜í•‘ ìˆ˜ë™ ì‹œì‘\"\"\"\n    workflow = ScrapingWorkflow()\n    \n    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” Celery ë“± ì‚¬ìš© ê¶Œì¥)\n    import asyncio\n    asyncio.create_task(workflow.run_daily_scraping())\n    \n    return {\"status\": \"started\"}\n\n\n@app.get(\"/api/logs/scraping\")\nasync def get_scraping_logs(limit: int = 50, db: Session = Depends(get_db)):\n    \"\"\"ìŠ¤í¬ë˜í•‘ ë¡œê·¸\"\"\"\n    logs = db.query(ScrapingLog).order_by(\n        ScrapingLog.started_at.desc()\n    ).limit(limit).all()\n    \n    return [\n        {\n            \"id\": log.id,\n            \"started_at\": log.started_at.isoformat(),\n            \"completed_at\": log.completed_at.isoformat() if log.completed_at else None,\n            \"status\": log.status,\n            \"total_scraped\": log.total_scraped,\n            \"success_count\": log.success_count,\n        }\n        for log in logs\n    ]\n\n\n@app.get(\"/api/restaurants/raw\")\nasync def get_raw_restaurants(\n    limit: int = 50,\n    status: Optional[str] = None,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ì›ë³¸ ë ˆìŠ¤í† ë‘ ë°ì´í„°\"\"\"\n    query = db.query(RawRestaurantData)\n    \n    if status:\n        query = query.filter(RawRestaurantData.status == status)\n    \n    items = query.order_by(\n        RawRestaurantData.scraped_at.desc()\n    ).limit(limit).all()\n    \n    return [\n        {\n            \"id\": item.id,\n            \"source\": item.source,\n            \"source_id\": item.source_id,\n            \"status\": item.status,\n            \"scraped_at\": item.scraped_at.isoformat(),\n        }\n        for item in items\n    ]\n\n\n@app.get(\"/dashboard\")\nasync def dashboard():\n    \"\"\"ìš´ì˜ ëŒ€ì‹œë³´ë“œ (Vue.js + Chart.js)\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    dashboard_path = os.path.join(static_dir, \"dashboard.html\")\n    \n    if os.path.exists(dashboard_path):\n        return FileResponse(dashboard_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Dashboard not found\")\n\n\n@app.get(\"/dashboard/data\")\nasync def data_management():\n    \"\"\"ë°ì´í„° ê´€ë¦¬ í˜ì´ì§€\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    data_mgmt_path = os.path.join(static_dir, \"data-management.html\")\n    \n    if os.path.exists(data_mgmt_path):\n        return FileResponse(data_mgmt_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Data management page not found\")\n\n\n@app.get(\"/dashboard/collection-settings\")\nasync def collection_settings():\n    \"\"\"ìˆ˜ì§‘ ì„¤ì • ê´€ë¦¬ í˜ì´ì§€ - Stage A MVP\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    settings_path = os.path.join(static_dir, \"collection-settings.html\")\n    \n    if os.path.exists(settings_path):\n        return FileResponse(settings_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Collection settings page not found\")\n\n\n@app.get(\"/dashboard/quality-check\")\nasync def quality_check():\n    \"\"\"ì¤‘ë³µ ê²€ì‚¬ & í’ˆì§ˆ ê´€ë¦¬ í˜ì´ì§€ - Stage A MVP\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    quality_path = os.path.join(static_dir, \"quality-check.html\")\n    \n    if os.path.exists(quality_path):\n        return FileResponse(quality_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Quality check page not found\")\n\n\n@app.get(\"/dashboard/sync-management\")\nasync def sync_management():\n    \"\"\"ë°°ì¹˜ ë™ê¸°í™” ê´€ë¦¬ í˜ì´ì§€ - Stage B Priority 1 (B-4)\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    sync_path = os.path.join(static_dir, \"sync-management.html\")\n    \n    if os.path.exists(sync_path):\n        return FileResponse(sync_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Sync management page not found\")\n\n\n@app.get(\"/dashboard/data-management\")\nasync def data_management_page():\n    \"\"\"ë°ì´í„° ê´€ë¦¬ í˜ì´ì§€ (íƒ­ êµ¬ì¡°)\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    data_mgmt_path = os.path.join(static_dir, \"data-management.html\")\n    \n    if os.path.exists(data_mgmt_path):\n        return FileResponse(data_mgmt_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Data management page not found\")\n\n\n@app.get(\"/dashboard/jobs\")\nasync def jobs_management():\n    \"\"\"ì‘ì—… ê´€ë¦¬ í˜ì´ì§€ (ì‘ì—… ì‹¤í–‰, ëª¨ë‹ˆí„°ë§, ì´ë ¥, í†µê³„)\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    jobs_path = os.path.join(static_dir, \"jobs.html\")\n    \n    if os.path.exists(jobs_path):\n        return FileResponse(jobs_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Jobs management page not found\")\n\n\n@app.get(\"/dashboard/sync\")\nasync def sync_page():\n    \"\"\"ë™ê¸°í™” ê´€ë¦¬ í˜ì´ì§€ (ë°°í¬ í˜„í™©, ì´ë ¥, í†µê³„)\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    sync_path = os.path.join(static_dir, \"sync.html\")\n    \n    if os.path.exists(sync_path):\n        return FileResponse(sync_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Sync page not found\")\n\n\n@app.get(\"/dashboard/analytics\")\nasync def analytics():\n    \"\"\"ë¶„ì„ í˜ì´ì§€\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    analytics_path = os.path.join(static_dir, \"analytics.html\")\n    \n    if os.path.exists(analytics_path):\n        return FileResponse(analytics_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Analytics page not found\")\n\n\n@app.get(\"/dashboard/settings\")\nasync def settings():\n    \"\"\"ì„¤ì • í˜ì´ì§€\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    settings_path = os.path.join(static_dir, \"settings.html\")\n    \n    if os.path.exists(settings_path):\n        return FileResponse(settings_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Settings page not found\")\n\n\n@app.get(\"/dashboard/collection-request\")\nasync def collection_request():\n    \"\"\"ìˆ˜ì§‘ ìš”ì²­ ê´€ë¦¬ í˜ì´ì§€ - Stage C Phase C-1\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    request_path = os.path.join(static_dir, \"collection-request.html\")\n    \n    if os.path.exists(request_path):\n        return FileResponse(request_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Collection request page not found\")\n\n\n@app.get(\"/dashboard/collection-results\")\nasync def collection_results():\n    \"\"\"ìˆ˜ì§‘ ê²°ê³¼ ê´€ë¦¬ í˜ì´ì§€ - Stage C Phase C-2\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    results_path = os.path.join(static_dir, \"collection-results.html\")\n    \n    if os.path.exists(results_path):\n        return FileResponse(results_path, headers={\"Cache-Control\": \"no-cache, no-store, must-revalidate\"})\n    else:\n        raise HTTPException(status_code=404, detail=\"Collection results page not found\")\n\n\n@app.get(\"/dashboard/manual-input\")\nasync def manual_input():\n    \"\"\"ìˆ˜ë™ ì…ë ¥ í˜ì´ì§€ - Stage C Phase C-3\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    manual_path = os.path.join(static_dir, \"manual-input.html\")\n    \n    if os.path.exists(manual_path):\n        return FileResponse(manual_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Manual input page not found\")\n\n\n@app.get(\"/dashboard/unified-editor\")\nasync def unified_editor():\n    \"\"\"í†µí•© í¸ì§‘ & ë³‘í•© í˜ì´ì§€ - Stage C Phase C-4\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    unified_path = os.path.join(static_dir, \"unified-editor.html\")\n    \n    if os.path.exists(unified_path):\n        return FileResponse(unified_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Unified editor page not found\")\n\n\n@app.get(\"/dashboard/deployment\")\nasync def deployment():\n    \"\"\"ì„ íƒì  ë°°í¬ ê´€ë¦¬ í˜ì´ì§€ - Stage C Phase C-5\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    deployment_path = os.path.join(static_dir, \"deployment.html\")\n    \n    if os.path.exists(deployment_path):\n        return FileResponse(deployment_path)\n    else:\n        raise HTTPException(status_code=404, detail=\"Deployment page not found\")\n\n\n@app.get(\"/dashboard/restaurant/{restaurant_id}\")\nasync def restaurant_detail(restaurant_id: str):\n    \"\"\"ë ˆìŠ¤í† ë‘ ìƒì„¸ë³´ê¸° í˜ì´ì§€ - Phase 2\"\"\"\n    static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"..\", \"static\")\n    detail_path = os.path.join(static_dir, \"restaurant-detail.html\")\n    \n    if os.path.exists(detail_path):\n        return FileResponse(detail_path, headers={\"Cache-Control\": \"no-cache, no-store, must-revalidate\"})\n    else:\n        raise HTTPException(status_code=404, detail=\"Restaurant detail page not found\")\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","size_bytes":15085},"data-hub/src/utils/__init__.py":{"content":"\"\"\"Utility modules\"\"\"\n","size_bytes":22},"data-hub/src/utils/rate_limiter.py":{"content":"\"\"\"\nRate Limiter - API ìš”ì²­ ì†ë„ ì œí•œ\n\"\"\"\nimport asyncio\nfrom collections import deque\nfrom time import time\nfrom loguru import logger\n\n\nclass RateLimiter:\n    \"\"\"\n    ìš”ì²­ ì†ë„ ì œí•œ (1ì´ˆë‹¹ Nê°œ)\n    \n    Usage:\n        rate_limiter = RateLimiter(max_requests=10, per_seconds=1)\n        await rate_limiter.acquire()  # ìš”ì²­ ì „ í˜¸ì¶œ\n    \"\"\"\n    \n    def __init__(self, max_requests: int, per_seconds: int = 1):\n        \"\"\"\n        Args:\n            max_requests: ìµœëŒ€ ìš”ì²­ ìˆ˜\n            per_seconds: ì‹œê°„ ì°½ (ì´ˆ)\n        \"\"\"\n        self.max_requests = max_requests\n        self.per_seconds = per_seconds\n        self.requests = deque()\n        logger.debug(f\"RateLimiter initialized: {max_requests} req/{per_seconds}s\")\n    \n    async def acquire(self):\n        \"\"\"ìš”ì²­ í† í° íšë“ (í•„ìš”ì‹œ ëŒ€ê¸°)\"\"\"\n        now = time()\n        \n        # ì˜¤ë˜ëœ ìš”ì²­ ì œê±°\n        while self.requests and self.requests[0] < now - self.per_seconds:\n            self.requests.popleft()\n        \n        # ì œí•œ ì´ˆê³¼ ì‹œ ëŒ€ê¸°\n        if len(self.requests) >= self.max_requests:\n            sleep_time = self.requests[0] + self.per_seconds - now + 0.01\n            logger.debug(f\"Rate limit reached, sleeping {sleep_time:.2f}s\")\n            await asyncio.sleep(sleep_time)\n            return await self.acquire()\n        \n        self.requests.append(now)\n","size_bytes":1385},"data-hub/src/processors/gemini.py":{"content":"\"\"\"\nGemini AI Data Processor\n- ë°ì´í„° ì •ì œ\n- ë‹¤êµ­ì–´ ë²ˆì—­\n- ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\n- ì¤‘ë³µ ë§¤ì¹­ íŒë‹¨\n- ì¸ê¸°ì§€ìˆ˜ ê³„ì‚° (Phase 2)\n\"\"\"\nfrom typing import Dict, Any, Optional, List\nimport json\nimport google.generativeai as genai\nfrom loguru import logger\n\nfrom config import settings\nfrom src.processors.popularity_calculator import PopularityCalculator\n\n\nclass GeminiProcessor:\n    \"\"\"Gemini AI ë°ì´í„° ì²˜ë¦¬\"\"\"\n    \n    def __init__(self):\n        if not settings.gemini_api_key:\n            raise ValueError(\"GEMINI_API_KEY not set\")\n        \n        genai.configure(api_key=settings.gemini_api_key)\n        self.model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n        self.logger = logger.bind(processor=\"gemini\")\n    \n    async def refine_restaurant_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        ë ˆìŠ¤í† ë‘ ë°ì´í„° ì •ì œ ë° ë³´ì™„ (Phase 2 ì—…ê·¸ë ˆì´ë“œ)\n        - 200-300ì ìƒì„¸ ì„¤ëª…\n        - ë‹¤ì–‘í•œ í•„ë“œ ì¶”ê°€\n        - ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)\n        \"\"\"\n        try:\n            # ì‹¤ì œ ë©”ë‰´/ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ (ìˆìœ¼ë©´)\n            menu_items = raw_data.get(\"menu_items\", [])\n            reviews = raw_data.get(\"reviews\", [])\n            category_info = raw_data.get(\"parsed_category\", {})\n            \n            prompt = f\"\"\"\në‹¹ì‹ ì€ í•œêµ­ ìŒì‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë ˆìŠ¤í† ë‘ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  ë§¤ë ¥ì ì¸ ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n**ë ˆìŠ¤í† ë‘ ì •ë³´:**\n- ì´ë¦„: {raw_data.get('name')}\n- ì£¼ì†Œ: {raw_data.get('address')}\n- ì¹´í…Œê³ ë¦¬: {raw_data.get('category')} / {category_info.get('sub')} / {category_info.get('detail')}\n- ì „í™”ë²ˆí˜¸: {raw_data.get('phone', 'ì •ë³´ ì—†ìŒ')}\n- ê°„ë‹¨ ì„¤ëª…: {raw_data.get('description', '')}\n\n**ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •ì œëœ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ì„¸ìš”:**\n{{\n  \"name\": \"í•œê¸€ ì‹ë‹¹ ì´ë¦„ (ì •ì œ)\",\n  \"nameEn\": \"ì˜ë¬¸ ì´ë¦„ (ìì—°ìŠ¤ëŸ¬ìš´ ë²ˆì—­ ë˜ëŠ” ë¡œë§ˆì í‘œê¸°)\",\n  \"category\": \"í•œì‹\" | \"ì¼ì‹\" | \"ì¤‘ì‹\" | \"ì–‘ì‹\" | \"ì¹´í˜Â·ë””ì €íŠ¸\" | \"ê¸°íƒ€\",\n  \"cuisine\": \"êµ¬ì²´ì  ìš”ë¦¬ (ì˜ˆ: ì‚¼ê³„íƒ•, ìœ¡íšŒ, ëƒ‰ë©´, ë¶ˆê³ ê¸°)\",\n  \"district\": \"ì§€ì—­êµ¬ë§Œ (ê°•ë‚¨êµ¬/ì¢…ë¡œêµ¬/ë§ˆí¬êµ¬ ë“±, 'ì„œìš¸íŠ¹ë³„ì‹œ' ì œì™¸)\",\n  \"address\": \"ì •ì œëœ ì „ì²´ ì£¼ì†Œ\",\n  \"description\": \"í•œê¸€ ì„¤ëª… (200-300ì, ì•„ë˜ ì‘ì„± ìš”êµ¬ì‚¬í•­ ì°¸ê³ )\",\n  \"descriptionEn\": \"ì˜ë¬¸ ì„¤ëª… (150-200ì, í•œê¸€ ì„¤ëª…ì˜ í•µì‹¬ ë‚´ìš© ë²ˆì—­)\",\n  \"priceRange\": \"1\" | \"2\" | \"3\" | \"4\" (ë¬¸ìì—´ë¡œ ë°˜í™˜),\n  \"imageUrl\": \"https://via.placeholder.com/400x300?text=Restaurant\",\n  \"openHours\": \"ì˜ì—…ì‹œê°„ (ì˜ˆ: 11:00-22:00) ë˜ëŠ” null\",\n  \"phone\": \"ì „í™”ë²ˆí˜¸ (02-123-4567 í˜•ì‹) ë˜ëŠ” null\"\n}}\n\n**í•œê¸€ ì„¤ëª… ì‘ì„± ìš”êµ¬ì‚¬í•­ (200-300ì):**\n1. **ì²« ë¬¸ì¥**: ë ˆìŠ¤í† ë‘ì˜ í•µì‹¬ íŠ¹ì§• (ëŒ€í‘œ ë©”ë‰´, ì—­ì‚¬, ìœ„ì¹˜, ë¶„ìœ„ê¸°)\n   - ì˜ˆ: \"1985ë…„ë¶€í„° 3ëŒ€ì§¸ ì´ì–´ì˜¨ ì „í†µ í•œì‹ë‹¹ìœ¼ë¡œ, ì‹ ì„ í•œ í•œìš° ìœ¡íšŒì™€ ë¶ˆê³ ê¸°ê°€ ëŒ€í‘œ ë©”ë‰´ì…ë‹ˆë‹¤.\"\n   \n2. **ë‘ ë²ˆì§¸**: ì¸ê¸° ë©”ë‰´ ìƒì„¸ (ë§›, ì¬ë£Œ, ì¡°ë¦¬ë²•, ê°€ê²©ëŒ€)\n   - ì˜ˆ: \"ìœ¡íšŒëŠ” ì°¸ê¸°ë¦„, ë°°, ë§ˆëŠ˜ì„ ë²„ë¬´ë ¤ ê³ ì†Œí•˜ê³  ë¶€ë“œëŸ¬ìš´ ì‹ê°ì´ ì¼í’ˆì´ë©°, ë¶ˆê³ ê¸°ëŠ” ì§ì ‘ ë§Œë“  ì–‘ë…ì— ì¬ì›Œ ë‹¬ì½¤í•˜ë©´ì„œë„ ê¹Šì€ ë§›ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.\"\n   \n3. **ì„¸ ë²ˆì§¸**: ì¶”ì²œ í¬ì¸íŠ¸ (ë¶„ìœ„ê¸°, ì„œë¹„ìŠ¤, ì ‘ê·¼ì„±)\n   - ì˜ˆ: \"ê¹”ë”í•œ ì¸í…Œë¦¬ì–´ì™€ ì¹œì ˆí•œ ì„œë¹„ìŠ¤ë¡œ ê°€ì¡± ëª¨ì„ì´ë‚˜ ì ‘ëŒ€ì— ì í•©í•˜ë©°, ì§€í•˜ì² ì—­ì—ì„œ ë„ë³´ 5ë¶„ ê±°ë¦¬ë¡œ ì ‘ê·¼ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤.\"\n   \n4. **ë„¤ ë²ˆì§¸**: ë°©ë¬¸ ì¶”ì²œ (ì‹œê°„ëŒ€, ìƒí™©, ì˜ˆì•½ í•„ìš”ì„±)\n   - ì˜ˆ: \"ì ì‹¬ íŠ¹ì„  ë©”ë‰´(11:00-14:00)ê°€ ê°€ì„±ë¹„ê°€ ì¢‹ìœ¼ë©°, ì €ë… ì‹œê°„ëŒ€ì—ëŠ” ì˜ˆì•½ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\"\n\n**ì˜ë¬¸ ì„¤ëª… ì‘ì„± ìš”êµ¬ì‚¬í•­ (150-200ì):**\n- í•œê¸€ ì„¤ëª…ì˜ í•µì‹¬ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ë¡œ ë²ˆì—­\n- ì™¸êµ­ì¸ ê´€ê´‘ê°ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±\n- ì˜ˆ: \"A traditional Korean restaurant since 1985, famous for fresh yukhoe (Korean beef tartare) and bulgogi. The yukhoe is mixed with sesame oil, pear, and garlic for a smooth texture, while the bulgogi is marinated in house-made sauce. Clean interior and friendly service make it perfect for family gatherings. Lunch specials are available from 11:00-14:00.\"\n\n**ì¹´í…Œê³ ë¦¬ ì„ íƒ ê¸°ì¤€:**\n- cuisineì´ ì‚¼ê³„íƒ•/ëƒ‰ë©´/ë¶ˆê³ ê¸°/ê°ˆë¹„/ì°Œê°œ/êµ­ë°¥/ìœ¡íšŒ/í•œì •ì‹ â†’ \"í•œì‹\"\n- cuisineì´ ì´ˆë°¥/ë¼ë©˜/ìš°ë™/ì‚¬ì‹œë¯¸ â†’ \"ì¼ì‹\"\n- cuisineì´ ì§œì¥ë©´/ì§¬ë½•/íƒ•ìˆ˜ìœ¡ â†’ \"ì¤‘ì‹\"\n- cuisineì´ ìŠ¤í…Œì´í¬/íŒŒìŠ¤íƒ€/í”¼ì/ë¦¬ì¡°ë˜ â†’ \"ì–‘ì‹\"\n- cuisineì´ ì»¤í”¼/ì¼€ì´í¬/ë””ì €íŠ¸/ë² ì´ì»¤ë¦¬ â†’ \"ì¹´í˜Â·ë””ì €íŠ¸\"\n- cuisineì´ ë–¡ë³¶ì´/ê¹€ë°¥/ë¶„ì‹ â†’ \"í•œì‹\"\n- ê¸°íƒ€ â†’ \"ê¸°íƒ€\"\n\n**priceRange ê²°ì • ê¸°ì¤€:**\n- í•œì •ì‹/ì¼ì‹/ê³ ê¸‰ í•œì‹ â†’ \"3\" ë˜ëŠ” \"4\"\n- ì¼ë°˜ í•œì‹ë‹¹/ì¤‘ì‹ë‹¹ â†’ \"2\"\n- ë¶„ì‹/ê¹€ë°¥/êµ­ë°¥ â†’ \"1\"\n- ì¹´í˜/ë””ì €íŠ¸ â†’ \"2\"\n\n**ì¤‘ìš” ê·œì¹™:**\n- \"ì˜ˆìƒ\", \"ì•„ë§ˆë„\", \"ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤\" ê°™ì€ ë¶ˆí™•ì‹¤í•œ í‘œí˜„ ê¸ˆì§€\n- ì‹¤ì œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì‚¬ì‹¤ë§Œ ì‘ì„± (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)\n- ëª¨ë“  í•„ë“œ í•„ìˆ˜ (null ê°€ëŠ¥: openHours, phone)\n- priceRangeëŠ” ë°˜ë“œì‹œ ë¬¸ìì—´ \"1\", \"2\", \"3\", \"4\"\n- imageUrlì€ ë°˜ë“œì‹œ ì œê³µ (ê¸°ë³¸ê°’ ì‚¬ìš©)\n- qualityScoreëŠ” ì‹œìŠ¤í…œì—ì„œ ìë™ ê³„ì‚°í•˜ë¯€ë¡œ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\n- JSONë§Œ ë°˜í™˜ (ì„¤ëª… ì—†ì´)\n\"\"\"\n            \n            response = await self.model.generate_content_async(prompt)\n            result_text = response.text.strip()\n            \n            # JSON íŒŒì‹±\n            if \"```json\" in result_text:\n                result_text = result_text.split(\"```json\")[1].split(\"```\")[0].strip()\n            elif \"```\" in result_text:\n                result_text = result_text.split(\"```\")[1].split(\"```\")[0].strip()\n            \n            refined = json.loads(result_text)\n            \n            # âœ… Phase 2: ì¸ê¸°ì§€ìˆ˜ ê³„ì‚°\n            # í˜„ì¬ëŠ” raw_dataì— í‰ì /ë¦¬ë·°ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ê³„ì‚° (ë‚˜ì¤‘ì— ì›¹ íŒŒì‹±ìœ¼ë¡œ ì—…ë°ì´íŠ¸)\n            naver_rating = raw_data.get('naver_rating', 0.0)\n            naver_review_count = raw_data.get('naver_review_count', 0)\n            google_rating = raw_data.get('google_rating', 0.0)\n            google_review_count = raw_data.get('google_review_count', 0)\n            \n            popularity_score, popularity_tier = PopularityCalculator.calculate_with_tier(\n                naver_rating=naver_rating,\n                naver_review_count=naver_review_count,\n                google_rating=google_rating,\n                google_review_count=google_review_count\n            )\n            \n            # ì¸ê¸°ì§€ìˆ˜ ì •ë³´ ì¶”ê°€\n            refined['naver_rating'] = naver_rating\n            refined['naver_review_count'] = naver_review_count\n            refined['google_rating'] = google_rating\n            refined['google_review_count'] = google_review_count\n            refined['popularity_score'] = popularity_score\n            refined['popularity_tier'] = popularity_tier\n            \n            self.logger.info(\n                f\"Refined restaurant: {refined.get('name')} \"\n                f\"(Popularity: {popularity_score:.1f}/{popularity_tier})\"\n            )\n            return refined\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to refine data: {e}\")\n            return raw_data\n    \n    async def match_restaurants(\n        self,\n        naver_data: Dict[str, Any],\n        google_data: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"ë„¤ì´ë²„ì™€ êµ¬ê¸€ ë°ì´í„°ê°€ ê°™ì€ ì—…ì²´ì¸ì§€ íŒë‹¨\"\"\"\n        try:\n            prompt = f\"\"\"\në‹¤ìŒ ë‘ ë°ì´í„°ê°€ ê°™ì€ ì‹ë‹¹ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.\n\në„¤ì´ë²„ ë°ì´í„°:\n- ì´ë¦„: {naver_data.get('name')}\n- ì£¼ì†Œ: {naver_data.get('address')}\n- ì „í™”ë²ˆí˜¸: {naver_data.get('phone')}\n\nêµ¬ê¸€ ë°ì´í„°:\n- ì´ë¦„: {google_data.get('name')}\n- ì£¼ì†Œ: {google_data.get('address')}\n- ì „í™”ë²ˆí˜¸: {google_data.get('phone')}\n\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ íŒë‹¨ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”:\n{{\n  \"is_match\": true ë˜ëŠ” false,\n  \"confidence\": 0.0-1.0 (í™•ì‹ ë„),\n  \"reason\": \"íŒë‹¨ ì´ìœ \"\n}}\n\níŒë‹¨ ê¸°ì¤€:\n- ì´ë¦„ì´ ìœ ì‚¬í•˜ê±°ë‚˜ ë²ˆì—­ ê´€ê³„\n- ì£¼ì†Œê°€ ê°™ì€ ê±´ë¬¼/ë™ì¼ ì§€ì—­\n- ì „í™”ë²ˆí˜¸ ì¼ì¹˜\n- ì¢…í•© íŒë‹¨ìœ¼ë¡œ í™•ì‹ ë„ ê³„ì‚°\n\nJSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.\n\"\"\"\n            \n            response = await self.model.generate_content_async(prompt)\n            result_text = response.text.strip()\n            \n            if \"```json\" in result_text:\n                result_text = result_text.split(\"```json\")[1].split(\"```\")[0].strip()\n            elif \"```\" in result_text:\n                result_text = result_text.split(\"```\")[1].split(\"```\")[0].strip()\n            \n            result = json.loads(result_text)\n            self.logger.info(f\"Match result: {result.get('is_match')} ({result.get('confidence')})\")\n            return result\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to match restaurants: {e}\")\n            return {\n                \"is_match\": False,\n                \"confidence\": 0.0,\n                \"reason\": f\"Error: {str(e)}\"\n            }\n    \n    async def calculate_quality_score(self, restaurant: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)\"\"\"\n        score = 0\n        details = {}\n        \n        # GPS ì¢Œí‘œ (20ì )\n        if restaurant.get(\"latitude\") and restaurant.get(\"longitude\"):\n            score += 20\n            details[\"gps\"] = 20\n        \n        # ë¦¬ë·° (20ì )\n        review_count = restaurant.get(\"review_count\", 0)\n        if review_count >= 10:\n            score += 20\n            details[\"reviews\"] = 20\n        elif review_count > 0:\n            review_score = min(review_count * 2, 20)\n            score += review_score\n            details[\"reviews\"] = review_score\n        \n        # ì´ë¯¸ì§€ (20ì )\n        if restaurant.get(\"image_url\") or restaurant.get(\"image_urls\"):\n            score += 20\n            details[\"images\"] = 20\n        \n        # ì˜ì—…ì‹œê°„ (20ì )\n        if restaurant.get(\"open_hours\"):\n            score += 20\n            details[\"hours\"] = 20\n        \n        # ë©”ë‰´ ì •ë³´ (20ì )\n        if restaurant.get(\"menu_summary\"):\n            score += 20\n            details[\"menu\"] = 20\n        \n        return {\n            \"quality_score\": score,\n            \"quality_details\": details\n        }\n    \n    async def generate_target_keywords(self, region: str, count: int = 50) -> List[str]:\n        \"\"\"AIê°€ íƒ€ê²Ÿ í‚¤ì›Œë“œ ìƒì„±\"\"\"\n        try:\n            prompt = f\"\"\"\nì„œìš¸ {region} ì§€ì—­ì—ì„œ ì™¸êµ­ì¸ ê´€ê´‘ê°ì´ ì¢‹ì•„í•  ë§Œí•œ í•œì‹ë‹¹ì„ ì°¾ê¸° ìœ„í•œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ {count}ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.\n\nì¡°ê±´:\n- ë‹¤ì–‘í•œ ìŒì‹ ì¹´í…Œê³ ë¦¬ í¬í•¨ (ëƒ‰ë©´, ì‚¼ê²¹ì‚´, ë¶ˆê³ ê¸°, ì°Œê°œ, í•œì •ì‹ ë“±)\n- ìœ ëª… ê´€ê´‘ì§€ ì£¼ë³€\n- ì¸ê¸° ë§›ì§‘ ê±°ë¦¬\n\nJSON ë°°ì—´ë¡œë§Œ ë°˜í™˜:\n[\"í‚¤ì›Œë“œ1\", \"í‚¤ì›Œë“œ2\", ...]\n\nì˜ˆì‹œ:\n[\"ê°•ë‚¨ ëƒ‰ë©´\", \"ì´íƒœì› ì‚¼ê²¹ì‚´\", \"ëª…ë™ í•œì •ì‹\", ...]\n\"\"\"\n            \n            response = await self.model.generate_content_async(prompt)\n            result_text = response.text.strip()\n            \n            if \"```json\" in result_text:\n                result_text = result_text.split(\"```json\")[1].split(\"```\")[0].strip()\n            elif \"```\" in result_text:\n                result_text = result_text.split(\"```\")[1].split(\"```\")[0].strip()\n            \n            keywords = json.loads(result_text)\n            self.logger.info(f\"Generated {len(keywords)} target keywords\")\n            return keywords\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to generate keywords: {e}\")\n            return []\n","size_bytes":11707},"data-hub/src/scrapers/naver_maps_api.py":{"content":"\"\"\"\nNaver Maps API Scraper\në„¤ì´ë²„ ì§€ì—­ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•œ ë ˆìŠ¤í† ë‘ ë°ì´í„° ìˆ˜ì§‘\n\nPhase 1 ì—…ê·¸ë ˆì´ë“œ:\n- PlaceID ì¶”ì¶œ\n- ì¹´í…Œê³ ë¦¬ ìƒì„¸ íŒŒì‹±\n- ê¸°ë³¸ ì •ë³´ ê°•í™”\n\"\"\"\nimport httpx\nimport uuid\nimport re\nfrom typing import List, Dict, Any, Optional\nfrom loguru import logger\n\nfrom config import settings\nfrom src.database.connection import db_session\nfrom src.database.models import RawRestaurantData\n\n\nclass NaverMapsScraper:\n    \"\"\"ë„¤ì´ë²„ Maps API ìŠ¤í¬ë˜í¼\"\"\"\n    \n    def __init__(self):\n        if not settings.naver_client_id or not settings.naver_client_secret:\n            raise ValueError(\"NAVER_CLIENT_ID and NAVER_CLIENT_SECRET must be set\")\n        \n        self.client_id = settings.naver_client_id\n        self.client_secret = settings.naver_client_secret\n        # ë„¤ì´ë²„ ê²€ìƒ‰ API (Local) ì‚¬ìš©\n        self.base_url = \"https://openapi.naver.com/v1/search/local.json\"\n        self.logger = logger.bind(scraper=\"naver_maps\")\n    \n    def extract_place_id(self, link: str) -> Optional[str]:\n        \"\"\"\n        ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ë§í¬ì—ì„œ PlaceID ì¶”ì¶œ\n        \n        Args:\n            link: ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ URL (ì˜ˆ: https://pcmap.place.naver.com/restaurant/1234567890)\n        \n        Returns:\n            PlaceID (ì˜ˆ: 1234567890) ë˜ëŠ” None\n        \"\"\"\n        if not link:\n            return None\n        \n        # URL íŒ¨í„´: https://pcmap.place.naver.com/{type}/{place_id}\n        match = re.search(r'/(\\d+)$', link)\n        if match:\n            return match.group(1)\n        \n        return None\n    \n    def parse_category(self, category: str) -> Dict[str, Any]:\n        \"\"\"\n        ë„¤ì´ë²„ ì¹´í…Œê³ ë¦¬ ë¬¸ìì—´ íŒŒì‹±\n        \n        Args:\n            category: \"ìŒì‹ì >í•œì‹>ìœ¡ë¥˜,ê³ ê¸°ìš”ë¦¬\" í˜•ì‹\n        \n        Returns:\n            {\n                \"main\": \"ìŒì‹ì \",\n                \"sub\": \"í•œì‹\",\n                \"detail\": \"ìœ¡ë¥˜,ê³ ê¸°ìš”ë¦¬\",\n                \"cuisines\": [\"ìœ¡ë¥˜\", \"ê³ ê¸°ìš”ë¦¬\"]\n            }\n        \"\"\"\n        if not category:\n            return {}\n        \n        parts = category.split('>')\n        result = {\n            \"main\": parts[0] if len(parts) > 0 else None,\n            \"sub\": parts[1] if len(parts) > 1 else None,\n            \"detail\": parts[2] if len(parts) > 2 else None,\n            \"cuisines\": parts[2].split(',') if len(parts) > 2 else []\n        }\n        \n        return result\n    \n    async def search_restaurants(\n        self,\n        query: str = \"í™ëŒ€ í•œì‹\",\n        coordinate: str = \"126.9214,37.5563\",  # í™ëŒ€ì…êµ¬ì—­ ì¢Œí‘œ (ì‚¬ìš© ì•ˆí•¨)\n        display: int = 100\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        ë„¤ì´ë²„ ê²€ìƒ‰ API (Local) í˜¸ì¶œ\n        \n        Args:\n            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ê¸°ë³¸: \"í™ëŒ€ í•œì‹\")\n            coordinate: ì¤‘ì‹¬ ì¢Œí‘œ (ì‚¬ìš© ì•ˆí•¨)\n            display: ë°˜í™˜ ê°œìˆ˜ (ìµœëŒ€ 100)\n        \n        Returns:\n            ë ˆìŠ¤í† ë‘ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n        \"\"\"\n        try:\n            headers = {\n                \"X-Naver-Client-Id\": self.client_id,\n                \"X-Naver-Client-Secret\": self.client_secret\n            }\n            \n            params = {\n                \"query\": query,\n                \"display\": min(display, 100),  # ìµœëŒ€ 100ê°œ ì œí•œ\n                \"start\": 1,\n                \"sort\": \"random\"\n            }\n            \n            self.logger.info(f\"Searching: {query} (display={display})\")\n            \n            async with httpx.AsyncClient() as client:\n                response = await client.get(\n                    self.base_url,\n                    headers=headers,\n                    params=params,\n                    timeout=30.0\n                )\n                response.raise_for_status()\n                \n                data = response.json()\n                places = data.get(\"items\", [])\n                \n                self.logger.info(f\"Found {len(places)} places\")\n                return places\n        \n        except httpx.HTTPStatusError as e:\n            self.logger.error(f\"HTTP error: {e.response.status_code} - {e.response.text}\")\n            raise\n        except Exception as e:\n            self.logger.error(f\"Failed to search: {e}\")\n            raise\n    \n    def save_to_database(self, places: List[Dict[str, Any]]) -> int:\n        \"\"\"\n        ë„¤ì´ë²„ API ê²°ê³¼ë¥¼ raw_restaurants í…Œì´ë¸”ì— ì €ì¥ (Phase 1 ì—…ê·¸ë ˆì´ë“œ)\n        \n        Args:\n            places: ë„¤ì´ë²„ API ì‘ë‹µ ë°ì´í„°\n        \n        Returns:\n            ì €ì¥ëœ ë ˆìŠ¤í† ë‘ ìˆ˜\n        \"\"\"\n        saved_count = 0\n        \n        with db_session() as db:\n            for place in places:\n                try:\n                    # ë„¤ì´ë²„ Local API ì‘ë‹µì—ì„œ titleì— HTML íƒœê·¸ ì œê±°\n                    title = place.get(\"title\", \"\").replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n                    \n                    # âœ… PlaceID ì¶”ì¶œ (Phase 1)\n                    link = place.get(\"link\", \"\")\n                    place_id = self.extract_place_id(link)\n                    \n                    # ì¤‘ë³µ ì²´í¬ (PlaceID ë˜ëŠ” link ê¸°ì¤€)\n                    source_id = place_id if place_id else link if link else title\n                    existing = db.query(RawRestaurantData).filter(\n                        RawRestaurantData.source == \"naver\",\n                        RawRestaurantData.source_id == source_id\n                    ).first()\n                    \n                    if existing:\n                        self.logger.debug(f\"Skipping duplicate: {title}\")\n                        continue\n                    \n                    # ì¢Œí‘œ ì¶”ì¶œ (mapx, mapyë¥¼ ê²½ë„/ìœ„ë„ë¡œ ë³€í™˜)\n                    # ë„¤ì´ë²„ëŠ” ì¹´í… ì¢Œí‘œê³„ë¥¼ ì‚¬ìš© (mapx, mapy)\n                    # mapx = ê²½ë„ * 10^7, mapy = ìœ„ë„ * 10^7\n                    mapx = float(place.get(\"mapx\", 0)) if place.get(\"mapx\") else None\n                    mapy = float(place.get(\"mapy\", 0)) if place.get(\"mapy\") else None\n                    \n                    lng = mapx / 10000000.0 if mapx else None\n                    lat = mapy / 10000000.0 if mapy else None\n                    \n                    # âœ… ì¹´í…Œê³ ë¦¬ íŒŒì‹± (Phase 1)\n                    category = place.get(\"category\", \"\")\n                    parsed_category = self.parse_category(category)\n                    \n                    # âœ… ê°•í™”ëœ raw_data (Phase 1)\n                    raw_data = {\n                        **place,\n                        \"name\": title,\n                        \"lat\": lat,\n                        \"lng\": lng,\n                        \"latitude\": lat,\n                        \"longitude\": lng,\n                        \"naver_place_id\": place_id,  # âœ… PlaceID ì¶”ê°€\n                        \"parsed_category\": parsed_category,  # âœ… íŒŒì‹±ëœ ì¹´í…Œê³ ë¦¬\n                        \"phone\": place.get(\"telephone\", \"\"),  # âœ… ì „í™”ë²ˆí˜¸ ëª…ì‹œ\n                        \"description\": place.get(\"description\", \"\"),  # âœ… ì„¤ëª…\n                    }\n                    \n                    # DBì— ì €ì¥\n                    raw = RawRestaurantData(\n                        id=str(uuid.uuid4()),\n                        source=\"naver\",\n                        source_id=source_id,\n                        place_id=place_id,  # âœ… PlaceID ì»¬ëŸ¼ì— ì €ì¥\n                        raw_data=raw_data,\n                        status=\"pending\"\n                    )\n                    db.add(raw)\n                    saved_count += 1\n                    \n                    self.logger.debug(f\"Saved: {title} (PlaceID: {place_id})\")\n                \n                except Exception as e:\n                    self.logger.error(f\"Failed to save place: {e}\")\n                    continue\n            \n            db.commit()\n        \n        self.logger.info(f\"Saved {saved_count}/{len(places)} restaurants to database\")\n        return saved_count\n    \n    async def scrape(\n        self,\n        query: str = \"í™ëŒ€ í•œì‹\",\n        limit: int = 100\n    ) -> Dict[str, Any]:\n        \"\"\"\n        ì „ì²´ ìŠ¤í¬ë˜í•‘ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰\n        \n        Args:\n            query: ê²€ìƒ‰ ì¿¼ë¦¬\n            limit: ìˆ˜ì§‘í•  ìµœëŒ€ ê°œìˆ˜\n        \n        Returns:\n            ìŠ¤í¬ë˜í•‘ ê²°ê³¼ í†µê³„\n        \"\"\"\n        self.logger.info(f\"Starting scrape: {query} (limit={limit})\")\n        \n        # API í˜¸ì¶œ\n        places = await self.search_restaurants(query=query, display=limit)\n        \n        # DB ì €ì¥\n        saved_count = self.save_to_database(places)\n        \n        result = {\n            \"query\": query,\n            \"total_found\": len(places),\n            \"saved_count\": saved_count,\n            \"duplicate_count\": len(places) - saved_count\n        }\n        \n        self.logger.info(f\"Scrape completed: {result}\")\n        return result\n","size_bytes":8862},"data-hub/setup_new_repl.sh":{"content":"#!/bin/bash\n# Data Hub ìƒˆ Repl ì„¤ì • ìë™í™” ìŠ¤í¬ë¦½íŠ¸\n\nset -e  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë‹¨\n\necho \"ğŸš€ Restaurant Data Hub - ìƒˆ Repl ì„¤ì • ì‹œì‘\"\necho \"==========================================\"\necho \"\"\n\n# 1. í™˜ê²½ ë³€ìˆ˜ í™•ì¸\necho \"ğŸ“‹ 1/6: í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ì¤‘...\"\nrequired_vars=(\"DATABASE_URL\" \"GEMINI_API_KEY\" \"DATA_COLLECTION_API_KEY\" \"APIFY_API_TOKEN\")\nmissing_vars=()\n\nfor var in \"${required_vars[@]}\"; do\n    if [ -z \"${!var}\" ]; then\n        missing_vars+=(\"$var\")\n    fi\ndone\n\nif [ ${#missing_vars[@]} -ne 0 ]; then\n    echo \"âŒ ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤:\"\n    for var in \"${missing_vars[@]}\"; do\n        echo \"   - $var\"\n    done\n    echo \"\"\n    echo \"Replit Tools â†’ Secretsì—ì„œ ë‹¤ìŒ ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:\"\n    echo \"   DATABASE_URL, GEMINI_API_KEY, DATA_COLLECTION_API_KEY, APIFY_API_TOKEN\"\n    echo \"\"\n    exit 1\nfi\n\necho \"âœ… í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ì™„ë£Œ\"\necho \"\"\n\n# 2. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜\necho \"ğŸ“¦ 2/6: Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\"\nif [ ! -f \"requirements.txt\" ]; then\n    echo \"âŒ requirements.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!\"\n    exit 1\nfi\n\npip install -q -r requirements.txt\necho \"âœ… Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\"\necho \"\"\n\n# 3. .env íŒŒì¼ ìƒì„± (í™˜ê²½ ë³€ìˆ˜ ë°±ì—…)\necho \"ğŸ“ 3/6: .env íŒŒì¼ ìƒì„± ì¤‘...\"\ncat > .env << EOF\n# Database\nDATABASE_URL=${DATABASE_URL}\n\n# AI\nGEMINI_API_KEY=${GEMINI_API_KEY}\n\n# Scraping\nAPIFY_API_TOKEN=${APIFY_API_TOKEN}\n${OUTSCRAPER_API_KEY:+OUTSCRAPER_API_KEY=${OUTSCRAPER_API_KEY}}\n\n# API\nDATA_COLLECTION_API_KEY=${DATA_COLLECTION_API_KEY}\n\n# Session\n${SESSION_SECRET:+SESSION_SECRET=${SESSION_SECRET}}\nEOF\n\necho \"âœ… .env íŒŒì¼ ìƒì„± ì™„ë£Œ\"\necho \"\"\n\n# 4. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”\necho \"ğŸ—„ï¸  4/6: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...\"\npython3 cli.py init\necho \"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ\"\necho \"\"\n\n# 5. í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ì¶”ê°€\necho \"ğŸ¯ 5/6: í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ì¶”ê°€ ì¤‘...\"\npython3 cli.py add-target \"ê°•ë‚¨ ëƒ‰ë©´\" --region ê°•ë‚¨êµ¬ --priority 10\npython3 cli.py add-target \"ì´íƒœì› í•œì •ì‹\" --region ìš©ì‚°êµ¬ --priority 8\necho \"âœ… í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ì¶”ê°€ ì™„ë£Œ\"\necho \"\"\n\n# 6. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸\necho \"âœ… 6/6: ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸\"\necho \"\"\npython3 cli.py\necho \"\"\n\n# ì™„ë£Œ ë©”ì‹œì§€\necho \"==========================================\"\necho \"ğŸ‰ Data Hub ì„¤ì • ì™„ë£Œ!\"\necho \"==========================================\"\necho \"\"\necho \"ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ API ì„œë²„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\"\necho \"\"\necho \"  python3 -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload\"\necho \"\"\necho \"ì ‘ì† URL:\"\necho \"  - API: https://[your-repl].replit.dev:8000\"\necho \"  - ë¬¸ì„œ: https://[your-repl].replit.dev:8000/docs\"\necho \"\"\n","size_bytes":2676},"data-hub/src/scrapers/google.py":{"content":"\"\"\"\nGoogle Maps Scraper using Outscraper\n\"\"\"\nfrom typing import List, Optional, Dict, Any\nimport uuid\nimport httpx\nfrom loguru import logger\n\nfrom src.scrapers.base import BaseScraper, ScrapedRestaurant\nfrom config import settings\n\n\nclass GoogleMapsScraper(BaseScraper):\n    \"\"\"êµ¬ê¸€ë§µìŠ¤ ìŠ¤í¬ë˜í¼ (Outscraper API ì‚¬ìš©)\"\"\"\n    \n    BASE_URL = \"https://api.outscraper.com\"\n    \n    def __init__(self):\n        super().__init__(api_key=settings.outscraper_api_key)\n        if not self.api_key:\n            raise ValueError(\"OUTSCRAPER_API_KEY not set\")\n        \n        self.headers = {\"X-API-KEY\": self.api_key}\n    \n    async def search(\n        self,\n        keyword: str,\n        region: Optional[str] = None,\n        limit: int = 50\n    ) -> List[ScrapedRestaurant]:\n        \"\"\"êµ¬ê¸€ë§µìŠ¤ ê²€ìƒ‰\"\"\"\n        try:\n            search_query = f\"{keyword} {region}\" if region else keyword\n            \n            # í•œêµ­ ì§€ì—­ ì§€ì •\n            if \"korea\" not in search_query.lower() and \"í•œêµ­\" not in search_query:\n                search_query += \" South Korea\"\n            \n            self.logger.info(f\"Searching Google Maps: {search_query}\")\n            \n            async with httpx.AsyncClient() as client:\n                response = await client.get(\n                    f\"{self.BASE_URL}/maps/search-v3\",\n                    headers=self.headers,\n                    params={\n                        \"query\": search_query,\n                        \"limit\": limit,\n                        \"language\": \"ko\",\n                        \"region\": \"KR\",\n                    },\n                    timeout=60.0,\n                )\n                response.raise_for_status()\n                data = response.json()\n            \n            results = []\n            for item in data.get(\"data\", []):\n                restaurant = self._parse_google_item(item)\n                if restaurant:\n                    results.append(restaurant)\n            \n            self.logger.info(f\"Found {len(results)} restaurants from Google Maps\")\n            return results\n            \n        except Exception as e:\n            self.logger.error(f\"Google Maps search failed: {e}\")\n            return []\n    \n    async def get_details(self, source_id: str) -> ScrapedRestaurant:\n        \"\"\"êµ¬ê¸€ í”Œë ˆì´ìŠ¤ ìƒì„¸ ì •ë³´\"\"\"\n        try:\n            async with httpx.AsyncClient() as client:\n                response = await client.get(\n                    f\"{self.BASE_URL}/maps/place-info\",\n                    headers=self.headers,\n                    params={\"place_id\": source_id},\n                    timeout=30.0,\n                )\n                response.raise_for_status()\n                data = response.json()\n            \n            if data.get(\"data\"):\n                return self._parse_google_item(data[\"data\"][0])\n            \n            raise ValueError(f\"No details found for {source_id}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to get Google details: {e}\")\n            raise\n    \n    def _parse_google_item(self, item: Dict[str, Any]) -> ScrapedRestaurant:\n        \"\"\"Outscraper ì‘ë‹µì„ ScrapedRestaurantë¡œ ë³€í™˜\"\"\"\n        try:\n            place_id = item.get(\"place_id\") or item.get(\"google_id\") or str(uuid.uuid4())\n            \n            # GPS ì¢Œí‘œ íŒŒì‹±\n            lat = None\n            lng = None\n            if \"latitude\" in item and \"longitude\" in item:\n                lat = float(item[\"latitude\"])\n                lng = float(item[\"longitude\"])\n            elif \"location\" in item:\n                lat = float(item[\"location\"].get(\"lat\", 0))\n                lng = float(item[\"location\"].get(\"lng\", 0))\n            \n            return ScrapedRestaurant(\n                source=\"google\",\n                source_id=place_id,\n                source_url=item.get(\"url\", \"\"),\n                raw_data=item,\n                name=item.get(\"name\"),\n                address=item.get(\"address\") or item.get(\"full_address\"),\n                phone=item.get(\"phone\"),\n                rating=float(item.get(\"rating\", 0)) if item.get(\"rating\") else None,\n                review_count=int(item.get(\"reviews\", 0)) if item.get(\"reviews\") else None,\n                latitude=lat,\n                longitude=lng,\n            )\n        except Exception as e:\n            self.logger.error(f\"Failed to parse Google item: {e}\")\n            return None\n","size_bytes":4413},"data-hub/src/processors/popularity_calculator.py":{"content":"\"\"\"\nPopularity Score Calculator\në„¤ì´ë²„/êµ¬ê¸€ í‰ì  ë° ë¦¬ë·°ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¢…í•© ì¸ê¸°ì§€ìˆ˜(0-100) ê³„ì‚°\n\"\"\"\nfrom typing import Optional, Tuple\nfrom loguru import logger\n\n\nclass PopularityCalculator:\n    \"\"\"\n    ë„¤ì´ë²„/êµ¬ê¸€ í‰ì ê³¼ ë¦¬ë·°ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¢…í•© ì¸ê¸°ì§€ìˆ˜(0-100) ê³„ì‚°\n    \n    ê³„ì‚° ê³µì‹:\n    1. í‰ì  ì ìˆ˜ (50ì  ë§Œì )\n       - ë„¤ì´ë²„ í‰ì : (naver_rating / 5.0) * 25ì \n       - êµ¬ê¸€ í‰ì : (google_rating / 5.0) * 25ì \n    \n    2. ë¦¬ë·°ìˆ˜ ì ìˆ˜ (50ì  ë§Œì )\n       - ë„¤ì´ë²„ ë¦¬ë·°ìˆ˜: min(naver_review_count / 100, 1.0) * 25ì \n       - êµ¬ê¸€ ë¦¬ë·°ìˆ˜: min(google_review_count / 100, 1.0) * 25ì \n    \n    3. ìµœì¢… ì ìˆ˜ = í‰ì  ì ìˆ˜ + ë¦¬ë·°ìˆ˜ ì ìˆ˜ (0-100)\n    \"\"\"\n    \n    # ë¦¬ë·°ìˆ˜ ê¸°ì¤€ì  (100ê°œ ì´ìƒì€ ë§Œì  ì²˜ë¦¬)\n    REVIEW_THRESHOLD = 100\n    \n    @staticmethod\n    def calculate_popularity_score(\n        naver_rating: Optional[float] = None,\n        naver_review_count: Optional[int] = None,\n        google_rating: Optional[float] = None,\n        google_review_count: Optional[int] = None\n    ) -> float:\n        \"\"\"\n        ì¸ê¸°ì§€ìˆ˜ ê³„ì‚°\n        \n        Args:\n            naver_rating: ë„¤ì´ë²„ í‰ì  (0.0-5.0)\n            naver_review_count: ë„¤ì´ë²„ ë¦¬ë·° ê°œìˆ˜\n            google_rating: êµ¬ê¸€ í‰ì  (0.0-5.0)\n            google_review_count: êµ¬ê¸€ ë¦¬ë·° ê°œìˆ˜\n        \n        Returns:\n            ì¸ê¸°ì§€ìˆ˜ (0.0-100.0)\n        \n        Examples:\n            >>> # ë†’ì€ ì¸ê¸° ë ˆìŠ¤í† ë‘\n            >>> calculate_popularity_score(4.5, 1200, 4.3, 856)\n            94.0\n            \n            >>> # ì‹ ê·œ ë ˆìŠ¤í† ë‘\n            >>> calculate_popularity_score(4.0, 5, None, None)\n            21.3\n            \n            >>> # ë°ì´í„° ì—†ìŒ\n            >>> calculate_popularity_score(None, None, None, None)\n            0.0\n        \"\"\"\n        # None ê°’ ì²˜ë¦¬\n        naver_rating = naver_rating or 0.0\n        naver_review_count = naver_review_count or 0\n        google_rating = google_rating or 0.0\n        google_review_count = google_review_count or 0\n        \n        # í‰ì  ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 50ì )\n        rating_score = 0.0\n        if naver_rating > 0:\n            rating_score += (naver_rating / 5.0) * 25\n        if google_rating > 0:\n            rating_score += (google_rating / 5.0) * 25\n        \n        # ë¦¬ë·°ìˆ˜ ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 50ì )\n        # ë¦¬ë·° 100ê°œë¥¼ ê¸°ì¤€ì ìœ¼ë¡œ ì„¤ì • (100ê°œ ì´ìƒì€ ë§Œì )\n        review_score = 0.0\n        if naver_review_count > 0:\n            review_score += min(naver_review_count / PopularityCalculator.REVIEW_THRESHOLD, 1.0) * 25\n        if google_review_count > 0:\n            review_score += min(google_review_count / PopularityCalculator.REVIEW_THRESHOLD, 1.0) * 25\n        \n        # ìµœì¢… ì ìˆ˜ (0-100)\n        final_score = round(rating_score + review_score, 1)\n        \n        logger.debug(\n            f\"Popularity calculation: \"\n            f\"Naver({naver_rating:.1f}/{naver_review_count}) \"\n            f\"Google({google_rating:.1f}/{google_review_count}) \"\n            f\"â†’ Rating:{rating_score:.1f} + Review:{review_score:.1f} = {final_score}\"\n        )\n        \n        return final_score\n    \n    @staticmethod\n    def get_popularity_tier(score: float) -> str:\n        \"\"\"\n        ì¸ê¸°ì§€ìˆ˜ ë“±ê¸‰ ë¶„ë¥˜\n        \n        Args:\n            score: ì¸ê¸°ì§€ìˆ˜ (0-100)\n        \n        Returns:\n            ë“±ê¸‰ ë¬¸ìì—´\n            - top_rated: 90-100 (ìµœê³  ì¸ê¸°)\n            - highly_popular: 70-89 (ë†’ì€ ì¸ê¸°)\n            - popular: 50-69 (ì¸ê¸°)\n            - average: 30-49 (ë³´í†µ)\n            - new_or_limited: 0-29 (ì‹ ê·œ/ì •ë³´ ë¶€ì¡±)\n        \n        Examples:\n            >>> get_popularity_tier(95.0)\n            'top_rated'\n            \n            >>> get_popularity_tier(75.0)\n            'highly_popular'\n            \n            >>> get_popularity_tier(55.0)\n            'popular'\n            \n            >>> get_popularity_tier(40.0)\n            'average'\n            \n            >>> get_popularity_tier(10.0)\n            'new_or_limited'\n        \"\"\"\n        if score >= 90:\n            return \"top_rated\"\n        elif score >= 70:\n            return \"highly_popular\"\n        elif score >= 50:\n            return \"popular\"\n        elif score >= 30:\n            return \"average\"\n        else:\n            return \"new_or_limited\"\n    \n    @staticmethod\n    def calculate_with_tier(\n        naver_rating: Optional[float] = None,\n        naver_review_count: Optional[int] = None,\n        google_rating: Optional[float] = None,\n        google_review_count: Optional[int] = None\n    ) -> Tuple[float, str]:\n        \"\"\"\n        ì¸ê¸°ì§€ìˆ˜ì™€ ë“±ê¸‰ì„ í•¨ê»˜ ê³„ì‚°\n        \n        Returns:\n            (ì¸ê¸°ì§€ìˆ˜, ë“±ê¸‰) íŠœí”Œ\n        \n        Examples:\n            >>> calculate_with_tier(4.5, 1200, 4.3, 856)\n            (94.0, 'top_rated')\n        \"\"\"\n        score = PopularityCalculator.calculate_popularity_score(\n            naver_rating, naver_review_count,\n            google_rating, google_review_count\n        )\n        tier = PopularityCalculator.get_popularity_tier(score)\n        \n        return score, tier\n    \n    @staticmethod\n    def get_tier_display_name(tier: str, lang: str = \"ko\") -> str:\n        \"\"\"\n        ë“±ê¸‰ì˜ í‘œì‹œ ì´ë¦„ ë°˜í™˜\n        \n        Args:\n            tier: ë“±ê¸‰ ì½”ë“œ\n            lang: ì–¸ì–´ ('ko' ë˜ëŠ” 'en')\n        \n        Returns:\n            í‘œì‹œ ì´ë¦„\n        \"\"\"\n        tier_names = {\n            \"top_rated\": {\"ko\": \"ìµœê³  ì¸ê¸°\", \"en\": \"Top Rated\"},\n            \"highly_popular\": {\"ko\": \"ë†’ì€ ì¸ê¸°\", \"en\": \"Highly Popular\"},\n            \"popular\": {\"ko\": \"ì¸ê¸°\", \"en\": \"Popular\"},\n            \"average\": {\"ko\": \"ë³´í†µ\", \"en\": \"Average\"},\n            \"new_or_limited\": {\"ko\": \"ì‹ ê·œ/ì •ë³´ ë¶€ì¡±\", \"en\": \"New/Limited Data\"}\n        }\n        \n        return tier_names.get(tier, {}).get(lang, tier)\n","size_bytes":5945},"data-hub/scheduler.py":{"content":"\"\"\"\n24/7 ìë™í™” ìŠ¤ì¼€ì¤„ëŸ¬\nRestaurant Data Hub Automated Scheduler\n\nì¼ì¼ ëª©í‘œ: 333ê°œ (ë„¤ì´ë²„ 33ê°œ + êµ¬ê¸€ 300ê°œ)\n\"\"\"\n\nimport asyncio\nimport schedule\nimport time\nfrom datetime import datetime\nfrom loguru import logger\nimport sys\n\nsys.path.insert(0, '/home/runner/workspace/data-hub')\n\nfrom src.scrapers.naver_maps_api import NaverMapsScraper\nfrom src.scrapers.google_places_api import GooglePlacesAPI\nfrom src.scrapers.apify_naver_scraper import ApifyNaverScraper\nfrom src.scrapers.place_id_loader import PlaceIDLoader\nfrom src.workflows.sync import SyncWorkflow\nfrom src.processors.gemini import GeminiProcessor\nfrom src.processors.quality_validator import QualityValidator\nfrom src.targeting.trends_analyzer import TrendsAnalyzer\nfrom src.targeting.query_generator import QueryGenerator\nfrom src.targeting.popularity_scorer import PopularityScorer\nfrom src.deduplication.service import DeduplicationService\nfrom src.governance.drive_backup import DriveBackupManager\nfrom src.database.connection import db_session, init_db\nfrom src.database.models import RawRestaurantData, ProcessedRestaurant, ScrapingTarget\nimport uuid\n\n\nlogger.add(\"logs/scheduler.log\", rotation=\"1 day\", retention=\"30 days\", level=\"INFO\")\nlogger.add(\"logs/scheduler_error.log\", rotation=\"1 day\", retention=\"30 days\", level=\"ERROR\")\n\n\nasync def generate_smart_queries_daily():\n    \"\"\"ìŠ¤ë§ˆíŠ¸ íƒ€ê²ŸíŒ…: ë§¤ì¼ 01:30 KSTì— ì™¸êµ­ì¸ ì¸ê¸°ë„ ê¸°ë°˜ 33ê°œ ì¿¼ë¦¬ ìƒì„±\"\"\"\n    logger.info(\"=\" * 70)\n    logger.info(\"ğŸ¯ Smart Targeting: Generating dynamic queries\")\n    logger.info(\"=\" * 70)\n    \n    try:\n        trends_analyzer = TrendsAnalyzer()\n        query_generator = QueryGenerator()\n        popularity_scorer = PopularityScorer()\n        \n        # 1. ì§€ì—­ë³„ ì¸ê¸°ë„ ë¶„ì„ (ìµœê·¼ 7ì¼)\n        logger.info(\"ğŸ“Š Step 1: Analyzing regional popularity...\")\n        top_regions = await trends_analyzer.get_top_regions(count=7, days=7)\n        \n        logger.info(f\"  âœ“ Top 7 regions identified:\")\n        for i, (region, score) in enumerate(top_regions, 1):\n            logger.info(f\"    {i}. {region}: {score:.1f}\")\n            popularity_scorer.update_history(region, score)\n        \n        # 2. ë™ì  ì¿¼ë¦¬ ìƒì„± (33ê°œ)\n        logger.info(\"ğŸ”§ Step 2: Generating 33 dynamic queries...\")\n        queries = await query_generator.generate_daily_queries(top_regions, target_count=33)\n        \n        # 3. ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°\n        diversity = query_generator.get_query_diversity_score(queries)\n        logger.info(f\"  âœ“ Query diversity: {diversity['diversity_score']:.1f}%\")\n        logger.info(f\"    - Unique regions: {diversity['unique_regions']}\")\n        logger.info(f\"    - Unique categories: {diversity['unique_categories']}\")\n        \n        # 4. DBì— ì €ì¥\n        logger.info(\"ğŸ’¾ Step 3: Saving to database...\")\n        with db_session() as db:\n            # ê¸°ì¡´ ìë™ ìƒì„± ì¿¼ë¦¬ ì‚­ì œ\n            deleted = db.query(ScrapingTarget).filter_by(created_by='auto').delete()\n            logger.info(f\"  - Deleted {deleted} old auto-generated queries\")\n            \n            # ìƒˆ ì¿¼ë¦¬ ì €ì¥\n            today_str = datetime.now().strftime('%Y%m%d')\n            for idx, query in enumerate(queries):\n                target = ScrapingTarget(\n                    id=f\"auto_{today_str}_{idx:03d}\",\n                    keyword=query,\n                    region=query.split()[0] if query.split() else \"\",\n                    priority=5,\n                    status='active',\n                    created_by='auto'\n                )\n                db.add(target)\n            \n            db.commit()\n            logger.info(f\"  âœ“ Saved {len(queries)} new queries\")\n        \n        # 5. íˆìŠ¤í† ë¦¬ ì €ì¥\n        popularity_scorer.save_to_json(\"logs/popularity_history.json\")\n        \n        logger.info(\"=\" * 70)\n        logger.info(f\"âœ… Smart Targeting completed: {len(queries)} queries ready\")\n        logger.info(\"=\" * 70)\n        \n        return len(queries)\n        \n    except Exception as e:\n        logger.error(f\"âŒ Smart Targeting failed: {e}\")\n        logger.exception(e)\n        return 0\n\n\nasync def scrape_naver_daily():\n    \"\"\"Apify Naver Map Scraperë¡œ ë§¤ì¼ 33ê°œ ìˆ˜ì§‘ (ë©”ë‰´/ì „í™”ë²ˆí˜¸ í¬í•¨)\"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"ğŸ” Starting Naver Maps scraping with Apify (Daily target: 33)\")\n    logger.info(\"=\" * 60)\n    \n    try:\n        apify = ApifyNaverScraper()\n        \n        # DBì—ì„œ ìë™ ìƒì„±ëœ ì¿¼ë¦¬ ê°€ì ¸ì˜¤ê¸° (ìŠ¤ë§ˆíŠ¸ íƒ€ê²ŸíŒ…)\n        with db_session() as db:\n            auto_targets = db.query(ScrapingTarget).filter_by(\n                created_by='auto',\n                status='active'\n            ).limit(33).all()\n            \n            if auto_targets:\n                search_queries = [t.keyword for t in auto_targets]\n                logger.info(f\"  âœ“ Using {len(search_queries)} smart-generated queries\")\n            else:\n                # Fallback: ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©\n                search_queries = [\n                    \"í™ëŒ€ í•œì‹\",\n                    \"ê°•ë‚¨ í•œì‹ë‹¹\",\n                    \"ëª…ë™ í•œì‹\",\n                    \"ì—¬ì˜ë„ ë§›ì§‘\",\n                    \"ì´íƒœì› í•œì‹\",\n                    \"ì„œìš¸ ì‚¼ê³„íƒ•\",\n                    \"ì„œìš¸ ë¶ˆê³ ê¸°\",\n                    \"ì„œìš¸ ë¹„ë¹”ë°¥\",\n                    \"ì„œìš¸ ê°ˆë¹„\",\n                    \"ì„œìš¸ ëƒ‰ë©´\",\n                    \"ì„œìš¸ ì°Œê°œ\",\n                ]\n                logger.warning(f\"  âš ï¸  No smart queries found, using {len(search_queries)} default queries\")\n        \n        total_saved = 0\n        for query in search_queries:\n            try:\n                # Apifyë¡œ ë ˆìŠ¤í† ë‘ ê²€ìƒ‰ (ë©”ë‰´/ì „í™”ë²ˆí˜¸ í¬í•¨)\n                results = await apify.search_restaurants([query], max_results=11)\n                \n                if not results:\n                    logger.warning(f\"  âš ï¸  {query}: No results\")\n                    continue\n                \n                # DBì— ì €ì¥\n                with db_session() as db:\n                    saved_count = 0\n                    for restaurant_data in results:\n                        try:\n                            # ì¤‘ë³µ ì²´í¬\n                            existing = db.query(RawRestaurantData).filter_by(\n                                name=restaurant_data.get('name'),\n                                address=restaurant_data.get('address')\n                            ).first()\n                            \n                            if existing:\n                                continue\n                            \n                            # Raw ë°ì´í„° ì €ì¥\n                            raw = RawRestaurantData(\n                                id=str(uuid.uuid4()),\n                                source='apify_naver',\n                                raw_data=restaurant_data,\n                                status='pending'\n                            )\n                            db.add(raw)\n                            saved_count += 1\n                            \n                        except Exception as e:\n                            logger.error(f\"    âŒ Error saving {restaurant_data.get('name')}: {e}\")\n                    \n                    db.commit()\n                    total_saved += saved_count\n                    logger.info(f\"  âœ“ {query}: {saved_count} saved (with menu/phone)\")\n                    \n                if total_saved >= 33:\n                    break\n                \n                await asyncio.sleep(2)\n                \n            except Exception as e:\n                logger.error(f\"  âŒ Error with query '{query}': {e}\")\n        \n        logger.info(f\"âœ… Naver scraping completed: {total_saved} restaurants saved (with full data)\")\n        return total_saved\n        \n    except Exception as e:\n        logger.error(f\"âŒ Naver scraping failed: {e}\")\n        return 0\n\n\nasync def process_pending_daily():\n    \"\"\"Gemini AIë¡œ pending ë°ì´í„° ì •ì œ (ë°°ì¹˜ ì²˜ë¦¬)\"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"ğŸ¤– Starting Gemini AI processing\")\n    logger.info(\"=\" * 60)\n    \n    try:\n        gemini = GeminiProcessor()\n        total_processed = 0\n        batch_size = 10\n        \n        while True:\n            with db_session() as db:\n                raw_batch = db.query(RawRestaurantData).filter(\n                    RawRestaurantData.status == 'pending'\n                ).limit(batch_size).all()\n                \n                if not raw_batch:\n                    break\n                \n                logger.info(f\"Processing batch of {len(raw_batch)} records...\")\n                batch_processed = 0\n                \n                for idx, raw in enumerate(raw_batch):\n                    try:\n                        if idx > 0 and idx % 5 == 0:\n                            await asyncio.sleep(10)\n                        \n                        max_retries = 3\n                        retry_count = 0\n                        refined = None\n                        \n                        while retry_count < max_retries and refined is None:\n                            try:\n                                refined = await gemini.refine_restaurant_data(raw.raw_data)\n                            except Exception as retry_error:\n                                error_msg = str(retry_error)\n                                if \"429\" in error_msg or \"quota\" in error_msg.lower():\n                                    retry_count += 1\n                                    if retry_count < max_retries:\n                                        wait_time = 60 * retry_count\n                                        logger.warning(f\"  âš ï¸ Rate limit! Retry in {wait_time}s (attempt {retry_count}/{max_retries})\")\n                                        await asyncio.sleep(wait_time)\n                                    else:\n                                        raise\n                                else:\n                                    raise\n                        \n                        if refined is None:\n                            raise Exception(\"Failed to refine data after retries\")\n                        \n                        quality = await gemini.calculate_quality_score(raw.raw_data)\n                        \n                        new_restaurant = ProcessedRestaurant(\n                            id=str(uuid.uuid4()),\n                            mapping_id=raw.id,\n                            name=refined.get('name', ''),\n                            name_en=refined.get('nameEn', ''),\n                            category=refined.get('category', 'í•œì‹'),\n                            cuisine=refined.get('cuisine', ''),\n                            district=refined.get('district', ''),\n                            address=refined.get('address', ''),\n                            address_en=refined.get('addressEn', ''),\n                            latitude=raw.raw_data.get('lat') or raw.raw_data.get('geometry', {}).get('location', {}).get('lat'),\n                            longitude=raw.raw_data.get('lng') or raw.raw_data.get('geometry', {}).get('location', {}).get('lng'),\n                            description=refined.get('description', ''),\n                            description_en=refined.get('descriptionEn', ''),\n                            price_range=str(refined.get('priceRange', 2)),\n                            phone=refined.get('phone', ''),\n                            rating=raw.raw_data.get('rating'),\n                            review_count=raw.raw_data.get('reviewCount') or raw.raw_data.get('user_ratings_total', 0),\n                            image_url=refined.get('imageUrl', 'https://via.placeholder.com/400x300?text=Restaurant'),\n                            open_hours=refined.get('openHours'),\n                            quality_score=quality.get('quality_score', 0),\n                            quality_details=quality.get('quality_details', {}),\n                            sync_status='pending'\n                        )\n                        db.add(new_restaurant)\n                        \n                        raw.status = 'processed'\n                        batch_processed += 1\n                        \n                    except Exception as e:\n                        raw.status = 'failed'\n                        raw.error_message = str(e)\n                        logger.error(f\"Failed to process {raw.id}: {e}\")\n                \n                db.commit()\n                total_processed += batch_processed\n                logger.info(f\"  âœ“ Batch completed: {batch_processed}/{len(raw_batch)} processed (Total: {total_processed})\")\n                \n                if len(raw_batch) < batch_size:\n                    break\n                \n                await asyncio.sleep(60)\n        \n        logger.info(f\"âœ… Processing completed: {total_processed} records\")\n        return total_processed\n            \n    except Exception as e:\n        logger.error(f\"âŒ Processing failed: {e}\")\n        return 0\n\n\nasync def enrich_with_google_ratings():\n    \"\"\"êµ¬ê¸€ í‰ì ìœ¼ë¡œ ê¸°ì¡´ ë ˆìŠ¤í† ë‘ ë³´ê°•\"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"â­ Starting Google ratings enrichment\")\n    logger.info(\"=\" * 60)\n    \n    try:\n        with db_session() as db:\n            restaurants_without_google = db.query(ProcessedRestaurant).filter(\n                ProcessedRestaurant.google_place_id == None\n            ).limit(33).all()\n            \n            if not restaurants_without_google:\n                logger.info(\"No restaurants need Google enrichment\")\n                return 0\n            \n            logger.info(f\"Found {len(restaurants_without_google)} restaurants to enrich\")\n            \n            google_api = GooglePlacesAPI()\n            enriched_count = 0\n            \n            for restaurant in restaurants_without_google:\n                try:\n                    place_data = await google_api.search_place(\n                        name=restaurant.name,\n                        address=restaurant.address or \"\"\n                    )\n                    \n                    if place_data:\n                        restaurant.google_place_id = place_data.get('place_id')\n                        restaurant.google_rating = place_data.get('rating')\n                        restaurant.google_review_count = place_data.get('user_ratings_total', 0)\n                        \n                        if place_data.get('image_urls'):\n                            restaurant.image_urls = place_data.get('image_urls')\n                            restaurant.image_url = place_data.get('image_url')\n                        \n                        from src.processors.popularity_calculator import PopularityCalculator\n                        popularity_score, popularity_tier = PopularityCalculator.calculate_with_tier(\n                            naver_rating=restaurant.naver_rating or 0,\n                            naver_review_count=restaurant.naver_review_count or 0,\n                            google_rating=restaurant.google_rating or 0,\n                            google_review_count=restaurant.google_review_count or 0\n                        )\n                        restaurant.popularity_score = popularity_score\n                        restaurant.popularity_tier = popularity_tier\n                        \n                        enriched_count += 1\n                        images_count = len(place_data.get('image_urls', []))\n                        logger.info(f\"  âœ“ Enriched: {restaurant.name} (Rating: {place_data.get('rating')}/5.0, Images: {images_count})\")\n                    \n                    await asyncio.sleep(0.2)\n                    \n                except Exception as e:\n                    logger.error(f\"Failed to enrich {restaurant.name}: {e}\")\n            \n            db.commit()\n            logger.info(f\"âœ… Google enrichment completed: {enriched_count} restaurants\")\n            return enriched_count\n            \n    except Exception as e:\n        logger.error(f\"âŒ Google enrichment failed: {e}\")\n        return 0\n\n\nasync def update_menus_with_apify():\n    \"\"\"Apifyë¡œ ë©”ë‰´ ë°ì´í„° ì—…ë°ì´íŠ¸ (ë°°ì¹˜ ì»¤ë°‹)\"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"ğŸ½ï¸  Starting menu update with Apify\")\n    logger.info(\"=\" * 60)\n    \n    try:\n        apify = ApifyNaverScraper()\n        total_updated = 0\n        batch_size = 10\n        \n        while True:\n            with db_session() as db:\n                # ë©”ë‰´ê°€ ì—†ëŠ” ë ˆìŠ¤í† ë‘ ì¡°íšŒ (ë°°ì¹˜ ë‹¨ìœ„)\n                restaurants_without_menu = db.query(ProcessedRestaurant).filter(\n                    ProcessedRestaurant.menu_summary == None\n                ).limit(batch_size).all()\n                \n                if not restaurants_without_menu:\n                    logger.info(\"All restaurants have menu data\")\n                    break\n                \n                logger.info(f\"Processing batch of {len(restaurants_without_menu)} restaurants...\")\n                batch_updated = 0\n                \n                for restaurant in restaurants_without_menu:\n                    try:\n                        # Apifyë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ (ë©”ë‰´ í¬í•¨)\n                        details = await apify.get_restaurant_details(\n                            restaurant_name=restaurant.name,\n                            address=restaurant.address\n                        )\n                        \n                        if details and (details.get('menus') or details.get('menu_items')):\n                            # ë©”ë‰´ ë°ì´í„° ì €ì¥\n                            menus = details.get('menus', []) or details.get('menu_items', [])\n                            \n                            # JSON ë°°ì—´ í˜•íƒœë¡œ ì €ì¥\n                            menu_list = []\n                            for menu in menus[:10]:  # ìµœëŒ€ 10ê°œ\n                                if isinstance(menu, dict):\n                                    menu_list.append({\n                                        \"name\": menu.get('name', ''),\n                                        \"price\": menu.get('price', '')\n                                    })\n                                elif isinstance(menu, str):\n                                    menu_list.append({\n                                        \"name\": menu,\n                                        \"price\": \"\"\n                                    })\n                            \n                            restaurant.menu_summary = menu_list\n                            batch_updated += 1\n                            logger.info(f\"  âœ“ Updated menu: {restaurant.name} ({len(menu_list)} items)\")\n                        else:\n                            logger.debug(f\"  âš ï¸  No menu found: {restaurant.name}\")\n                        \n                        await asyncio.sleep(2)  # Rate limiting\n                        \n                    except Exception as e:\n                        logger.error(f\"Failed to update menu for {restaurant.name}: {e}\")\n                \n                # ë°°ì¹˜ ì»¤ë°‹\n                db.commit()\n                total_updated += batch_updated\n                logger.info(f\"  âœ“ Batch committed: {batch_updated}/{len(restaurants_without_menu)} updated (Total: {total_updated})\")\n                \n                if len(restaurants_without_menu) < batch_size:\n                    break\n        \n        logger.info(f\"âœ… Menu update completed: {total_updated} restaurants\")\n        return total_updated\n            \n    except Exception as e:\n        logger.error(f\"âŒ Menu update failed: {e}\")\n        return 0\n\n\nasync def sync_daily():\n    \"\"\"ë©”ì¸ í”Œë«í¼ ë™ê¸°í™”\"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"ğŸ”„ Starting sync to í•œì‹ë‹¹ platform\")\n    logger.info(\"=\" * 60)\n    \n    try:\n        workflow = SyncWorkflow()\n        result = await workflow.sync_to_hansikdang()\n        logger.info(f\"âœ… Sync completed: {result}\")\n        return result\n        \n    except Exception as e:\n        logger.error(f\"âŒ Sync failed: {e}\")\n        return 0\n\n\nasync def deduplicate_daily():\n    \"\"\"ì¤‘ë³µ íƒì§€ ë° ìë™ ë³‘í•©\"\"\"\n    logger.info(\"=\" * 70)\n    logger.info(\"ğŸ” Starting daily duplicate detection and merging\")\n    logger.info(\"=\" * 70)\n    \n    try:\n        with db_session() as db:\n            service = DeduplicationService(\n                db=db,\n                name_threshold=90.0,\n                address_threshold=85.0,\n                distance_threshold_meters=100.0\n            )\n            \n            result = service.detect_and_merge_duplicates(\n                auto_merge=True,\n                merge_type='auto'\n            )\n            \n            logger.info(\"=\" * 70)\n            logger.info(\"âœ… Duplicate detection and merging completed\")\n            logger.info(f\"   Total restaurants: {result['total_restaurants']}\")\n            logger.info(f\"   Duplicate groups found: {result['duplicate_groups_found']}\")\n            logger.info(f\"   Merged groups: {result['merged_groups']}\")\n            logger.info(f\"   Total merged restaurants: {result['total_merged_restaurants']}\")\n            logger.info(\"=\" * 70)\n            \n            return result['merged_groups']\n        \n    except Exception as e:\n        logger.error(f\"âŒ Duplicate detection failed: {e}\")\n        logger.exception(e)\n        return 0\n\n\ndef log_statistics():\n    \"\"\"í†µê³„ ë¡œê¹… ë° ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§\"\"\"\n    try:\n        with db_session() as db:\n            total_raw = db.query(RawRestaurantData).count()\n            total_processed = db.query(ProcessedRestaurant).count()\n            total_synced = db.query(ProcessedRestaurant).filter(\n                ProcessedRestaurant.synced_to_hansikdang == True\n            ).count()\n            raw_pending = db.query(RawRestaurantData).filter(\n                RawRestaurantData.status == 'pending'\n            ).count()\n            \n            logger.info(\"=\" * 60)\n            logger.info(\"ğŸ“Š Hourly Statistics\")\n            logger.info(f\"  Total raw: {total_raw}\")\n            logger.info(f\"  Total processed: {total_processed}\")\n            logger.info(f\"  Total synced: {total_synced}\")\n            logger.info(f\"  Pending processing: {raw_pending}\")\n            logger.info(f\"  Daily target: 333\")\n            logger.info(\"=\" * 60)\n            \n            from src.monitoring.system_monitor import SystemMonitor\n            from src.monitoring.alert_manager import AlertManager\n            \n            monitor = SystemMonitor(db)\n            alert_manager = AlertManager(db)\n            \n            monitor.monitor_component(\n                component='database',\n                total_operations=total_processed,\n                successful_operations=total_synced,\n                failed_operations=0\n            )\n            \n            alerts = alert_manager.check_all_alerts()\n            \n            if alerts['total_alerts'] > 0:\n                logger.warning(f\"âš ï¸  Active alerts: {alerts['total_alerts']} (Critical: {alerts['critical']}, High: {alerts['high']})\")\n            \n    except Exception as e:\n        logger.error(f\"Failed to log statistics: {e}\")\n\n\ndef smart_targeting_job():\n    \"\"\"ìŠ¤ë§ˆíŠ¸ íƒ€ê²ŸíŒ… ì‘ì—… (ë™ê¸° ë˜í¼)\"\"\"\n    asyncio.run(generate_smart_queries_daily())\n\n\ndef scrape_naver_job():\n    \"\"\"ë„¤ì´ë²„ ìˆ˜ì§‘ ì‘ì—… (ë™ê¸° ë˜í¼)\"\"\"\n    asyncio.run(scrape_naver_daily())\n\n\ndef process_job():\n    \"\"\"ì •ì œ ì‘ì—… (ë™ê¸° ë˜í¼)\"\"\"\n    asyncio.run(process_pending_daily())\n\n\ndef sync_job():\n    \"\"\"ë™ê¸°í™” ì‘ì—… (ë™ê¸° ë˜í¼)\"\"\"\n    asyncio.run(sync_daily())\n\n\ndef google_enrichment_job():\n    \"\"\"êµ¬ê¸€ í‰ì  ë³´ê°• ì‘ì—… (ë™ê¸° ë˜í¼)\"\"\"\n    asyncio.run(enrich_with_google_ratings())\n\n\ndef deduplication_job():\n    \"\"\"ì¤‘ë³µ íƒì§€ ë° ë³‘í•© ì‘ì—… (ë™ê¸° ë˜í¼)\"\"\"\n    asyncio.run(deduplicate_daily())\n\n\ndef weekly_update_job():\n    \"\"\"ì£¼ê°„ ì „ì²´ ë°ì´í„° ì—…ë°ì´íŠ¸ (ë™ê¸° ë˜í¼)\"\"\"\n    asyncio.run(update_all_restaurants_weekly())\n\n\ndef backup_job():\n    \"\"\"Google Drive ë°±ì—… ì‘ì—… (ë™ê¸° ë˜í¼)\"\"\"\n    backup_daily_data()\n\n\ndef backup_daily_data():\n    \"\"\"ë‹¹ì¼ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ Google Driveì— ë°±ì—…\"\"\"\n    logger.info(\"=\" * 70)\n    logger.info(\"ğŸ’¾ Starting Google Drive backup\")\n    logger.info(\"=\" * 70)\n    \n    try:\n        with db_session() as db:\n            backup_manager = DriveBackupManager(db)\n            \n            backup_history = backup_manager.backup_daily(\n                backup_date=None,\n                backup_type='daily'\n            )\n            \n            if backup_history.status == 'success':\n                logger.info(f\"âœ… Backup completed successfully:\")\n                logger.info(f\"   File: {backup_history.file_path}\")\n                logger.info(f\"   Records: {backup_history.total_records}\")\n                logger.info(f\"   Size: {backup_history.file_size_bytes / 1024 / 1024:.2f} MB\")\n                logger.info(f\"   Quality: {backup_history.average_quality_score}\")\n                logger.info(f\"   Time: {backup_history.execution_time_seconds}s\")\n            else:\n                logger.error(f\"âŒ Backup failed: {backup_history.error_message}\")\n            \n            return backup_history.status == 'success'\n            \n    except Exception as e:\n        logger.error(f\"âŒ Backup job failed: {e}\")\n        return False\n\n\nasync def update_all_restaurants_weekly():\n    \"\"\"ë§¤ì£¼ ëª¨ë“  ë ˆìŠ¤í† ë‘ì˜ ëˆ„ë½ëœ ì •ë³´ë¥¼ Apifyë¡œ ì—…ë°ì´íŠ¸\"\"\"\n    logger.info(\"=\" * 70)\n    logger.info(\"ğŸ“… Starting weekly comprehensive data update\")\n    logger.info(\"=\" * 70)\n    \n    try:\n        apify = ApifyNaverScraper()\n        total_updated = 0\n        batch_size = 10\n        \n        while True:\n            with db_session() as db:\n                # ì „í™”ë²ˆí˜¸ ë˜ëŠ” ë©”ë‰´ê°€ ì—†ëŠ” ë ˆìŠ¤í† ë‘ ì¡°íšŒ\n                restaurants_to_update = db.query(ProcessedRestaurant).filter(\n                    (ProcessedRestaurant.phone == None) | \n                    (ProcessedRestaurant.menu_summary == None) |\n                    (ProcessedRestaurant.open_hours == None)\n                ).limit(batch_size).all()\n                \n                if not restaurants_to_update:\n                    logger.info(\"âœ… All restaurants have complete data\")\n                    break\n                \n                logger.info(f\"Processing batch of {len(restaurants_to_update)} restaurants...\")\n                batch_updated = 0\n                \n                for restaurant in restaurants_to_update:\n                    try:\n                        logger.info(f\"  ğŸ” Updating: {restaurant.name}\")\n                        \n                        details = await apify.get_restaurant_details(\n                            restaurant_name=restaurant.name,\n                            address=restaurant.address\n                        )\n                        \n                        if details:\n                            updated_fields = []\n                            \n                            if not restaurant.phone and details.get('phone'):\n                                restaurant.phone = details.get('phone')\n                                updated_fields.append('phone')\n                            \n                            if not restaurant.menu_summary:\n                                menus = details.get('menus', []) or details.get('menu_items', [])\n                                if menus:\n                                    menu_list = []\n                                    for menu in menus[:10]:\n                                        if isinstance(menu, dict):\n                                            menu_list.append({\n                                                \"name\": menu.get('name', ''),\n                                                \"price\": menu.get('price', '')\n                                            })\n                                    if menu_list:\n                                        restaurant.menu_summary = menu_list\n                                        updated_fields.append(f'menu({len(menu_list)})')\n                            \n                            if not restaurant.open_hours and details.get('businessHours'):\n                                restaurant.open_hours = {\"hours\": details.get('businessHours')}\n                                updated_fields.append('hours')\n                            \n                            if details.get('rating') and not restaurant.naver_rating:\n                                restaurant.naver_rating = details.get('rating')\n                                restaurant.naver_review_count = details.get('reviewCount', 0)\n                                updated_fields.append('rating')\n                            \n                            if updated_fields:\n                                batch_updated += 1\n                                logger.info(f\"    âœ“ Updated: {', '.join(updated_fields)}\")\n                        \n                        await asyncio.sleep(2)\n                        \n                    except Exception as e:\n                        logger.error(f\"    âŒ Failed to update {restaurant.name}: {e}\")\n                \n                db.commit()\n                total_updated += batch_updated\n                logger.info(f\"  âœ“ Batch committed: {batch_updated}/{len(restaurants_to_update)} updated (Total: {total_updated})\")\n                \n                if len(restaurants_to_update) < batch_size:\n                    break\n        \n        logger.info(\"=\" * 70)\n        logger.info(f\"âœ… Weekly update completed: {total_updated} restaurants\")\n        logger.info(\"=\" * 70)\n        return total_updated\n            \n    except Exception as e:\n        logger.error(f\"âŒ Weekly update failed: {e}\")\n        return 0\n\n\ndef setup_schedule():\n    \"\"\"ìŠ¤ì¼€ì¤„ ì„¤ì • (UTC ê¸°ì¤€, KST = UTC + 9ì‹œê°„)\"\"\"\n    logger.info(\"ğŸš€ Setting up 24/7 automated schedule...\")\n    \n    # UTC 16:30 = KST 01:30 (ë‹¤ìŒë‚ ) - ìŠ¤ë§ˆíŠ¸ íƒ€ê²ŸíŒ…\n    schedule.every().day.at(\"16:30\").do(smart_targeting_job)\n    logger.info(\"  âœ“ Smart targeting: Daily at 16:30 UTC (KST 01:30, dynamic queries)\")\n    \n    # UTC 18:00 = KST 03:00 (ë‹¤ìŒë‚ )\n    schedule.every().day.at(\"18:00\").do(scrape_naver_job)\n    logger.info(\"  âœ“ Naver scraping: Daily at 18:00 UTC (KST 03:00, 33 restaurants)\")\n    \n    # UTC 18:05 = KST 03:05 (ë‹¤ìŒë‚ ) - ì¤‘ë³µ íƒì§€ ë° ë³‘í•© (Gemini/Google ì „ì— ì‹¤í–‰)\n    schedule.every().day.at(\"18:05\").do(deduplication_job)\n    logger.info(\"  âœ“ Duplicate detection: Daily at 18:05 UTC (KST 03:05, auto-merge)\")\n    \n    # UTC 21:00 = KST 06:00 (ë‹¤ìŒë‚ )\n    schedule.every().day.at(\"21:00\").do(process_job)\n    logger.info(\"  âœ“ Gemini processing: Daily at 21:00 UTC (KST 06:00)\")\n    \n    # UTC 22:00 = KST 07:00 (ë‹¤ìŒë‚ )\n    schedule.every().day.at(\"22:00\").do(google_enrichment_job)\n    logger.info(\"  âœ“ Google enrichment: Daily at 22:00 UTC (KST 07:00, 33 restaurants)\")\n    \n    # UTC 23:00 = KST 08:00 (ë‹¤ìŒë‚ )\n    schedule.every().day.at(\"23:00\").do(sync_job)\n    logger.info(\"  âœ“ Hansikdang sync: Daily at 23:00 UTC (KST 08:00)\")\n    \n    # UTC 13:00 = KST 22:00 (ê°™ì€ ë‚ ) - Google Drive ë°±ì—…\n    schedule.every().day.at(\"13:00\").do(backup_job)\n    logger.info(\"  âœ“ Google Drive backup: Daily at 13:00 UTC (KST 22:00)\")\n    \n    # ë§¤ì£¼ ì¼ìš”ì¼ UTC 03:00 = KST 12:00 (ì •ì˜¤)\n    schedule.every().sunday.at(\"03:00\").do(weekly_update_job)\n    logger.info(\"  âœ“ Weekly data update: Sunday at 03:00 UTC (KST 12:00)\")\n    \n    schedule.every().hour.do(log_statistics)\n    logger.info(\"  âœ“ Statistics logging: Every hour\")\n    \n    logger.info(\"\\nâœ… Schedule setup completed!\")\n    logger.info(\"=\" * 60)\n    logger.info(\"ğŸ“… Daily Schedule (KST):\")\n    logger.info(\"  01:30 KST - Smart Targeting (ì™¸êµ­ì¸ ì¸ê¸°ë„ ë¶„ì„ + ë™ì  ì¿¼ë¦¬ ìƒì„±)\")\n    logger.info(\"  03:00 KST - Naver Maps scraping (33 smart queries)\")\n    logger.info(\"  03:05 KST - Duplicate detection & merging (ì¤‘ë³µ ì œê±°)\")\n    logger.info(\"  06:00 KST - Gemini AI processing\")\n    logger.info(\"  07:00 KST - Google rating enrichment (33 restaurants)\")\n    logger.info(\"  08:00 KST - Sync to í•œì‹ë‹¹ platform\")\n    logger.info(\"  22:00 KST - Google Drive daily backup\")\n    logger.info(\"  Every hour - Statistics logging\")\n    logger.info(\"\")\n    logger.info(\"ğŸ“… Weekly Schedule (KST):\")\n    logger.info(\"  Sunday 12:00 KST - Full data update (phone, menu, hours)\")\n    logger.info(\"=\" * 60)\n    logger.info(f\"ğŸ¯ Daily target: 33 restaurants (ìŠ¤ë§ˆíŠ¸ íƒ€ê²ŸíŒ…)\")\n    logger.info(f\"ğŸ¯ Monthly target: 990 restaurants\")\n    logger.info(f\"ğŸ’¾ Backup: Daily 22:00 KST to Google Drive\")\n    logger.info(\"=\" * 60)\n\n\ndef main():\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"ğŸš€ Restaurant Data Hub - 24/7 Scheduler Starting\")\n    logger.info(\"=\" * 60)\n    logger.info(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    logger.info(\"=\" * 60)\n    \n    init_db()\n    \n    setup_schedule()\n    \n    log_statistics()\n    \n    logger.info(\"\\nğŸ”„ Scheduler running... (Press Ctrl+C to stop)\")\n    logger.info(\"â° Schedule check interval: 10 seconds\")\n    logger.info(\"=\" * 60)\n    \n    try:\n        loop_count = 0\n        while True:\n            schedule.run_pending()\n            loop_count += 1\n            \n            if loop_count % 360 == 0:\n                logger.debug(f\"Scheduler loop: {loop_count} iterations ({loop_count // 360} hours)\")\n            \n            time.sleep(10)\n            \n    except KeyboardInterrupt:\n        logger.info(\"\\nğŸ›‘ Scheduler stopped by user\")\n    except Exception as e:\n        logger.error(f\"âŒ Scheduler error: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":33839},"data-hub/src/api/__init__.py":{"content":"\"\"\"API module\"\"\"\n","size_bytes":17},"data-hub/QUICK_START.md":{"content":"# ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ\n\n## 5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸° ğŸš€\n\n### 1ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (2ë¶„)\n\n```bash\ncd data-hub\ncp .env.example .env\n```\n\n`.env` íŒŒì¼ í¸ì§‘:\n```bash\n# í•„ìˆ˜ (ì§€ê¸ˆ ë‹¹ì¥)\nGEMINI_API_KEY=your_key_here\n\n# ë‚˜ì¤‘ì— (Apify ê°€ì… í›„)\nAPIFY_API_TOKEN=\nOUTSCRAPER_API_KEY=\n\n# ê¸°ì¡´ í•œì‹ë‹¹ ì„¤ì • ì¬ì‚¬ìš©\nDATA_COLLECTION_API_KEY=your_existing_key\nHANSIKDANG_API_URL=http://localhost:5000\n```\n\n### 2ë‹¨ê³„: ì˜ì¡´ì„± ì„¤ì¹˜ (1ë¶„)\n\n```bash\npip install -r requirements.txt\n```\n\n### 3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (30ì´ˆ)\n\n```bash\npython cli.py init\n```\n\n### 4ë‹¨ê³„: AIë¡œ íƒ€ê²Ÿ ìƒì„± (1ë¶„)\n\n```bash\n# ê°•ë‚¨êµ¬ í•œì‹ë‹¹ í‚¤ì›Œë“œ 50ê°œ ìë™ ìƒì„±\npython cli.py generate-targets --region ê°•ë‚¨êµ¬ --count 50\n```\n\n### 5ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (30ì´ˆ)\n\n```bash\n# ìˆ˜ë™ìœ¼ë¡œ íƒ€ê²Ÿ ì¶”ê°€ (Apify ì—†ì´ í…ŒìŠ¤íŠ¸)\npython cli.py add-target \"ê°•ë‚¨ ëƒ‰ë©´\" --region ê°•ë‚¨êµ¬\n\n# API ì„œë²„ ì‹œì‘\npython -m src.api.main\n```\n\në¸Œë¼ìš°ì €ì—ì„œ í™•ì¸:\n- http://localhost:8000 (API ì„œë²„)\n- http://localhost:8000/api/stats (í†µê³„)\n- http://localhost:8000/api/targets (íƒ€ê²Ÿ ëª©ë¡)\n\n---\n\n## ì‹¤ì œ ìŠ¤í¬ë˜í•‘ (Apify í•„ìš”)\n\n### 1. Apify ê°€ì…\n1. https://apify.com ê°€ì…\n2. Starter í”Œëœ ($49/ì›”)\n3. API Token ë³µì‚¬ â†’ `.env`ì— ì¶”ê°€\n\n### 2. ìŠ¤í¬ë˜í•‘ ì‹œì‘\n```bash\n# ì „ì²´ íŒŒì´í”„ë¼ì¸ (ìŠ¤í¬ë˜í•‘ â†’ ì²˜ë¦¬ â†’ ë™ê¸°í™”)\npython cli.py full-pipeline\n```\n\n### 3. ìë™í™” (í¬ë¡ )\n```bash\n# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\nnohup python cron_schedule.py &\n```\n\n---\n\n## ë‹¤ìŒ ë‹¨ê³„\n\nâœ… DEPLOYMENT_GUIDE.md: Google Cloud Run ë°°í¬  \nâœ… README.md: ì „ì²´ ë¬¸ì„œ  \nâœ… API ë¬¸ì„œ: http://localhost:8000/docs (FastAPI ìë™ ìƒì„±)\n\n---\n\n## ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?\n\n**ëª…ë ¹ì–´ í•œëˆˆì— ë³´ê¸°**:\n```bash\n./run.sh init       # DB ì´ˆê¸°í™”\n./run.sh scrape     # ìŠ¤í¬ë˜í•‘\n./run.sh process    # ë°ì´í„° ì²˜ë¦¬\n./run.sh sync       # í•œì‹ë‹¹ ë™ê¸°í™”\n./run.sh pipeline   # ì „ì²´ íŒŒì´í”„ë¼ì¸\n./run.sh server     # API ì„œë²„\n./run.sh cron       # í¬ë¡  ìŠ¤ì¼€ì¤„ëŸ¬\n```\n","size_bytes":2054},"data-hub/run.sh":{"content":"#!/bin/bash\n# Restaurant Data Hub ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸\n\nset -e\n\necho \"ğŸš€ Restaurant Data Hub\"\necho \"\"\n\n# í™˜ê²½ ë³€ìˆ˜ í™•ì¸\nif [ ! -f .env ]; then\n    echo \"âš ï¸  .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. .env.exampleì„ ë³µì‚¬í•˜ì„¸ìš”.\"\n    cp .env.example .env\n    echo \"âœ… .env íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.\"\n    exit 1\nfi\n\n# ëª…ë ¹ì–´ íŒŒì‹±\ncase \"$1\" in\n    init)\n        echo \"ğŸ“¦ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”...\"\n        python cli.py init\n        ;;\n    \n    scrape)\n        echo \"ğŸ” ìŠ¤í¬ë˜í•‘ ì‹œì‘...\"\n        python cli.py scrape\n        ;;\n    \n    process)\n        echo \"âš™ï¸  ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...\"\n        python cli.py process\n        ;;\n    \n    sync)\n        echo \"ğŸ”„ í•œì‹ë‹¹ ë™ê¸°í™”...\"\n        python cli.py sync\n        ;;\n    \n    pipeline)\n        echo \"ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰...\"\n        python cli.py full-pipeline\n        ;;\n    \n    server)\n        echo \"ğŸŒ API ì„œë²„ ì‹œì‘...\"\n        python -m src.api.main\n        ;;\n    \n    cron)\n        echo \"â° í¬ë¡  ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘...\"\n        python cron_schedule.py\n        ;;\n    \n    *)\n        echo \"ì‚¬ìš©ë²•:\"\n        echo \"  ./run.sh init      - DB ì´ˆê¸°í™”\"\n        echo \"  ./run.sh scrape    - ìŠ¤í¬ë˜í•‘\"\n        echo \"  ./run.sh process   - ë°ì´í„° ì²˜ë¦¬\"\n        echo \"  ./run.sh sync      - í•œì‹ë‹¹ ë™ê¸°í™”\"\n        echo \"  ./run.sh pipeline  - ì „ì²´ íŒŒì´í”„ë¼ì¸\"\n        echo \"  ./run.sh server    - API ì„œë²„\"\n        echo \"  ./run.sh cron      - í¬ë¡  ìŠ¤ì¼€ì¤„ëŸ¬\"\n        exit 1\n        ;;\nesac\n","size_bytes":1576},"data-hub/src/database/connection.py":{"content":"\"\"\"\nDatabase connection management\n\"\"\"\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom sqlalchemy.pool import NullPool\nfrom contextlib import contextmanager\nfrom typing import Generator\n\nfrom config import settings\nfrom src.database.models import Base\n\n\n# Create engine\nengine = create_engine(\n    settings.data_hub_database_url,\n    poolclass=NullPool,  # Cloud Runì—ì„œëŠ” ì—°ê²° í’€ ì‚¬ìš© ì•ˆ í•¨\n    echo=False,\n)\n\n# Create session factory\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\n\ndef init_db():\n    \"\"\"ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±\"\"\"\n    Base.metadata.create_all(bind=engine)\n\n\ndef get_db() -> Generator[Session, None, None]:\n    \"\"\"FastAPI dependency\"\"\"\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n\n@contextmanager\ndef db_session() -> Generator[Session, None, None]:\n    \"\"\"Context manager for database sessions\"\"\"\n    db = SessionLocal()\n    try:\n        yield db\n        db.commit()\n    except Exception:\n        db.rollback()\n        raise\n    finally:\n        db.close()\n","size_bytes":1109},"data-hub/src/scrapers/google_places_api.py":{"content":"\"\"\"\nGoogle Places API ìŠ¤í¬ë˜í¼ - í‰ì  & ë¦¬ë·°ìˆ˜ ìˆ˜ì§‘\në¹„ìš©: ì›” $200 ë¬´ë£Œ ì¿¼í„° (Basic Data í•„ë“œ)\n\"\"\"\nimport os\nimport asyncio\nimport httpx\nfrom typing import Optional, Dict, List\nfrom loguru import logger\n\nfrom config import settings\nfrom src.utils.rate_limiter import RateLimiter\n\n\nclass GooglePlacesAPI:\n    \"\"\"\n    Google Places APIë¥¼ ì‚¬ìš©í•œ ë ˆìŠ¤í† ë‘ í‰ì /ë¦¬ë·°ìˆ˜ ìˆ˜ì§‘\n    \n    API ë¹„ìš©:\n    - Find Place: $17/1000 requests (ë¬´ë£Œ ì¿¼í„°: $200/ì›”)\n    - í•„ë“œ: place_id, name, rating, user_ratings_total (Basic Data)\n    \"\"\"\n    \n    def __init__(self):\n        # Google Places API Key ì‚¬ìš© (GOOGLE_PLACES_API_KEY ë˜ëŠ” GEMINI_API_KEY)\n        self.api_key = os.getenv(\"GOOGLE_PLACES_API_KEY\") or settings.gemini_api_key\n        if not self.api_key:\n            raise ValueError(\"GOOGLE_PLACES_API_KEY or GEMINI_API_KEY not set\")\n        \n        self.base_url = \"https://maps.googleapis.com/maps/api/place\"\n        # Rate Limit: 10 requests/second (ì•ˆì „í•˜ê²Œ 5 req/s)\n        self.rate_limiter = RateLimiter(max_requests=5, per_seconds=1)\n        self.logger = logger.bind(scraper=\"google_places\")\n    \n    async def search_place(\n        self, \n        name: str, \n        address: str\n    ) -> Optional[Dict]:\n        \"\"\"\n        ë ˆìŠ¤í† ë‘ ì´ë¦„ + ì£¼ì†Œë¡œ ê²€ìƒ‰\n        \n        Args:\n            name: ë ˆìŠ¤í† ë‘ ì´ë¦„\n            address: ì£¼ì†Œ\n        \n        Returns:\n            {\n                \"place_id\": str,\n                \"name\": str,\n                \"rating\": float,\n                \"user_ratings_total\": int\n            } ë˜ëŠ” None\n        \"\"\"\n        try:\n            await self.rate_limiter.acquire()\n            \n            query = f\"{name} {address}\"\n            params = {\n                \"input\": query,\n                \"inputtype\": \"textquery\",\n                \"fields\": \"place_id,name,rating,user_ratings_total,photos\",\n                \"key\": self.api_key,\n                \"language\": \"ko\"\n            }\n            \n            self.logger.debug(f\"Searching: {name}\")\n            \n            async with httpx.AsyncClient() as client:\n                response = await client.get(\n                    f\"{self.base_url}/findplacefromtext/json\",\n                    params=params,\n                    timeout=10\n                )\n                \n                if response.status_code != 200:\n                    self.logger.error(\n                        f\"HTTP {response.status_code}: {response.text[:200]}\"\n                    )\n                    return None\n                \n                data = response.json()\n                \n                if data.get(\"status\") != \"OK\":\n                    self.logger.warning(\n                        f\"API status {data.get('status')} for {name}\"\n                    )\n                    return None\n                \n                candidates = data.get(\"candidates\", [])\n                if not candidates:\n                    self.logger.warning(f\"No candidates for {name}\")\n                    return None\n                \n                result = candidates[0]\n                \n                # âœ… ì´ë¯¸ì§€ URL ì¶”ì¶œ (ìµœëŒ€ 10ê°œ)\n                image_urls = []\n                if 'photos' in result and len(result['photos']) > 0:\n                    for photo in result['photos'][:10]:\n                        photo_reference = photo.get('photo_reference')\n                        if photo_reference:\n                            image_url = (\n                                f\"https://maps.googleapis.com/maps/api/place/photo\"\n                                f\"?maxwidth=800&photo_reference={photo_reference}&key={self.api_key}\"\n                            )\n                            image_urls.append(image_url)\n                \n                result['image_urls'] = image_urls\n                result['image_url'] = image_urls[0] if image_urls else \"https://via.placeholder.com/400x300?text=Restaurant\"\n                \n                self.logger.info(\n                    f\"Found: {result.get('name')} \"\n                    f\"(rating: {result.get('rating')}, \"\n                    f\"reviews: {result.get('user_ratings_total')}, \"\n                    f\"images: {len(image_urls)})\"\n                )\n                return result\n        \n        except httpx.HTTPError as e:\n            self.logger.error(f\"HTTP error for {name}: {e}\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Failed to search {name}: {e}\")\n            return None\n    \n    async def batch_search(\n        self, \n        restaurants: List[Dict]\n    ) -> List[Dict]:\n        \"\"\"\n        ë°°ì¹˜ ì²˜ë¦¬ - ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ë˜ Rate Limit ì¤€ìˆ˜\n        \n        Args:\n            restaurants: [{\"id\": str, \"name\": str, \"address\": str}, ...]\n        \n        Returns:\n            [{\"restaurant_id\": str, \"google_data\": dict}, ...]\n        \"\"\"\n        self.logger.info(f\"Batch search: {len(restaurants)} restaurants\")\n        \n        tasks = [\n            self.search_place(r[\"name\"], r[\"address\"]) \n            for r in restaurants\n        ]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        successful = [\n            {\n                \"restaurant_id\": restaurants[i][\"id\"],\n                \"google_data\": result\n            }\n            for i, result in enumerate(results)\n            if result and not isinstance(result, Exception)\n        ]\n        \n        self.logger.info(\n            f\"Batch complete: {len(successful)}/{len(restaurants)} successful\"\n        )\n        return successful\n","size_bytes":5597},"data-hub/src/workflows/__init__.py":{"content":"\"\"\"Workflow module\"\"\"\n","size_bytes":22},"data-hub/src/processors/quality_validator.py":{"content":"\"\"\"\nQuality Validator\ní’ˆì§ˆ ê¸°ì¤€ ê²€ì¦: Quality Score >= 75, ì´ë¯¸ì§€ >= 5, ë¦¬ë·° >= 10\n\"\"\"\nfrom typing import Dict, Any, Tuple\nfrom loguru import logger\nfrom config import settings\n\n\nclass QualityValidator:\n    \"\"\"í’ˆì§ˆ ê²€ì¦\"\"\"\n    \n    @staticmethod\n    def validate(data: Dict[str, Any]) -> Tuple[bool, Dict[str, Any]]:\n        \"\"\"\n        í’ˆì§ˆ ê¸°ì¤€ ê²€ì¦\n        \n        Returns:\n            (is_valid, validation_details)\n        \"\"\"\n        details = {\n            \"quality_score\": data.get(\"quality_score\", data.get(\"qualityScore\", 0)),\n            \"image_count\": len(data.get(\"images\", [])),\n            \"review_count\": (\n                data.get(\"naver_review_count\", 0) + \n                data.get(\"google_review_count\", 0)\n            ),\n            \"checks\": {}\n        }\n        \n        # 1. Quality Score ê²€ì¦\n        quality_check = details[\"quality_score\"] >= settings.quality_threshold\n        details[\"checks\"][\"quality_score\"] = {\n            \"value\": details[\"quality_score\"],\n            \"required\": settings.quality_threshold,\n            \"passed\": quality_check\n        }\n        \n        # 2. ì´ë¯¸ì§€ ìˆ˜ ê²€ì¦\n        image_check = details[\"image_count\"] >= settings.min_images\n        details[\"checks\"][\"images\"] = {\n            \"value\": details[\"image_count\"],\n            \"required\": settings.min_images,\n            \"passed\": image_check\n        }\n        \n        # 3. ë¦¬ë·° ìˆ˜ ê²€ì¦\n        review_check = details[\"review_count\"] >= settings.min_reviews\n        details[\"checks\"][\"reviews\"] = {\n            \"value\": details[\"review_count\"],\n            \"required\": settings.min_reviews,\n            \"passed\": review_check\n        }\n        \n        # ì „ì²´ ê²€ì¦ ê²°ê³¼\n        is_valid = quality_check and image_check and review_check\n        details[\"overall_passed\"] = is_valid\n        \n        if not is_valid:\n            logger.warning(\n                f\"Quality check failed for {data.get('name')}: \"\n                f\"Score={details['quality_score']}, \"\n                f\"Images={details['image_count']}, \"\n                f\"Reviews={details['review_count']}\"\n            )\n        \n        return is_valid, details\n    \n    @staticmethod\n    def get_summary(details: Dict[str, Any]) -> str:\n        \"\"\"ê²€ì¦ ê²°ê³¼ ìš”ì•½\"\"\"\n        checks = details[\"checks\"]\n        passed = [k for k, v in checks.items() if v[\"passed\"]]\n        failed = [k for k, v in checks.items() if not v[\"passed\"]]\n        \n        return (\n            f\"Quality Check: {details['overall_passed']} | \"\n            f\"Passed: {', '.join(passed)} | \"\n            f\"Failed: {', '.join(failed) if failed else 'None'}\"\n        )\n","size_bytes":2665},"data-hub/src/scrapers/place_id_loader.py":{"content":"\"\"\"\nGoogle Place ID Loader\nê¸°ì¡´ 183ê°œ Place IDë¥¼ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜ì§‘\n\"\"\"\nfrom typing import List, Set\nfrom pathlib import Path\nfrom loguru import logger\n\n\nclass PlaceIDLoader:\n    \"\"\"Google Place ID ìš°ì„  ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬\"\"\"\n    \n    def __init__(self, file_path: str = \"data/google_place_ids_priority.txt\"):\n        self.file_path = Path(file_path)\n        self.place_ids: Set[str] = set()\n        self._load()\n    \n    def _load(self):\n        \"\"\"íŒŒì¼ì—ì„œ Place ID ë¡œë“œ\"\"\"\n        if not self.file_path.exists():\n            logger.warning(f\"Place ID file not found: {self.file_path}\")\n            return\n        \n        with open(self.file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                # ì£¼ì„ê³¼ ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°\n                if line and not line.startswith('#'):\n                    self.place_ids.add(line)\n        \n        logger.info(f\"Loaded {len(self.place_ids)} priority Place IDs\")\n    \n    def get_priority_list(self) -> List[str]:\n        \"\"\"ìš°ì„  ìˆ˜ì§‘ Place ID ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n        return list(self.place_ids)\n    \n    def is_priority(self, place_id: str) -> bool:\n        \"\"\"í•´ë‹¹ Place IDê°€ ìš°ì„  ìˆ˜ì§‘ ëŒ€ìƒì¸ì§€ í™•ì¸\"\"\"\n        return place_id in self.place_ids\n    \n    def remove_collected(self, place_id: str):\n        \"\"\"ìˆ˜ì§‘ ì™„ë£Œëœ Place ID ì œê±°\"\"\"\n        if place_id in self.place_ids:\n            self.place_ids.remove(place_id)\n    \n    def remaining_count(self) -> int:\n        \"\"\"ë‚¨ì€ ìš°ì„  ìˆ˜ì§‘ ëŒ€ìƒ ìˆ˜\"\"\"\n        return len(self.place_ids)\n","size_bytes":1596},"data-hub/update_all_data.py":{"content":"\"\"\"\nApifyë¡œ ëª¨ë“  ë ˆìŠ¤í† ë‘ ë°ì´í„° ì—…ë°ì´íŠ¸\n- ì „í™”ë²ˆí˜¸\n- ë©”ë‰´\n- ì˜ì—…ì‹œê°„\n- ë¦¬ë·°\n\"\"\"\nimport asyncio\nimport sys\nsys.path.insert(0, '/home/runner/workspace/data-hub')\n\nfrom loguru import logger\nfrom src.scrapers.apify_naver_scraper import ApifyNaverScraper\nfrom src.database.connection import db_session\nfrom src.database.models import ProcessedRestaurant\n\nlogger.add(\"logs/update_all.log\", rotation=\"1 day\", retention=\"7 days\")\n\n\nasync def update_all_restaurant_data():\n    \"\"\"ëª¨ë“  ë ˆìŠ¤í† ë‘ì˜ ëˆ„ë½ëœ ì •ë³´ë¥¼ Apifyë¡œ ì—…ë°ì´íŠ¸\"\"\"\n    logger.info(\"=\" * 70)\n    logger.info(\"ğŸ”„ Starting comprehensive data update with Apify\")\n    logger.info(\"=\" * 70)\n    \n    try:\n        apify = ApifyNaverScraper()\n        total_updated = 0\n        batch_size = 10\n        \n        while True:\n            with db_session() as db:\n                # ì „í™”ë²ˆí˜¸ ë˜ëŠ” ë©”ë‰´ê°€ ì—†ëŠ” ë ˆìŠ¤í† ë‘ ì¡°íšŒ\n                restaurants_to_update = db.query(ProcessedRestaurant).filter(\n                    (ProcessedRestaurant.phone == None) | \n                    (ProcessedRestaurant.menu_summary == None)\n                ).limit(batch_size).all()\n                \n                if not restaurants_to_update:\n                    logger.info(\"âœ… All restaurants have complete data\")\n                    break\n                \n                logger.info(f\"Processing batch of {len(restaurants_to_update)} restaurants...\")\n                batch_updated = 0\n                \n                for restaurant in restaurants_to_update:\n                    try:\n                        logger.info(f\"  ğŸ” Updating: {restaurant.name}\")\n                        \n                        # Apifyë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ\n                        details = await apify.get_restaurant_details(\n                            restaurant_name=restaurant.name,\n                            address=restaurant.address\n                        )\n                        \n                        if details:\n                            updated_fields = []\n                            \n                            # 1. ì „í™”ë²ˆí˜¸ ì—…ë°ì´íŠ¸\n                            if not restaurant.phone and details.get('phone'):\n                                restaurant.phone = details.get('phone')\n                                updated_fields.append('phone')\n                            \n                            # 2. ë©”ë‰´ ì—…ë°ì´íŠ¸\n                            if not restaurant.menu_summary:\n                                menus = details.get('menus', []) or details.get('menu_items', [])\n                                if menus:\n                                    menu_list = []\n                                    for menu in menus[:10]:\n                                        if isinstance(menu, dict):\n                                            menu_list.append({\n                                                \"name\": menu.get('name', ''),\n                                                \"price\": menu.get('price', '')\n                                            })\n                                    if menu_list:\n                                        restaurant.menu_summary = menu_list\n                                        updated_fields.append(f'menu({len(menu_list)})')\n                            \n                            # 3. ì˜ì—…ì‹œê°„ ì—…ë°ì´íŠ¸\n                            if not restaurant.open_hours and details.get('businessHours'):\n                                restaurant.open_hours = {\"hours\": details.get('businessHours')}\n                                updated_fields.append('hours')\n                            \n                            # 4. í‰ì  ì—…ë°ì´íŠ¸ (Naver)\n                            if details.get('rating') and not restaurant.naver_rating:\n                                restaurant.naver_rating = details.get('rating')\n                                restaurant.naver_review_count = details.get('reviewCount', 0)\n                                updated_fields.append('rating')\n                            \n                            if updated_fields:\n                                batch_updated += 1\n                                logger.info(f\"    âœ“ Updated: {', '.join(updated_fields)}\")\n                            else:\n                                logger.debug(f\"    âš ï¸  No updates needed for {restaurant.name}\")\n                        else:\n                            logger.warning(f\"    âš ï¸  No data found for {restaurant.name}\")\n                        \n                        await asyncio.sleep(2)  # Rate limiting\n                        \n                    except Exception as e:\n                        logger.error(f\"    âŒ Failed to update {restaurant.name}: {e}\")\n                \n                # ë°°ì¹˜ ì»¤ë°‹\n                db.commit()\n                total_updated += batch_updated\n                logger.info(f\"  âœ“ Batch committed: {batch_updated}/{len(restaurants_to_update)} updated (Total: {total_updated})\")\n                \n                if len(restaurants_to_update) < batch_size:\n                    break\n        \n        logger.info(\"=\" * 70)\n        logger.info(f\"âœ… Update completed: {total_updated} restaurants\")\n        logger.info(\"=\" * 70)\n        return total_updated\n            \n    except Exception as e:\n        logger.error(f\"âŒ Update failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return 0\n\n\nif __name__ == \"__main__\":\n    result = asyncio.run(update_all_restaurant_data())\n    print(f\"\\n{'='*70}\")\n    print(f\"Total restaurants updated: {result}\")\n    print(f\"{'='*70}\")\n","size_bytes":5653},"data-hub/src/scrapers/apify_naver_scraper.py":{"content":"\"\"\"\nApify Naver Map Scraper Integration\nApifyì˜ 'Naver Map Search Results Scraper'ë¥¼ í™œìš©í•œ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘\n\níŠ¹ì§•:\n- ë©”ë‰´ ì •ë³´ í¬í•¨\n- ë¦¬ë·°, ì˜ì—…ì‹œê°„, GPS ë“± í’ë¶€í•œ ë°ì´í„°\n- 99%+ ì„±ê³µë¥  ë³´ì¥\n\"\"\"\nimport httpx\nimport asyncio\nimport uuid\nfrom typing import List, Dict, Any, Optional\nfrom loguru import logger\n\nfrom config import settings\n\n\nclass ApifyNaverScraper:\n    \"\"\"Apify Naver Map Search Results Scraper\"\"\"\n    \n    ACTOR_ID = \"UCpUxFUNcdKdbBdYg\"  # delicious_zebu/naver-map-search-results-scraper\n    \n    def __init__(self):\n        if not settings.apify_api_token:\n            raise ValueError(\"APIFY_API_TOKEN not set\")\n        \n        self.api_token = settings.apify_api_token\n        self.base_url = \"https://api.apify.com/v2\"\n        self.logger = logger.bind(scraper=\"apify_naver\")\n    \n    async def search_restaurants(\n        self,\n        keywords: List[str],\n        max_results_per_keyword: int = 100\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë ˆìŠ¤í† ë‘ ê²€ìƒ‰ (ë©”ë‰´ í¬í•¨)\n        \n        Args:\n            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [\"ê°•ë‚¨ í•œì‹\", \"í™ëŒ€ ë§›ì§‘\"])\n            max_results_per_keyword: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸: 100)\n        \n        Returns:\n            ë ˆìŠ¤í† ë‘ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ë©”ë‰´ í¬í•¨)\n        \"\"\"\n        try:\n            # Apify Actor ì‹¤í–‰\n            run_input = {\n                \"keywords\": keywords,\n                \"maxResultsPerKeyword\": max_results_per_keyword\n            }\n            \n            self.logger.info(f\"Starting Apify scrape: {len(keywords)} keywords\")\n            \n            # Actor ì‹¤í–‰ ì‹œì‘\n            async with httpx.AsyncClient(timeout=300.0) as client:\n                # 1. Actor ì‹¤í–‰\n                run_response = await client.post(\n                    f\"{self.base_url}/acts/{self.ACTOR_ID}/runs\",\n                    params={\"token\": self.api_token},\n                    json=run_input\n                )\n                run_response.raise_for_status()\n                run_data = run_response.json()\n                run_id = run_data[\"data\"][\"id\"]\n                \n                self.logger.info(f\"Actor run started: {run_id}\")\n                \n                # 2. ì‹¤í–‰ ì™„ë£Œ ëŒ€ê¸°\n                status = \"RUNNING\"\n                while status in [\"RUNNING\", \"READY\"]:\n                    await asyncio.sleep(5)\n                    \n                    status_response = await client.get(\n                        f\"{self.base_url}/actor-runs/{run_id}\",\n                        params={\"token\": self.api_token}\n                    )\n                    status_response.raise_for_status()\n                    status_data = status_response.json()\n                    status = status_data[\"data\"][\"status\"]\n                    \n                    self.logger.debug(f\"Run status: {status}\")\n                \n                # 3. ê²°ê³¼ í™•ì¸\n                if status == \"SUCCEEDED\":\n                    dataset_id = run_data[\"data\"][\"defaultDatasetId\"]\n                    \n                    # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n                    dataset_response = await client.get(\n                        f\"{self.base_url}/datasets/{dataset_id}/items\",\n                        params={\"token\": self.api_token}\n                    )\n                    dataset_response.raise_for_status()\n                    results = dataset_response.json()\n                    \n                    self.logger.info(f\"âœ… Scraped {len(results)} restaurants with menu data\")\n                    return results\n                else:\n                    self.logger.error(f\"Actor run failed: {status}\")\n                    return []\n                \n        except Exception as e:\n            self.logger.error(f\"Apify scraping failed: {e}\")\n            return []\n    \n    def parse_apify_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Apify ë°ì´í„°ë¥¼ ë‚´ë¶€ í¬ë§·ìœ¼ë¡œ ë³€í™˜\n        \n        Args:\n            raw_data: Apifyê°€ ë°˜í™˜í•œ ì›ë³¸ ë°ì´í„°\n        \n        Returns:\n            ì •ê·œí™”ëœ ë ˆìŠ¤í† ë‘ ë°ì´í„°\n        \"\"\"\n        try:\n            # Apify ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ì¶° íŒŒì‹±\n            parsed = {\n                \"id\": str(uuid.uuid4()),\n                \"source\": \"naver_apify\",\n                \"source_id\": \"\",  # ApifyëŠ” place_idë¥¼ ì§ì ‘ ì œê³µí•˜ì§€ ì•ŠìŒ\n                \"source_url\": \"\",\n                \"place_id\": None,\n                \n                # ê¸°ë³¸ ì •ë³´\n                \"name\": raw_data.get(\"Name\", \"\"),\n                \"address\": raw_data.get(\"Address\", \"\"),\n                \"category\": raw_data.get(\"Category\", \"\"),\n                \"phone\": raw_data.get(\"Contact\"),\n                \"description\": raw_data.get(\"Description\"),\n                \n                # ìœ„ì¹˜ ì •ë³´ (ApifyëŠ” ì¢Œí‘œë¥¼ ì§ì ‘ ì œê³µí•˜ì§€ ì•ŠìŒ)\n                \"lat\": None,\n                \"lng\": None,\n                \n                # í‰ê°€ ì •ë³´\n                \"rating\": raw_data.get(\"OverallRating\"),\n                \"reviewCount\": raw_data.get(\"ReviewCount\", 0),\n                \n                # âœ… ë©”ë‰´ ë°ì´í„° (í•µì‹¬!)\n                \"menus\": raw_data.get(\"MenuItems\", []),\n                \"menu_items\": raw_data.get(\"MenuItems\", []),\n                \n                # ìš´ì˜ ì •ë³´\n                \"businessHours\": raw_data.get(\"BusinessHours\"),\n                \"openingHours\": raw_data.get(\"BusinessHours\"),\n                \n                # ë¦¬ë·° ë°ì´í„°\n                \"reviews\": raw_data.get(\"Reviews\", []),\n                \n                # ì¶”ê°€ ì •ë³´\n                \"imageUrl\": None,\n                \"images\": [],\n            }\n            \n            return parsed\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to parse Apify data: {e}\")\n            return raw_data\n    \n    async def get_restaurant_details(\n        self,\n        restaurant_name: str,\n        address: Optional[str] = None\n    ) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        íŠ¹ì • ë ˆìŠ¤í† ë‘ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ (ë©”ë‰´ í¬í•¨)\n        \n        Args:\n            restaurant_name: ë ˆìŠ¤í† ë‘ ì´ë¦„\n            address: ì£¼ì†Œ (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒìš©)\n        \n        Returns:\n            ìƒì„¸ ì •ë³´ ë˜ëŠ” None\n        \"\"\"\n        try:\n            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±\n            query = restaurant_name\n            if address:\n                # êµ¬ ì´ë¦„ë§Œ ì¶”ì¶œ (ì˜ˆ: \"ê°•ë‚¨êµ¬\", \"ë§ˆí¬êµ¬\")\n                district = self._extract_district(address)\n                if district:\n                    query = f\"{district} {restaurant_name}\"\n            \n            results = await self.search_restaurants(\n                keywords=[query],\n                max_results_per_keyword=5\n            )\n            \n            if not results:\n                return None\n            \n            # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ ë°˜í™˜\n            best_match = results[0]\n            return self.parse_apify_data(best_match)\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to get details for {restaurant_name}: {e}\")\n            return None\n    \n    def _extract_district(self, address: str) -> Optional[str]:\n        \"\"\"ì£¼ì†Œì—ì„œ êµ¬ ì´ë¦„ ì¶”ì¶œ\"\"\"\n        if not address:\n            return None\n        \n        # \"ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬\" â†’ \"ê°•ë‚¨êµ¬\"\n        import re\n        match = re.search(r'([ê°€-í£]+êµ¬)', address)\n        return match.group(1) if match else None\n","size_bytes":7573},"data-hub/DATA_UPGRADE_REPORT_2025-11-03.md":{"content":"# ë°ì´í„° ì—…ê·¸ë ˆì´ë“œ ë³´ê³ ì„œ - 2025-11-03\n\n## ğŸ“‹ ê°œìš”\n\n**ì—…ê·¸ë ˆì´ë“œ ëª©í‘œ**: ê¸°ì¡´ 23ê°œ ë ˆìŠ¤í† ë‘ ë°ì´í„°ë¥¼ ìµœì‹  ì‹œìŠ¤í…œ ê¸°ì¤€ìœ¼ë¡œ ì¬ì²˜ë¦¬\n**ì‹¤í–‰ ì‹œê°**: 2025-11-03 02:30 - 02:38 UTC (11:30 - 11:38 KST)\n**ì²˜ë¦¬ ì‹œê°„**: ì•½ 8ë¶„\n\n---\n\n## âœ… ì—…ê·¸ë ˆì´ë“œ ê²°ê³¼\n\n### **ì „ì²´ í†µê³„**\n\n| í•­ëª© | ì—…ê·¸ë ˆì´ë“œ ì „ | ì—…ê·¸ë ˆì´ë“œ í›„ | ë³€í™” |\n|------|-------------|-------------|------|\n| **ì´ ë ˆìŠ¤í† ë‘** | 18ê°œ | 23ê°œ | +5ê°œ |\n| **ì‹¤ì œ ì´ë¯¸ì§€** | 0ê°œ (0%) | 23ê°œ (100%) | +23ê°œ (100%) |\n| **í‰ì  ë°ì´í„°** | 0ê°œ (0%) | 23ê°œ (100%) | +23ê°œ (100%) |\n| **í‰ê·  í’ˆì§ˆ ì ìˆ˜** | 75ì  (ê³ ì •) | 26.1ì  (ì‹¤ì œ) | -48.9ì  |\n| **ì´ ë¦¬ë·° ìˆ˜** | 0ê°œ | 11,627ê°œ | +11,627ê°œ |\n\n### **ì²˜ë¦¬ ë‚´ì—­**\n\n#### 1ï¸âƒ£ **ê¸°ì¡´ 18ê°œ processed ë ˆìŠ¤í† ë‘**\n- âœ… Google Places API ì¬ì‹¤í–‰: 16ê°œ ì„±ê³µ, 2ê°œ ì´ë¯¸ ì™„ë£Œ\n- âœ… ì´ë¯¸ì§€ ìˆ˜ì§‘: 16/16 (100%)\n- âœ… í‰ì /ë¦¬ë·° ìˆ˜ì§‘: 16/16 (100%)\n- âœ… í’ˆì§ˆ ì ìˆ˜ ì¬ê³„ì‚°: 75ì  â†’ 20ì  (GPSë§Œ ì¡´ì¬)\n\n#### 2ï¸âƒ£ **5ê°œ pending ë ˆìŠ¤í† ë‘ (ì¢…ë¡œ ì§€ì—­)**\n- âœ… Gemini AI ì •ì œ: 5/5 ì„±ê³µ\n- âœ… Google enrichment: 5/5 ì„±ê³µ\n- âœ… í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°: 5/5 ì„±ê³µ (40ì )\n- âœ… ProcessedRestaurant ìƒì„±: 5/5 ì„±ê³µ\n\n---\n\n## ğŸ“Š ë°ì´í„° í’ˆì§ˆ ê°œì„ \n\n### **ì´ë¯¸ì§€ ìˆ˜ì§‘ë¥ **\n\n```\nì—…ê·¸ë ˆì´ë“œ ì „:  0% (0/18)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\nì—…ê·¸ë ˆì´ë“œ í›„: 100% (23/23) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n```\n\n**ê°œì„ ìœ¨**: +100% (ëª¨ë“  ë ˆìŠ¤í† ë‘ì´ ì‹¤ì œ Google ì´ë¯¸ì§€ ë³´ìœ )\n\n### **í‰ì  ë°ì´í„° ìˆ˜ì§‘ë¥ **\n\n```\nì—…ê·¸ë ˆì´ë“œ ì „:  0% (0/18)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\nì—…ê·¸ë ˆì´ë“œ í›„: 100% (23/23) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n```\n\n**ì´ ë¦¬ë·° ìˆ˜**: 11,627ê°œ (í‰ê·  505ê°œ/ë ˆìŠ¤í† ë‘)\n\n### **í’ˆì§ˆ ì ìˆ˜ ë¶„í¬**\n\n#### **ì—…ê·¸ë ˆì´ë“œ ì „** (ê³ ì •ê°’)\n- 75ì : 18ê°œ (100%)\n\n#### **ì—…ê·¸ë ˆì´ë“œ í›„** (ì‹¤ì œ ê³„ì‚°)\n- **40ì **: 7ê°œ (30.4%) - GPS + Reviews + Images\n- **20ì **: 16ê°œ (69.6%) - GPSë§Œ ì¡´ì¬\n\n**ë¶„ì„**:\n- ì‹ ê·œ ì²˜ë¦¬ëœ ì¢…ë¡œ ë ˆìŠ¤í† ë‘ 5ê°œ: 40ì  (ëª¨ë“  ë°ì´í„° ì™„ë¹„)\n- ì—…ê·¸ë ˆì´ë“œëœ 2ê°œ: 40ì  (í•©ì •ê´‘ì•ˆë¦¬, ì´ë‚¨ì¥)\n- ë‚˜ë¨¸ì§€ 16ê°œ: 20ì  (Hours, Menu ë°ì´í„° ë¶€ì¡±)\n\n---\n\n## ğŸŒŸ ìƒìœ„ 10ê°œ ë ˆìŠ¤í† ë‘ (í‰ì  ê¸°ì¤€)\n\n| ìˆœìœ„ | ì´ë¦„ | í‰ì  | ë¦¬ë·° | í’ˆì§ˆ | ì´ë¯¸ì§€ |\n|------|------|------|------|------|--------|\n| 1 | ë¬´í•œë¦¬í•„ëª½ë¸”ë¦¬ ëª…ë™2í˜¸ì  | 5.0 | 937 | 20 | âœ… |\n| 2 | í•´ë°€ê°€ ì¢…ê°ì  | 5.0 | 12 | 20 | âœ… |\n| 3 | ëŒì‚°ë“±ëŒ€ì§‘ | 5.0 | 109 | 20 | âœ… |\n| 4 | í˜¸ë³´ì‹ë‹¹ ì‹ ì‚¬ì—­ì§ì˜ì  | 5.0 | 182 | 20 | âœ… |\n| 5 | ê¹€ìˆ™ì„± í•©ì •ì  | 5.0 | 88 | 20 | âœ… |\n| 6 | ë°¥ì¥ì¸ í™ëŒ€ì  | 4.9 | 206 | 20 | âœ… |\n| 7 | í•˜ì™€ì´ì¡°ê°œ í™ëŒ€ì  | 4.9 | 208 | 20 | âœ… |\n| 8 | ë™í™”ê³ ì˜¥ ì„ ë¦‰ì  | 4.8 | 129 | 20 | âœ… |\n| 9 | ì§€ê°•í•œì‹ë‹¹ ì••êµ¬ì •ë³¸ì  | 4.8 | 1040 | 20 | âœ… |\n| 10 | ìµì„ ì• ëœ» | 4.4 | 213 | 40 | âœ… |\n\n**í‰ê·  í‰ì **: 4.78 / 5.0 (ë§¤ìš° ìš°ìˆ˜)\n**í‰ê·  ë¦¬ë·°**: 312ê°œ/ë ˆìŠ¤í† ë‘\n\n---\n\n## ğŸ”§ ì—…ê·¸ë ˆì´ë“œ ì„¸ë¶€ ë‚´ì—­\n\n### **1ë‹¨ê³„: ê¸°ì¡´ 18ê°œ ë ˆìŠ¤í† ë‘ Google Enrichment**\n\n**ì²˜ë¦¬ ì‹œê°„**: 02:34 - 02:35 UTC (ì•½ 30ì´ˆ)\n\n| ë ˆìŠ¤í† ë‘ | ê²°ê³¼ | í‰ì  | ë¦¬ë·° | ì´ë¯¸ì§€ |\n|---------|------|------|------|--------|\n| ë¬´ì›”ì‹íƒ | âœ… | 4.3 | 1079 | âœ… |\n| í˜¸ë³´ì‹ë‹¹ ì‹ ì‚¬ì—­ì§ì˜ì  | âœ… | 5.0 | 182 | âœ… |\n| í† ë‹´ê³¨ ë…¼í˜„ì  | âœ… | 3.8 | 517 | âœ… |\n| í•˜ì™€ì´ì¡°ê°œ í™ëŒ€ì  | âœ… | 4.9 | 208 | âœ… |\n| í•´ë°€ê°€ ì¢…ê°ì  | âœ… | 5.0 | 12 | âœ… |\n| ëª…ë™ì • | âœ… | 4.3 | 646 | âœ… |\n| ë¬´í•œë¦¬í•„ëª½ë¸”ë¦¬ ëª…ë™2í˜¸ì  | âœ… | 5.0 | 937 | âœ… |\n| ê¹€ìˆ™ì„± í•©ì •ì  | âœ… | 5.0 | 88 | âœ… |\n| ëŒì‚°ë“±ëŒ€ì§‘ | âœ… | 5.0 | 109 | âœ… |\n| ë°¥ì¥ì¸ í™ëŒ€ì  | âœ… | 4.9 | 206 | âœ… |\n| ì‹ ë™ê¶ê°ìíƒ• ëª…ë™ì§ì˜ì  | âœ… | 3.9 | 441 | âœ… |\n| ë™í™”ê³ ì˜¥ ì„ ë¦‰ì  | âœ… | 4.8 | 129 | âœ… |\n| ì§€ê°•í•œì‹ë‹¹ ì••êµ¬ì •ë³¸ì  | âœ… | 4.8 | 1040 | âœ… |\n| ëª©ë©±ì‚°ë°© | âœ… | 4.0 | 2041 | âœ… |\n| 401ì •ìœ¡ì‹ë‹¹ | âœ… | 4.1 | 530 | âœ… |\n| ë°”ë‹¤íšŒì‚¬ë‘ ì„œêµì  | âœ… | 4.1 | 975 | âœ… |\n\n**ì„±ê³µë¥ **: 16/16 (100%)\n\n### **2ë‹¨ê³„: 5ê°œ Pending ë ˆìŠ¤í† ë‘ ì „ì²´ ì²˜ë¦¬**\n\n**ì²˜ë¦¬ ì‹œê°„**: 02:34 - 02:36 UTC (ì•½ 2ë¶„, Rate limit í¬í•¨)\n\n| ë ˆìŠ¤í† ë‘ | Google | Gemini | ì„¤ëª… ê¸¸ì´ | í’ˆì§ˆ |\n|---------|--------|--------|----------|------|\n| ê½ƒë°¥ì—í”¼ë‹¤ | âœ… 4.0/642 | âœ… | 242ì | 40 |\n| ê³ ê¶ì˜ì•„ì¹¨ ì„œìš¸ëŒ€ë³‘ì› | âœ… 3.7/400 | âœ… | 257ì | 40 |\n| ìµì„ ì• ëœ» | âœ… 4.4/213 | âœ… | 307ì | 40 |\n| ì„¤ê°€ì˜¨ | âœ… 3.7/644 | âœ… | 271ì | 40 |\n| ìµœëŒ€ê°ë„¤ | âœ… 3.9/188 | âœ… | 267ì | 40 |\n\n**ì„±ê³µë¥ **: 5/5 (100%)\n**í‰ê·  ì„¤ëª… ê¸¸ì´**: 268.8ì (ëª©í‘œ 200-300ì ë‹¬ì„±)\n\n---\n\n## ğŸ’¡ ì£¼ìš” ê°œì„  ì‚¬í•­\n\n### âœ… **ì™„ë£Œëœ ì—…ê·¸ë ˆì´ë“œ**\n\n1. **Google Photos API í™œì„±í™”**\n   - ì´ë¯¸ì§€ ìˆ˜ì§‘ë¥ : 0% â†’ 100% (+100%)\n   - ëª¨ë“  ë ˆìŠ¤í† ë‘ì´ ì‹¤ì œ Google ì´ë¯¸ì§€ ë³´ìœ \n\n2. **í‰ì /ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘**\n   - í‰ì  ë³´ìœ ìœ¨: 0% â†’ 100%\n   - ì´ ë¦¬ë·° ìˆ˜: 11,627ê°œ\n   - í‰ê·  í‰ì : 4.5/5.0\n\n3. **í’ˆì§ˆ ì ìˆ˜ ì‹¤ì œ ê³„ì‚°**\n   - ê³ ì •ê°’ 75ì  ì œê±°\n   - GPS, Reviews, Images, Hours, Menu ê¸°ë°˜ ì‹¤ì œ ê³„ì‚°\n   - í˜„ì¬ í‰ê· : 26.1ì  (ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë‚®ìŒ)\n\n4. **Gemini AI ì •ì œ**\n   - 5ê°œ ì‹ ê·œ ë ˆìŠ¤í† ë‘ ì²˜ë¦¬\n   - ì„¤ëª… ê¸¸ì´: 242-307ì (ëª©í‘œ ë‹¬ì„±)\n   - qualityScore ê³ ì •ê°’ ì œê±°\n\n---\n\n## ğŸ“ˆ í–¥í›„ ê°œì„  ë°©ì•ˆ\n\n### **1. í’ˆì§ˆ ì ìˆ˜ í–¥ìƒ (í˜„ì¬ í‰ê·  26.1ì )**\n\n**ë¬¸ì œì **: 16ê°œ ë ˆìŠ¤í† ë‘ì´ 20ì  (GPSë§Œ ì¡´ì¬)\n\n**í•´ê²°ì±…**:\n- Hours ë°ì´í„° ìˆ˜ì§‘ ê°•í™” (ë„¤ì´ë²„/Google)\n- Menu ë°ì´í„° ìˆ˜ì§‘ ì¶”ê°€\n- ì¶”ê°€ ë°ì´í„° í•„ë“œ í™•ë³´\n\n**ì˜ˆìƒ íš¨ê³¼**: 20ì  â†’ 40-60ì  (ì•½ 2ë°° ì¦ê°€)\n\n### **2. ë°ì´í„° ìˆ˜ì§‘ í™•ëŒ€**\n\n**í˜„ì¬ ìƒíƒœ**:\n- ì´ 23ê°œ ë ˆìŠ¤í† ë‘\n- ì›”ê°„ ëª©í‘œ: 990ê°œ\n- ì¼ì¼ ëª©í‘œ: 33ê°œ\n\n**ë‹¤ìŒ ì‹¤í–‰**: 2025-11-04 03:00 KST (ìë™)\n- ë„¤ì´ë²„ ìˆ˜ì§‘: 33ê°œ ì˜ˆìƒ\n- ì „ì²´ ì²˜ë¦¬: 56ê°œ ëˆ„ì \n\n### **3. ë™ê¸°í™” ìë™í™”**\n\n**í˜„ì¬**: 23ê°œ processed, ë™ê¸°í™” ëŒ€ê¸° ì¤‘\n**ëª©í‘œ**: ë©”ì¸ í”Œë«í¼ ìë™ ë™ê¸°í™” (Daily 08:00 KST)\n\n---\n\n## ğŸ¯ ì—…ê·¸ë ˆì´ë“œ ê²°ë¡ \n\n### âœ… **ì„±ê³µ ì§€í‘œ**\n\n- **ì²˜ë¦¬ìœ¨**: 23/23 (100%)\n- **ì´ë¯¸ì§€ ìˆ˜ì§‘**: 23/23 (100%)\n- **í‰ì  ìˆ˜ì§‘**: 23/23 (100%)\n- **Gemini ì •ì œ**: 5/5 (100%)\n- **ì´ ì²˜ë¦¬ ì‹œê°„**: 8ë¶„\n\n### ğŸ“Š **ë°ì´í„° í’ˆì§ˆ**\n\n- **í‰ê·  í‰ì **: 4.5/5.0 (ìš°ìˆ˜)\n- **ì´ ë¦¬ë·°**: 11,627ê°œ\n- **ì´ë¯¸ì§€ í’ˆì§ˆ**: Google Photos (ê³ í’ˆì§ˆ)\n- **ì„¤ëª… ê¸¸ì´**: í‰ê·  268ì (ëª©í‘œ ë‹¬ì„±)\n\n### ğŸš€ **ì‹œìŠ¤í…œ ìƒíƒœ**\n\n- âœ… ìŠ¤ì¼€ì¤„ëŸ¬: ì •ìƒ ì‘ë™ (íƒ€ì„ì¡´ ìˆ˜ì • ì™„ë£Œ)\n- âœ… ì„œë²„: ì •ìƒ ì‘ë™\n- âœ… ë°ì´í„°ë² ì´ìŠ¤: 23ê°œ ë ˆìŠ¤í† ë‘\n- âœ… ë‹¤ìŒ ìë™ ìˆ˜ì§‘: 2025-11-04 03:00 KST\n\n---\n\n**ë³´ê³ ì„œ ì‘ì„±**: 2025-11-03 11:38 KST\n**ì‘ì„±ì**: Hansikdang Data Hub System\n**ë²„ì „**: 1.0.0\n","size_bytes":7078},"data-hub/TEST_REPORT_2025-11-03.md":{"content":"# ğŸ“Š ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ\n## 2025-11-03 ì „ì²´ ì§„ë‹¨ ê²°ê³¼\n\n---\n\n## ğŸ“‹ **ìš”ì•½**\n\n| í•­ëª© | ìƒíƒœ | ê²°ê³¼ |\n|------|------|------|\n| **ë„¤ì´ë²„ ìˆ˜ì§‘** | âœ… ì„±ê³µ | 5/5 ì‹ ê·œ ë ˆìŠ¤í† ë‘ ìˆ˜ì§‘ |\n| **Google Photos API** | âœ… ì‘ë™ | ì´ë¯¸ì§€ URL ìˆ˜ì§‘ í™•ì¸ |\n| **Gemini AI ì²˜ë¦¬** | âœ… ì‘ë™ | ì„¤ëª… 272ì ìƒì„± |\n| **íƒ€ì„ì¡´ ì„¤ì •** | âš ï¸ ë¬¸ì œ | UTC ì‚¬ìš© (KST ë³€í™˜ í•„ìš”) |\n| **ìë™ ìŠ¤ì¼€ì¤„** | âš ï¸ ë¯¸ì‹¤í–‰ | 11/2 ìˆ˜ì§‘ ì—†ìŒ (íƒ€ì„ì¡´ ë¬¸ì œ) |\n\n---\n\n## 1ï¸âƒ£ **11/1-11/3 ë¡œê·¸ ë¶„ì„**\n\n### **11/1 ë¡œê·¸** (`scheduler.2025-11-01_07-06-59_096870.log`)\n\n```\n2025-11-01 07:06:59 UTC - ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘\n2025-11-01 08:00:03 UTC - ë™ê¸°í™” ì‹¤í–‰ (No restaurants to sync)\n2025-11-01 08:30:23 UTC - ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ì‹œì‘ (6ê°€ì§€ ìˆ˜ì • ì ìš©)\n```\n\n**ì‹¤í–‰ ë‚´ì—­**:\n- âœ… 07:06 UTC (KST 16:06) - ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘\n- âœ… 08:00 UTC (KST 17:00) - ë™ê¸°í™” ì‹¤í–‰\n- âœ… 08:30 UTC (KST 17:30) - ì¬ì‹œì‘ (ìˆ˜ì • ì ìš©)\n\n**í†µê³„**:\n- Total raw: 18\n- Total processed: 18\n- Total synced: 16\n- Pending: 0\n\n### **11/2 ë¡œê·¸**\nâŒ **ë¡œê·¸ ì—†ìŒ - ìë™ ìˆ˜ì§‘ ë¯¸ì‹¤í–‰**\n\n**ì›ì¸**: íƒ€ì„ì¡´ ë¬¸ì œë¡œ ì¸í•´ ì˜ˆìƒ ì‹œê°(KST 03:00)ì— ì‹¤í–‰ ì•ˆ ë¨\n\n### **11/3 ë¡œê·¸** (`Scheduler_20251103_020906_781.log`)\n\n```\n2025-11-03 02:08:04 UTC - ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘\n2025-11-03 02:08:09 UTC - í†µê³„ ë¡œê¹…\n```\n\n**í†µê³„**:\n- Total raw: 18 (ë³€ë™ ì—†ìŒ)\n- Total processed: 18\n- Total synced: 16\n- Daily target: 333\n\n### **ì—ëŸ¬ ë¡œê·¸** (`scheduler_error.log`)\n```\n2025-10-28 09:22:00 - ê³¼ê±° ì—ëŸ¬ (8ì¤„, 3.4KB)\n```\n\nâœ… **11/1-11/3 ê¸°ê°„ ì—ëŸ¬ ì—†ìŒ**\n\n---\n\n## 2ï¸âƒ£ **íƒ€ì„ì¡´ ì„¤ì • í™•ì¸**\n\n### **ì‹œìŠ¤í…œ ì„¤ì •**\n```bash\nì‹œìŠ¤í…œ íƒ€ì„ì¡´: UTC\ní˜„ì¬ ì‹œê°: 2025-11-03 02:19 AM UTC\ní˜„ì¬ ì‹œê° (KST): 2025-11-03 11:19 AM\n```\n\n### **ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •**\n```python\n# scheduler.py\nschedule.every().day.at(\"03:00\")  # UTC ê¸°ì¤€!\n```\n\n### **ë¬¸ì œì **\n| ì„¤ì • ì‹œê° | UTC | KST (ì‹¤ì œ ì‹¤í–‰) |\n|----------|-----|----------------|\n| 03:00 | 03:00 AM | **12:00 PM (ë‚® 12ì‹œ)** âŒ |\n| 06:00 | 06:00 AM | 03:00 PM (ì˜¤í›„ 3ì‹œ) âŒ |\n| 07:00 | 07:00 AM | 04:00 PM (ì˜¤í›„ 4ì‹œ) âŒ |\n| 08:00 | 08:00 AM | 05:00 PM (ì˜¤í›„ 5ì‹œ) âŒ |\n\n**ê²°ë¡ **: âš ï¸ **ëª¨ë“  ìŠ¤ì¼€ì¤„ì´ 9ì‹œê°„ ëŠ¦ê²Œ ì‹¤í–‰ë¨**\n\n### **ë‹¤ìŒ ì‹¤í–‰ ì˜ˆì •**\n- **UTC 03:00** = **2025-11-03 12:00 KST (ë‚® 12ì‹œ)**\n- ì•½ 50ë¶„ í›„ ì‹¤í–‰ ì˜ˆì •\n\n---\n\n## 3ï¸âƒ£ **ìˆ˜ë™ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ê²°ê³¼**\n\n### **í…ŒìŠ¤íŠ¸ 1: ê°•ë‚¨ í•œì‹ë‹¹ (ì¤‘ë³µ ì²´í¬)**\n```\nì¿¼ë¦¬: ê°•ë‚¨ í•œì‹ë‹¹\në°œê²¬: 3ê°œ\nì €ì¥: 0ê°œ (ëª¨ë‘ ì¤‘ë³µ)\nì¤‘ë³µ: ì§€ê°•í•œì‹ë‹¹, ë¬´ì›”ì‹íƒ, ëŒì‚°ë“±ëŒ€ì§‘\n```\n\nâœ… **ì¤‘ë³µ ì²´í¬ ì •ìƒ ì‘ë™**\n\n### **í…ŒìŠ¤íŠ¸ 2: ì¢…ë¡œ í•œì‹ë‹¹ (ì‹ ê·œ ìˆ˜ì§‘)**\n```\nì¿¼ë¦¬: ì¢…ë¡œ í•œì‹ë‹¹\në°œê²¬: 5ê°œ\nì €ì¥: 5ê°œ âœ…\nì¤‘ë³µ: 0ê°œ\n\nìˆ˜ì§‘ëœ ë ˆìŠ¤í† ë‘:\n1. ê½ƒë°¥ì—í”¼ë‹¤ (ì¢…ë¡œêµ¬)\n2. ê³ ê¶ì˜ì•„ì¹¨ ì„œìš¸ëŒ€ë³‘ì› (ì¢…ë¡œêµ¬)\n3. ìµì„ ì• ëœ» (ì¢…ë¡œêµ¬)\n4. ì„¤ê°€ì˜¨ (ì¢…ë¡œêµ¬)\n5. ìµœëŒ€ê°ë„¤ (ì¢…ë¡œêµ¬)\n```\n\nâœ… **ë„¤ì´ë²„ Maps API ìˆ˜ì§‘ ì„±ê³µ: 100%**\n\n---\n\n## 4ï¸âƒ£ **Google Places API Photos í…ŒìŠ¤íŠ¸**\n\n### **í…ŒìŠ¤íŠ¸ ëŒ€ìƒ**\në°©ê¸ˆ ìˆ˜ì§‘í•œ ì¢…ë¡œ ë ˆìŠ¤í† ë‘ 2ê°œ\n\n### **í…ŒìŠ¤íŠ¸ ê²°ê³¼**\n\n#### **ë ˆìŠ¤í† ë‘ 1: ê½ƒë°¥ì—í”¼ë‹¤**\n```\nâœ… ê²€ìƒ‰ ì„±ê³µ\ní‰ì : 4.0/5.0\në¦¬ë·°ìˆ˜: 642ê°œ\nì´ë¯¸ì§€ URLs: 1ê°œ ìˆ˜ì§‘\nì´ë¯¸ì§€ URL: https://maps.googleapis.com/maps/api/place/photo?maxwidth=800&photo_reference=AW...\n```\n\n#### **ë ˆìŠ¤í† ë‘ 2: ê³ ê¶ì˜ì•„ì¹¨**\n```\nâœ… ê²€ìƒ‰ ì„±ê³µ\ní‰ì : 3.7/5.0\në¦¬ë·°ìˆ˜: 400ê°œ\nì´ë¯¸ì§€ URLs: 1ê°œ ìˆ˜ì§‘\nì´ë¯¸ì§€ URL: https://maps.googleapis.com/maps/api/place/photo?maxwidth=800&photo_reference=AW...\n```\n\n### **ë¶„ì„**\nâœ… **photos í•„ë“œ ìˆ˜ì • ì„±ê³µ**\n- Google Places APIê°€ ì´ë¯¸ì§€ URL ë°˜í™˜\n- photo_referenceë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë§í¬ ìƒì„± í™•ì¸\n- í‰ì /ë¦¬ë·°ìˆ˜ë„ í•¨ê»˜ ìˆ˜ì§‘\n\nâš ï¸ **ê°œì„  í•„ìš”**\n- í˜„ì¬ 1ê°œë§Œ ìˆ˜ì§‘ (ëª©í‘œ: ìµœëŒ€ 5ê°œ)\n- Google API ì‘ë‹µì— photos ë°°ì—´ì´ 1ê°œë§Œ í¬í•¨ë˜ì—ˆì„ ê°€ëŠ¥ì„±\n- ë˜ëŠ” ë£¨í”„ ë¡œì§ í™•ì¸ í•„ìš”\n\n---\n\n## 5ï¸âƒ£ **Gemini AI ì²˜ë¦¬ í…ŒìŠ¤íŠ¸**\n\n### **í…ŒìŠ¤íŠ¸ ëŒ€ìƒ**\n- ë ˆìŠ¤í† ë‘: ê½ƒë°¥ì—í”¼ë‹¤\n- ì£¼ì†Œ: ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ê´€í›ˆë™ 118-27\n\n### **ì²˜ë¦¬ ê²°ê³¼**\n\n#### **ì •ì œ ë°ì´í„°**\n```\nâœ… Gemini ì •ì œ ì™„ë£Œ\nì¹´í…Œê³ ë¦¬: í•œì‹\nì§€ì—­êµ¬: ì¢…ë¡œêµ¬\ní•œê¸€ ì„¤ëª…: 272ì âœ… (ëª©í‘œ: 200-300ì)\nì˜ë¬¸ ì„¤ëª…: 542ì (ëª©í‘œ: 150-200ì ì´ˆê³¼)\n```\n\n#### **í’ˆì§ˆ ì ìˆ˜**\n```\nì´ì : 20/100\nìƒì„¸:\n  - GPS: 20ì  âœ…\n  - Reviews: 0ì  (ë°ì´í„° ì—†ìŒ)\n  - Images: 0ì  (ì•„ì§ Google ë³´ê°• ì „)\n  - Hours: 0ì  (ë°ì´í„° ì—†ìŒ)\n  - Menu: 0ì  (ë°ì´í„° ì—†ìŒ)\n```\n\n#### **ì¸ê¸°ì§€ìˆ˜**\n```\nNaver í‰ì : 0.0 (ë°ì´í„° ì—†ìŒ)\nGoogle í‰ì : 0.0 (ì•„ì§ ë³´ê°• ì „)\nì¸ê¸°ì§€ìˆ˜: 0.0ì \në“±ê¸‰: new_or_limited\n```\n\n### **ë¶„ì„**\nâœ… **Gemini ì •ì œ ì„±ê³µ**\n- qualityScore ê³ ì •ê°’ ì œê±° í™•ì¸\n- ì‹¤ì œ ê³„ì‚° ì ìˆ˜(20ì ) ì‚¬ìš©\n- ì„¤ëª… ê¸¸ì´ ëª©í‘œ ë‹¬ì„± (272ì)\n\nâš ï¸ **Google ë³´ê°• í•„ìš”**\n- í‰ì /ë¦¬ë·°ìˆ˜ ë°ì´í„° ì—†ìŒ (ì•„ì§ Google enrichment ì „)\n- í’ˆì§ˆ ì ìˆ˜ ë‚®ìŒ (20/100)\n- ë‹¤ìŒ ë‹¨ê³„: Google enrichment ì‹¤í–‰ í•„ìš”\n\n---\n\n## ğŸ” **ì „ì²´ ì‹œìŠ¤í…œ í˜„í™©**\n\n### **ë°ì´í„°ë² ì´ìŠ¤ í†µê³„** (2025-11-03 02:20 UTC ê¸°ì¤€)\n\n| í•­ëª© | ìˆ˜ëŸ‰ | ë³€í™” |\n|------|------|------|\n| **Raw ë°ì´í„°** | 23ê°œ | +5 (ì¢…ë¡œ ìˆ˜ì§‘) |\n| **Pending** | 5ê°œ | +5 (ì¢…ë¡œ ìˆ˜ì§‘) |\n| **Processed** | 18ê°œ | ë³€ë™ ì—†ìŒ |\n| **Synced** | 16ê°œ | ë³€ë™ ì—†ìŒ |\n\n### **ìµœê·¼ ìˆ˜ì§‘ ë‚´ì—­**\n```\n2025-11-03 02:20 - ì¢…ë¡œ í•œì‹ë‹¹ 5ê°œ âœ… (ìˆ˜ë™ í…ŒìŠ¤íŠ¸)\n2025-10-31 03:00 - 2ê°œ\n2025-10-30 03:00 - 16ê°œ\n```\n\n### **ì´ë¯¸ì§€ ìˆ˜ì§‘ë¥ **\n```\nì „ì²´ 18ê°œ processed ë ˆìŠ¤í† ë‘:\n- ì‹¤ì œ ì´ë¯¸ì§€: 0ê°œ (0%)\n- Placeholder: 18ê°œ (100%)\n```\n\nâš ï¸ **Google enrichment ì•„ì§ ì‹¤í–‰ ì•ˆ ë¨**\n\n---\n\n## âš ï¸ **ë°œê²¬ëœ ë¬¸ì œ**\n\n### **1. íƒ€ì„ì¡´ ë¬¸ì œ (Critical)**\n**ë¬¸ì œ**: ìŠ¤ì¼€ì¤„ëŸ¬ê°€ UTC ê¸°ì¤€ìœ¼ë¡œ ì‘ë™\n**ì˜í–¥**: \n- ìƒˆë²½ 03:00 ì˜ˆìƒ â†’ ì‹¤ì œ ë‚® 12:00 ì‹¤í–‰\n- 11/2 ìë™ ìˆ˜ì§‘ ëˆ„ë½\n- ë‹¤ìŒ ì‹¤í–‰: 11/3 12:00 (ì•½ 50ë¶„ í›„)\n\n**í•´ê²° ë°©ì•ˆ**:\n```python\n# scheduler.py ìˆ˜ì • í•„ìš”\nimport pytz\nKST = pytz.timezone('Asia/Seoul')\n\n# ë˜ëŠ” UTC ì‹œê°ìœ¼ë¡œ ë³€í™˜\nschedule.every().day.at(\"18:00\")  # UTC 18:00 = KST 03:00\n```\n\n### **2. Google ì´ë¯¸ì§€ 1ê°œë§Œ ìˆ˜ì§‘**\n**ë¬¸ì œ**: photos ë°°ì—´ì—ì„œ 1ê°œë§Œ ì¶”ì¶œ\n**ì˜ˆìƒ ì›ì¸**: API ì‘ë‹µì— 1ê°œë§Œ í¬í•¨ ë˜ëŠ” ë£¨í”„ ì œí•œ\n**í•´ê²° í•„ìš”**: ì½”ë“œ ì¬í™•ì¸\n\n### **3. Google Enrichment ë¯¸ì‹¤í–‰**\n**ë¬¸ì œ**: ì‹ ê·œ ë ˆìŠ¤í† ë‘ì— Google ë°ì´í„° ì—†ìŒ\n**í•´ê²° ë°©ì•ˆ**: ìˆ˜ë™ìœ¼ë¡œ Google enrichment ì‹¤í–‰ í•„ìš”\n\n---\n\n## âœ… **ìˆ˜ì • ì‚¬í•­ ê²€ì¦**\n\n### **11/1 ìˆ˜ì •í•œ 6ê°€ì§€ í•­ëª©**\n\n| ìˆ˜ì • í•­ëª© | ìƒíƒœ | ê²€ì¦ ê²°ê³¼ |\n|----------|------|----------|\n| 1. Google Photos í•„ë“œ | âœ… | ì´ë¯¸ì§€ URL ìˆ˜ì§‘ í™•ì¸ |\n| 2. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° | âœ… | 20ì  (ê³ ì •ê°’ ì•„ë‹˜) |\n| 3. ì¿¼ë¦¬ ì „ì²´ í™œì„±í™” | âœ… | [:3] ì œê±° í™•ì¸ |\n| 4. Gemini í”„ë¡¬í”„íŠ¸ | âœ… | qualityScore ì œê±° |\n| 5. ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ì‹œì‘ | âœ… | ì •ìƒ ì‘ë™ ì¤‘ |\n| 6. ì„¤ëª… ê¸¸ì´ | âœ… | 272ì (ëª©í‘œ ë‹¬ì„±) |\n\n---\n\n## ğŸ“Š **ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­**\n\n### **ì¦‰ì‹œ ì‹¤í–‰ (ê¸´ê¸‰)**\n1. âš ï¸ **íƒ€ì„ì¡´ ìˆ˜ì •** - UTC â†’ KST ë³€í™˜\n2. ğŸ”§ **ìˆ˜ë™ Google Enrichment** - 23ê°œ ë ˆìŠ¤í† ë‘ ë³´ê°•\n3. ğŸ§ª **íƒ€ì„ì¡´ ìˆ˜ì • í›„ ì¬í…ŒìŠ¤íŠ¸**\n\n### **ëª¨ë‹ˆí„°ë§ (11/3 ë‚® 12ì‹œ)**\n1. ğŸ“Š ì²« ìë™ ìˆ˜ì§‘ ëª¨ë‹ˆí„°ë§ (UTC 03:00)\n2. ğŸ“ˆ ìˆ˜ì§‘ ê°œìˆ˜ í™•ì¸ (ëª©í‘œ: 33ê°œ)\n3. ğŸ“¸ Google ì´ë¯¸ì§€ ìˆ˜ì§‘ë¥  í™•ì¸\n\n### **ì¶”ê°€ ê°œì„ **\n1. Google Photos 5ê°œ ìˆ˜ì§‘ ê²€ì¦\n2. ì˜ë¬¸ ì„¤ëª… ê¸¸ì´ ì¡°ì • (542ì â†’ 150-200ì)\n3. ë¡œê·¸ ìë™ ì••ì¶•/ë³´ê´€\n\n---\n\n## ğŸ“Œ **ê²°ë¡ **\n\n### **âœ… ì‘ë™í•˜ëŠ” ê²ƒ**\n- ë„¤ì´ë²„ Maps API ìˆ˜ì§‘ (100% ì„±ê³µ)\n- Google Places API Photos í•„ë“œ (ì´ë¯¸ì§€ URL ìˆ˜ì§‘)\n- Gemini AI ì •ì œ (ì„¤ëª… 272ì, í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°)\n- ì¤‘ë³µ ì²´í¬ ì‹œìŠ¤í…œ\n- ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥\n\n### **âš ï¸ ìˆ˜ì • í•„ìš”**\n- íƒ€ì„ì¡´ ì„¤ì • (UTC â†’ KST)\n- Google Enrichment ì‹¤í–‰\n- Google Photos ìµœëŒ€ 5ê°œ ìˆ˜ì§‘ ê²€ì¦\n\n### **ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ**\n**í˜„ì¬**: 23ê°œ ë ˆìŠ¤í† ë‘ (5ê°œ pending, 18ê°œ processed)\n**ë‹¤ìŒ ì‹¤í–‰**: 11/3 12:00 KST (50ë¶„ í›„)\n**ì˜ˆìƒ ê²°ê³¼**: íƒ€ì„ì¡´ ìˆ˜ì • í›„ ì •ìƒ ìš´ì˜ ê°€ëŠ¥\n\n---\n\n**ë³´ê³ ì„œ ì‘ì„±**: 2025-11-03 02:22 UTC (11:22 KST)\n**í…ŒìŠ¤íŠ¸ ë‹´ë‹¹**: Replit Agent\n**ì‹œìŠ¤í…œ ë²„ì „**: 0.2.0\n","size_bytes":8406},"data-hub/replit.md":{"content":"\n## Recent Changes\n\n### **2025-11-08**: ğŸ¨ UI/UX ì¬êµ¬ì¡°í™” ì™„ë£Œ - ì¢Œì¸¡ ì‚¬ì´ë“œë°” ë„¤ë¹„ê²Œì´ì…˜ í†µí•© â­ MAJOR UPDATE\n\n**ê´€ë¦¬ì ì¸í„°í˜ì´ìŠ¤ ì „ë©´ ê°œí¸**\n\n- âœ… **ì¢Œì¸¡ ì‚¬ì´ë“œë°” ë„¤ë¹„ê²Œì´ì…˜ ì‹œìŠ¤í…œ (240px Dark Theme)**\n  - 5ê°œ ë©”ì¸ ì„¹ì…˜: ëŒ€ì‹œë³´ë“œ, ë°ì´í„° ê´€ë¦¬, ì‘ì—… ê´€ë¦¬, ë™ê¸°í™” ê´€ë¦¬, ë¶„ì„, ì„¤ì •\n  - 9ê°œ ì„œë¸Œ ë©”ë‰´: ìˆ˜ì§‘ ì„¤ì •, ìˆ˜ì§‘ ê²°ê³¼, ì¤‘ë³µ ê²€ì‚¬, í’ˆì§ˆ ê´€ë¦¬, ì‘ì—… ì‹¤í–‰, ëª¨ë‹ˆí„°ë§, ì‘ì—… ì´ë ¥, ë°°í¬ í˜„í™©, ë°°í¬ ì´ë ¥\n  - Active ìƒíƒœ í•˜ì´ë¼ì´íŠ¸ & í˜¸ë²„ ì• ë‹ˆë©”ì´ì…˜\n  - ëª¨ë“  í˜ì´ì§€ì— ì¼ê´€ëœ ë„¤ë¹„ê²Œì´ì…˜ ì ìš©\n\n- âœ… **4ê°œ ì‹ ê·œ í˜ì´ì§€ ìƒì„±**\n  1. `/dashboard/jobs` - ì‘ì—… ê´€ë¦¬ (3ê°œ íƒ­: ì‘ì—… ì‹¤í–‰, ëª¨ë‹ˆí„°ë§, ì‘ì—… ì´ë ¥)\n     - 7ê°œ ì‘ì—… ì¹´ë“œ: Smart Targeting, Naver ìˆ˜ì§‘, ì¤‘ë³µ ê²€ì‚¬, AI ì²˜ë¦¬, Google í‰ì , í”Œë«í¼ ë™ê¸°í™”\n     - ì›í´ë¦­ ìˆ˜ë™ ì‹¤í–‰ ë²„íŠ¼\n  2. `/dashboard/sync` - ë™ê¸°í™” ê´€ë¦¬ (2ê°œ íƒ­: ë°°í¬ í˜„í™©, ë°°í¬ ì´ë ¥)\n     - ì‹¤ì‹œê°„ í†µê³„: ì „ì²´/ì™„ë£Œ/ëŒ€ê¸°/ë™ê¸°í™”ìœ¨\n     - sync-management ê¸°ëŠ¥ í™œìš©\n  3. `/dashboard/analytics` - ë°ì´í„° ë¶„ì„ (Stage C ì˜ˆì •)\n  4. `/dashboard/settings` - ì‹œìŠ¤í…œ ì„¤ì • (Stage C ì˜ˆì •)\n\n- âœ… **ê¸°ì¡´ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ í†µí•©**\n  - `/dashboard` - ë©”ì¸ ëŒ€ì‹œë³´ë“œ\n  - `/dashboard/data-management` - ë°ì´í„° ê´€ë¦¬ (4ê°œ íƒ­)\n  - `/dashboard/collection-settings` - ìˆ˜ì§‘ ì„¤ì •\n  - `/dashboard/quality-check` - í’ˆì§ˆ ê´€ë¦¬\n  - `/dashboard/sync-management` - ë°°ì¹˜ ë™ê¸°í™”\n\n- ğŸ“Š **ê°œì„  íš¨ê³¼**\n  - í˜ì´ì§€ ê°„ ì¼ê´€ëœ ì‚¬ìš©ì ê²½í—˜\n  - ì§ê´€ì ì¸ ë©”ë‰´ êµ¬ì¡° (ê³„ì¸µì  ë„¤ë¹„ê²Œì´ì…˜)\n  - ê´€ë¦¬ì ì›Œí¬í”Œë¡œìš° ìµœì í™”\n  - Stage C í™•ì¥ ì¤€ë¹„ ì™„ë£Œ\n\n### **2025-11-08**: ğŸ“Š ëŒ€ì‹œë³´ë“œ ì—…ê·¸ë ˆì´ë“œ (40% â†’ 95%) â­ MAJOR UPDATE\n\n**ì›¹ ê¸°ë°˜ ìš´ì˜ ëŒ€ì‹œë³´ë“œ ì™„ì „ ê°œí¸ - ì™¸ë¶€ ì»¨ì„¤í„´íŠ¸ ê¶Œì¥ì‚¬í•­ ë°˜ì˜**\n\n- âœ… **Phase 1: ë ˆìŠ¤í† ë‘ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ (0% â†’ 100%)**\n  - ì „ìš© ë°ì´í„° ê´€ë¦¬ í˜ì´ì§€: `/dashboard/data`\n  - CRUD API ì—”ë“œí¬ì¸íŠ¸: GET /api/restaurants (ê²€ìƒ‰, í•„í„°, ì •ë ¬, í˜ì´ì§€ë„¤ì´ì…˜)\n  - PUT /api/restaurants/{id} (ë°ì´í„° ìˆ˜ì •)\n  - DELETE /api/restaurants/{id} (ë°ì´í„° ì‚­ì œ)\n  - Vue.js ê¸°ë°˜ ì¸í„°ë™í‹°ë¸Œ UI (ëª¨ë‹¬, ì‹¤ì‹œê°„ ê²€ìƒ‰, ìƒì„¸ ë³´ê¸°)\n  - 59ê°œ ë ˆìŠ¤í† ë‘ ì‹¤ì‹œê°„ ê´€ë¦¬ ê°€ëŠ¥\n\n- âœ… **Phase 2: ì‘ì—… ì‹¤í–‰ ì œì–´ ì„¼í„° (5% â†’ 100%)**\n  - 7ê°œ ìë™í™” ì‘ì—… ìˆ˜ë™ ì‹¤í–‰ API:\n    - POST /api/jobs/targeting/run (Smart Targeting)\n    - POST /api/jobs/scraping/run (Naver ìŠ¤í¬ë˜í•‘)\n    - POST /api/jobs/deduplication/run (ì¤‘ë³µ ì œê±°)\n    - POST /api/jobs/gemini/run (Gemini AI ì •ì œ)\n    - POST /api/jobs/places/run (Google Places ë³´ê°•)\n    - POST /api/jobs/sync/run (ë©”ì¸ í”Œë«í¼ ë™ê¸°í™”)\n    - POST /api/jobs/backup/run (Google Drive ë°±ì—…)\n  - GET /api/jobs/status (ëª¨ë“  ì‘ì—… ìƒíƒœ ì¡°íšŒ)\n  - ë©”ì¸ ëŒ€ì‹œë³´ë“œì— ì‘ì—… ì‹¤í–‰ ì œì–´ ì„¼í„° UI í†µí•©\n  - ê° ì‘ì—…ì˜ ì‹¤ì‹œê°„ ìƒíƒœ í‘œì‹œ (ì™„ë£Œ/ì‹¤í–‰ì¤‘/ëŒ€ê¸°/ì‹¤íŒ¨)\n  - ì›í´ë¦­ ì‘ì—… ì‹¤í–‰ ë° ìƒíƒœ ì—…ë°ì´íŠ¸\n\n- ğŸ“ˆ **ëŒ€ì‹œë³´ë“œ ê¸°ëŠ¥ ì™„ì„±ë„**\n  - ì´ì „: 40% (ì½ê¸° ì „ìš© ëª¨ë‹ˆí„°ë§ë§Œ)\n  - í˜„ì¬: 95% (ë°ì´í„° ê´€ë¦¬ + ì‹¤í–‰ ì œì–´ + ëª¨ë‹ˆí„°ë§)\n  - ì„ íƒì‚¬í•­: Phase 3 (ë°ì´í„° ê²€ì¦ ë„êµ¬ 5%)\n\n- ğŸ¯ **ì™¸ë¶€ ì»¨ì„¤í„´íŠ¸ í‰ê°€ ê²°ê³¼**\n  - ê¶Œì¥ì‚¬í•­: \"ë°ì´í„° ê´€ë¦¬ ê¸°ëŠ¥ ì¶”ê°€ (0% â†’ í•„ìˆ˜), ì‹¤í–‰ ì œì–´ í™•ëŒ€ (5% â†’ í•„ìˆ˜), ê²€ì¦ ë„êµ¬ (ì„ íƒ)\"\n  - êµ¬í˜„ ê²°ê³¼: Phase 1 & 2 ì™„ë£Œ, ëŒ€ì‹œë³´ë“œê°€ ë‹¨ìˆœ ëª¨ë‹ˆí„°ë§ì—ì„œ **ì „ì²´ ìš´ì˜ ì œì–´ ì„¼í„°**ë¡œ ë³€ëª¨\n  - ê¸°ëŒ€ íš¨ê³¼: ìˆ˜ë™ ê°œì… ì—†ì´ ë¸Œë¼ìš°ì €ì—ì„œ ëª¨ë“  ë°ì´í„° ê´€ë¦¬ ë° ì‘ì—… ì‹¤í–‰ ê°€ëŠ¥\n\n- ğŸ”— **ì ‘ì† URL**\n  - ë©”ì¸ ëŒ€ì‹œë³´ë“œ: `/dashboard` (ì‹œìŠ¤í…œ í—¬ìŠ¤, í†µê³„, ì‘ì—… ì‹¤í–‰ ì œì–´)\n  - ë°ì´í„° ê´€ë¦¬: `/dashboard/data` (ë ˆìŠ¤í† ë‘ CRUD)\n  - API ë¬¸ì„œ: `/docs` (Swagger UI)\n\n### **2025-11-06**: ğŸš€ Apify í†µí•© ì™„ë£Œ ë° ìë™í™” ìµœì í™” â­ MAJOR UPDATE\n- âœ… **Apify Naver Map Scraper ì™„ì „ í†µí•©**\n  - Actor ID: `UCpUxFUNcdKdbBdYg` (ì›” $30 êµ¬ë…)\n  - ë©”ë‰´ ë°ì´í„°: 59/59 (100%)\n  - ì „í™”ë²ˆí˜¸: 58/59 (98%, 1ê°œ ì˜¨ë¼ì¸ ë¯¸ê³µê°œ)\n  - ì˜ì—…ì‹œê°„: 59/59 (100%)\n  - ë„¤ì´ë²„ í‰ì : 59/59 (100%)\n  \n- âœ… **ì‹ ê·œ ìˆ˜ì§‘ ë°©ì‹ ë³€ê²½**\n  - ì´ì „: ë„¤ì´ë²„ Maps API (ê¸°ë³¸ ì •ë³´ë§Œ)\n  - ë³€ê²½: Apify Naver Map Scraper (ë©”ë‰´/ì „í™”ë²ˆí˜¸/ì˜ì—…ì‹œê°„ í¬í•¨)\n  - íš¨ê³¼: ì‹ ê·œ ìˆ˜ì§‘ ì‹œ ë°ì´í„° ì™„ì„±ë„ 100%\n  \n- âœ… **ì£¼ê°„ ìë™ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ ì¶”ê°€**\n  - ë§¤ì£¼ ì¼ìš”ì¼ 12:00 KST ìë™ ì‹¤í–‰\n  - ëˆ„ë½ëœ ì „í™”ë²ˆí˜¸, ë©”ë‰´, ì˜ì—…ì‹œê°„ ìë™ ë³´ì™„\n  - ë°°ì¹˜ ì²˜ë¦¬ (10ê°œì”©) ë° ì•ˆì •ì  ì»¤ë°‹\n  \n- ğŸ’° **ë¹„ìš© íš¨ìœ¨ì„±**\n  - ì›” ë¹„ìš©: $30 (Apify êµ¬ë…)\n  - ê¸°ì¡´ ë°©ì‹ ëŒ€ë¹„: 87% ì ˆê° ($360 vs $2,500-5,000/ë…„)\n  \n- ğŸ“Š **ìµœì¢… ë°ì´í„° í˜„í™© (59ê°œ ë ˆìŠ¤í† ë‘)**\n  - ë©”ë‰´: 100%, ì˜ì—…ì‹œê°„: 100%, ë„¤ì´ë²„ í‰ì : 100%\n  - ì „í™”ë²ˆí˜¸: 98%, Google í‰ì : 98%, ì„¤ëª…: 100%\n  - ì „ì²´ ì™„ì„±ë„: **99%**\n\n### **2025-11-03 (ì˜¤í›„ 12ì‹œ)**: ğŸ“¸ ì´ë¯¸ì§€ ìˆ˜ì§‘ ê°œìˆ˜ ì¡°ì •\n- âœ… **Google Photos ìˆ˜ì§‘ ê°œìˆ˜ ë³€ê²½**\n  - ì´ì „: ì—…ì²´ë‹¹ ìµœëŒ€ 5ê°œ\n  - ë³€ê²½: ì—…ì²´ë‹¹ ìµœëŒ€ 10ê°œ\n  - ì‚¬ìœ : ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì´ë¯¸ì§€ ìˆ˜ì§‘ ê°œìˆ˜ ì¦ê°€\n\n### **2025-11-03 (ì˜¤ì „ 11ì‹œ)**: ğŸ”§ íƒ€ì„ì¡´ ìˆ˜ì • ì™„ë£Œ\n- âœ… **íƒ€ì„ì¡´ ë¬¸ì œ í•´ê²° (UTC â†’ KST ë³€í™˜)**\n  - ìŠ¤ì¼€ì¤„ ì‹œê°: 18:00/21:00/22:00/23:00 UTC\n  - ì‹¤ì œ ì‹¤í–‰: KST 03:00/06:00/07:00/08:00 (ì •í™•)\n","size_bytes":5528},"data-hub/PROJECT_REPORT.md":{"content":"# í•œì‹ë‹¹ ë°ì´í„° í—ˆë¸Œ - ì¢…í•© í”„ë¡œì íŠ¸ ë³´ê³ ì„œ\n\n**Project Name:** Hansikdang Data Hub  \n**Version:** 1.0.0  \n**Report Date:** 2025-11-06  \n**Project Period:** 2025-10 ~ 2025-11\n\n---\n\n## ğŸ“‹ ëª©ì°¨\n\n1. [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)\n2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)\n3. [ê¸°ìˆ  ìŠ¤íƒ](#ê¸°ìˆ -ìŠ¤íƒ)\n4. [ì£¼ìš” ê¸°ëŠ¥](#ì£¼ìš”-ê¸°ëŠ¥)\n5. [ì‘ì—… ë‚´ì—­](#ì‘ì—…-ë‚´ì—­)\n6. [ë°ì´í„° í˜„í™©](#ë°ì´í„°-í˜„í™©)\n7. [ìš´ì˜ ë¹„ìš©](#ìš´ì˜-ë¹„ìš©)\n8. [í–¥í›„ ê³„íš](#í–¥í›„-ê³„íš)\n\n---\n\n## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”\n\n### í”„ë¡œì íŠ¸ ì†Œê°œ\n\n**í•œì‹ë‹¹ ë°ì´í„° í—ˆë¸Œ**ëŠ” í•œì‹ë‹¹ í”Œë«í¼ì„ ìœ„í•œ ëŒ€ê·œëª¨ ë ˆìŠ¤í† ë‘ ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ë„¤ì´ë²„í”Œë ˆì´ìŠ¤ì™€ êµ¬ê¸€ë§µìŠ¤ì—ì„œ ì›” 990ê°œì˜ ë ˆìŠ¤í† ë‘ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³ , Google Gemini AIë¡œ ì •ì œí•˜ì—¬ í•œì‹ë‹¹ í”Œë«í¼ê³¼ ë™ê¸°í™”í•©ë‹ˆë‹¤.\n\n### í•µì‹¬ ëª©í‘œ\n\n- **ë°ì´í„° ìˆ˜ì§‘**: ì›” 990ê°œ ë ˆìŠ¤í† ë‘ ìë™ ìˆ˜ì§‘ (ì¼ 33ê°œ)\n- **í’ˆì§ˆ ë³´ì¥**: AI ê¸°ë°˜ ë°ì´í„° ì •ì œ ë° ê²€ì¦\n- **ì™„ì „ì„±**: ë©”ë‰´, ì „í™”ë²ˆí˜¸, ì˜ì—…ì‹œê°„, í‰ì , ì´ë¯¸ì§€ ë“± ì™„ì „í•œ ì •ë³´ ì œê³µ\n- **ìë™í™”**: 24/7 ë¬´ì¸ ìš´ì˜ ì‹œìŠ¤í…œ\n- **ë¹„ìš© íš¨ìœ¨**: ì›” $30ë¡œ ì™„ì „í•œ ë°ì´í„° í™•ë³´\n\n### í”„ë¡œì íŠ¸ íŠ¹ì§•\n\n1. **ì™„ì „ ìë™í™”**: ìˆ˜ì§‘ë¶€í„° ë™ê¸°í™”ê¹Œì§€ ì‚¬ëŒ ê°œì… ì—†ì´ ìë™ ì‹¤í–‰\n2. **AI ì •ì œ**: Google Gemini 2.0 Flashë¡œ ê³ í’ˆì§ˆ ì„¤ëª… ìƒì„±\n3. **ë‹¤ì¤‘ ì†ŒìŠ¤**: Apify Naver Map + Google Places API í™œìš©\n4. **ì‹¤ì‹œê°„ ë™ê¸°í™”**: ë©”ì¸ í”Œë«í¼ê³¼ ìë™ ì—°ë™\n5. **í™•ì¥ ê°€ëŠ¥**: í•„ìš”ì‹œ ì›” ìˆ˜ì§‘ëŸ‰ ì¦ëŒ€ ê°€ëŠ¥\n\n---\n\n## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜\n\n### ì „ì²´ êµ¬ì¡°ë„\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ë°ì´í„° ìˆ˜ì§‘ ë ˆì´ì–´                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Apify Naver Map Scraper    â”‚    Google Places API          â”‚\nâ”‚  - ë ˆìŠ¤í† ë‘ ê¸°ë³¸ ì •ë³´        â”‚    - Google í‰ì /ë¦¬ë·°         â”‚\nâ”‚  - ë©”ë‰´ ì •ë³´                â”‚    - ì´ë¯¸ì§€ URL (ìµœëŒ€ 10ê°œ)   â”‚\nâ”‚  - ì „í™”ë²ˆí˜¸                 â”‚    - ì¶”ê°€ ë©”íƒ€ë°ì´í„°          â”‚\nâ”‚  - ì˜ì—…ì‹œê°„                 â”‚                               â”‚\nâ”‚  - ë„¤ì´ë²„ í‰ì /ë¦¬ë·°         â”‚                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ë°ì´í„° ì²˜ë¦¬ ë ˆì´ì–´                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚             Google Gemini 2.0 Flash AI                       â”‚\nâ”‚  - ì„¤ëª… ì •ì œ (200-300ì)                                     â”‚\nâ”‚  - í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)                                    â”‚\nâ”‚  - ì¸ê¸°ë„ í‹°ì–´ ë¶„ë¥˜                                          â”‚\nâ”‚  - í™˜ê° ë°©ì§€ ê²€ì¦                                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ë°ì´í„° ì €ì¥ ë ˆì´ì–´                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚              PostgreSQL Database (Replit)                    â”‚\nâ”‚  - raw_restaurant_data (ìˆ˜ì§‘ ì›ë³¸)                           â”‚\nâ”‚  - processed_restaurants (ì •ì œ ë°ì´í„°)                       â”‚\nâ”‚  - scraping_targets (ìˆ˜ì§‘ ëŒ€ìƒ ê´€ë¦¬)                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ë™ê¸°í™” ë ˆì´ì–´                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚           í•œì‹ë‹¹ ë©”ì¸ í”Œë«í¼ API (X-API-Key ì¸ì¦)            â”‚\nâ”‚  - ì‹ ê·œ ë ˆìŠ¤í† ë‘ ë“±ë¡                                        â”‚\nâ”‚  - ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸                                      â”‚\nâ”‚  - ì‹¤ì‹œê°„ ë™ê¸°í™”                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### ë°ì´í„° í”Œë¡œìš°\n\n```\n1. ìˆ˜ì§‘ (Daily 03:00 KST)\n   Apify Naver Map Scraper â†’ Raw Data (33ê°œ/ì¼)\n   â†“\n2. ì •ì œ (Daily 06:00 KST)\n   Gemini AI â†’ Processed Data (ì„¤ëª…, í’ˆì§ˆ ì ìˆ˜)\n   â†“\n3. ë³´ê°• (Daily 07:00 KST)\n   Google Places API â†’ Google í‰ì , ì´ë¯¸ì§€\n   â†“\n4. ë™ê¸°í™” (Daily 08:00 KST)\n   ë©”ì¸ í”Œë«í¼ API â†’ í”Œë«í¼ DB ì—…ë°ì´íŠ¸\n   â†“\n5. ì£¼ê°„ ê²€ì¦ (Weekly Sunday 12:00 KST)\n   ëˆ„ë½ ë°ì´í„° ìë™ ë³´ì™„ â†’ ì™„ì „ì„± ë³´ì¥\n```\n\n---\n\n## ğŸ’» ê¸°ìˆ  ìŠ¤íƒ\n\n### Backend\n\n| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |\n|------|------|------|\n| **Python** | 3.11 | ë©”ì¸ ì–¸ì–´ |\n| **FastAPI** | 0.109.0 | REST API ì„œë²„ |\n| **Uvicorn** | - | ASGI ì„œë²„ |\n| **SQLAlchemy** | - | ORM |\n| **PostgreSQL** | - | ë°ì´í„°ë² ì´ìŠ¤ |\n| **Schedule** | - | ì‘ì—… ìŠ¤ì¼€ì¤„ë§ |\n\n### AI & Data Processing\n\n| ì„œë¹„ìŠ¤ | ìš©ë„ | ë¹„ìš© |\n|--------|------|------|\n| **Google Gemini 2.0 Flash** | ë°ì´í„° ì •ì œ, ì„¤ëª… ìƒì„± | ë¬´ë£Œ (í• ë‹¹ëŸ‰ ë‚´) |\n| **Apify Naver Map Scraper** | ë„¤ì´ë²„ ë°ì´í„° ìˆ˜ì§‘ | $30/ì›” |\n| **Google Places API** | Google í‰ì , ì´ë¯¸ì§€ | ë¬´ë£Œ (í• ë‹¹ëŸ‰ ë‚´) |\n\n### Infrastructure\n\n- **Hosting**: Replit (24/7 ìš´ì˜)\n- **Database**: PostgreSQL (Replit ë‚´ì¥)\n- **Logging**: Loguru (íŒŒì¼ ë¡œê·¸ + ì½˜ì†”)\n- **API Documentation**: Swagger UI (FastAPI ë‚´ì¥)\n\n---\n\n## âš™ï¸ ì£¼ìš” ê¸°ëŠ¥\n\n### 1. ìë™ ë°ì´í„° ìˆ˜ì§‘ (Apify Integration)\n\n**ê¸°ëŠ¥:**\n- Apify Naver Map Scraperë¥¼ í†µí•œ ì™„ì „í•œ ë°ì´í„° ìˆ˜ì§‘\n- ë©”ë‰´, ì „í™”ë²ˆí˜¸, ì˜ì—…ì‹œê°„, ë„¤ì´ë²„ í‰ì  ë“± í¬ê´„ì  ì •ë³´\n- ì¼ì¼ 33ê°œ ë ˆìŠ¤í† ë‘ ìë™ ìˆ˜ì§‘ (ì›” 990ê°œ)\n\n**ì£¼ìš” ì½”ë“œ:**\n```python\nasync def scrape_naver_daily():\n    \"\"\"Apify Naver Map Scraperë¡œ ë§¤ì¼ 33ê°œ ìˆ˜ì§‘\"\"\"\n    apify = ApifyNaverScraper()\n    results = await apify.search_restaurants(search_queries, max_results=11)\n    # ë©”ë‰´, ì „í™”ë²ˆí˜¸, ì˜ì—…ì‹œê°„ í¬í•¨ ì €ì¥\n```\n\n### 2. AI ê¸°ë°˜ ë°ì´í„° ì •ì œ (Gemini AI)\n\n**ê¸°ëŠ¥:**\n- Google Gemini 2.0 Flashë¥¼ í™œìš©í•œ ê³ í’ˆì§ˆ ì„¤ëª… ìƒì„±\n- 200-300ì í•œêµ­ì–´ ì„¤ëª… (ì›ë³¸ ê¸°ë°˜, í™˜ê° ë°©ì§€)\n- í’ˆì§ˆ ì ìˆ˜ (0-100) ë° ì¸ê¸°ë„ í‹°ì–´ ìë™ ê³„ì‚°\n\n**í’ˆì§ˆ ê²€ì¦:**\n- ì„¤ëª… ê¸¸ì´: 200ì ì´ìƒ (í‰ê·  250ì)\n- í™˜ê° ë°©ì§€: ì›ë³¸ ë°ì´í„° ê¸°ë°˜ ê²€ì¦\n- Rate Limiting: 429 ì—ëŸ¬ ìë™ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)\n\n**ì£¼ìš” ì½”ë“œ:**\n```python\nasync def refine_restaurant_data(raw_data):\n    \"\"\"Gemini AIë¡œ ë°ì´í„° ì •ì œ\"\"\"\n    prompt = f\"\"\"\n    ë‹¤ìŒ ë ˆìŠ¤í† ë‘ ì •ë³´ë¥¼ 200-300ìë¡œ ì •ì œí•˜ì„¸ìš”.\n    í™˜ê°(hallucination) ê¸ˆì§€, ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©.\n    \"\"\"\n    refined = await gemini.generate_content(prompt)\n```\n\n### 3. Google Places API í†µí•©\n\n**ê¸°ëŠ¥:**\n- Google í‰ì  ë° ë¦¬ë·° ìˆ˜ ìˆ˜ì§‘\n- ë ˆìŠ¤í† ë‘ ì´ë¯¸ì§€ URL (ìµœëŒ€ 10ê°œ)\n- ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° (Google + Naver í‰ì  ì¢…í•©)\n\n**ì¸ê¸°ë„ ê³„ì‚° ê³µì‹:**\n```python\npopularity_score = (\n    (google_rating / 5.0) * 40 +\n    min(log10(google_reviews + 1) * 10, 30) +\n    (naver_rating / 5.0) * 20 +\n    min(log10(naver_reviews + 1) * 5, 10)\n)\n```\n\n### 4. 24/7 ìë™í™” ìŠ¤ì¼€ì¤„ëŸ¬\n\n**ì¼ì¼ ìŠ¤ì¼€ì¤„ (KST):**\n- **03:00** - Naver ë°ì´í„° ìˆ˜ì§‘ (33ê°œ)\n- **06:00** - Gemini AI ì •ì œ\n- **07:00** - Google í‰ì  ë³´ê°•\n- **08:00** - ë©”ì¸ í”Œë«í¼ ë™ê¸°í™”\n- **ë§¤ì‹œê°„** - í†µê³„ ë¡œê¹…\n\n**ì£¼ê°„ ìŠ¤ì¼€ì¤„ (KST):**\n- **ì¼ìš”ì¼ 12:00** - ì „ì²´ ë°ì´í„° ê²€ì¦ ë° ëˆ„ë½ ì •ë³´ ë³´ì™„\n\n**ì£¼ìš” ì½”ë“œ:**\n```python\ndef setup_schedule():\n    schedule.every().day.at(\"18:00\").do(scrape_naver_job)  # UTC 18:00 = KST 03:00\n    schedule.every().day.at(\"21:00\").do(process_job)\n    schedule.every().day.at(\"22:00\").do(google_enrichment_job)\n    schedule.every().day.at(\"23:00\").do(sync_job)\n    schedule.every().sunday.at(\"03:00\").do(weekly_update_job)  # ì£¼ê°„ ê²€ì¦\n```\n\n### 5. REST API ì„œë²„\n\n**ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸:**\n\n| Method | Endpoint | ì„¤ëª… |\n|--------|----------|------|\n| GET | `/` | í™˜ì˜ ë©”ì‹œì§€ ë° ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡ |\n| GET | `/health` | í—¬ìŠ¤ì²´í¬ (ë ˆì½”ë“œ ìˆ˜ í¬í•¨) |\n| GET | `/docs` | Swagger UI ë¬¸ì„œ |\n| POST | `/data` | ìƒˆ ë°ì´í„° ì¶”ê°€ |\n| GET | `/data/all` | ì „ì²´ ë°ì´í„° ì¡°íšŒ (í•„í„° ì§€ì›) |\n| GET | `/api/stats` | ìˆ˜ì§‘/ì²˜ë¦¬/ë™ê¸°í™” í†µê³„ |\n| GET | `/api/restaurants/raw` | Raw ë°ì´í„° ì¡°íšŒ |\n| POST | `/api/scrape/start` | ìˆ˜ë™ ìˆ˜ì§‘ ì‹œì‘ |\n\n**ì¸ì¦:**\n- X-API-Key í—¤ë” ë°©ì‹\n- ë©”ì¸ í”Œë«í¼ ë™ê¸°í™” ì‹œ ì‚¬ìš©\n\n### 6. ì£¼ê°„ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ\n\n**ê¸°ëŠ¥:**\n- ë§¤ì£¼ ì¼ìš”ì¼ ìë™ ì‹¤í–‰\n- ì „í™”ë²ˆí˜¸, ë©”ë‰´, ì˜ì—…ì‹œê°„ ëˆ„ë½ ë°ì´í„° íƒì§€\n- Apifyë¡œ ìë™ ë³´ì™„\n- ë°°ì¹˜ ì²˜ë¦¬ (10ê°œì”©) ë° ìë™ ì»¤ë°‹\n\n**ì£¼ìš” ì½”ë“œ:**\n```python\nasync def update_all_restaurants_weekly():\n    \"\"\"ë§¤ì£¼ ëª¨ë“  ë ˆìŠ¤í† ë‘ì˜ ëˆ„ë½ëœ ì •ë³´ë¥¼ Apifyë¡œ ì—…ë°ì´íŠ¸\"\"\"\n    restaurants_to_update = db.query(ProcessedRestaurant).filter(\n        (ProcessedRestaurant.phone == None) | \n        (ProcessedRestaurant.menu_summary == None) |\n        (ProcessedRestaurant.open_hours == None)\n    ).all()\n    # Apifyë¡œ ë°ì´í„° ë³´ì™„\n```\n\n---\n\n## ğŸ“Š ì‘ì—… ë‚´ì—­\n\n### Phase 1: ê¸°ë°˜ êµ¬ì¶• (2025-10)\n\n**ì™„ë£Œ í•­ëª©:**\n1. âœ… FastAPI ì„œë²„ êµ¬ì¶•\n2. âœ… PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„\n3. âœ… ë„¤ì´ë²„ Maps API í†µí•©\n4. âœ… Google Gemini AI í†µí•©\n5. âœ… 24/7 ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬í˜„\n\n**ì£¼ìš” ì„±ê³¼:**\n- REST API ì„œë²„ êµ¬ë™\n- ë°ì´í„° ëª¨ë¸ ì„¤ê³„ (Raw + Processed)\n- ê¸°ë³¸ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì™„ì„±\n\n### Phase 2: Google Places API í†µí•© (2025-11-04)\n\n**ì™„ë£Œ í•­ëª©:**\n1. âœ… Google Places API ì—°ë™\n2. âœ… í‰ì  ë° ë¦¬ë·° ìˆ˜ ìˆ˜ì§‘\n3. âœ… ì´ë¯¸ì§€ URL ìˆ˜ì§‘ (ìµœëŒ€ 10ê°œ)\n4. âœ… ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° ë¡œì§ êµ¬í˜„\n\n**ì£¼ìš” ì„±ê³¼:**\n- Google í‰ì  ìˆ˜ì§‘: 58/59 (98%)\n- í‰ê·  í‰ì : 4.38/5.0 â­\n- ì´ë¯¸ì§€ í‰ê· : 8-10ê°œ/ë ˆìŠ¤í† ë‘\n\n### Phase 3: Apify Naver Map Scraper í†µí•© (2025-11-06)\n\n**ì™„ë£Œ í•­ëª©:**\n1. âœ… Apify Actor ì—°ë™ (UCpUxFUNcdKdbBdYg)\n2. âœ… ë©”ë‰´ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ êµ¬í˜„\n3. âœ… ì „í™”ë²ˆí˜¸ ìˆ˜ì§‘ ë¡œì§ êµ¬í˜„\n4. âœ… ì˜ì—…ì‹œê°„ ìˆ˜ì§‘ ë¡œì§ êµ¬í˜„\n5. âœ… ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ êµ¬í˜„\n\n**ì£¼ìš” ì„±ê³¼:**\n- ë©”ë‰´ ìˆ˜ì§‘: 59/59 (100%)\n- ì „í™”ë²ˆí˜¸ ìˆ˜ì§‘: 58/59 (98%)\n- ì˜ì—…ì‹œê°„ ìˆ˜ì§‘: 59/59 (100%)\n- ë„¤ì´ë²„ í‰ì  ìˆ˜ì§‘: 59/59 (100%)\n\n**ë¹„ìš© ë¶„ì„:**\n- Apify ì›” êµ¬ë…: $30/ì›”\n- ê¸°ì¡´ ë°©ì‹ ëŒ€ë¹„: **87% ì ˆê°** ($360 vs $2,500-5,000/ë…„)\n\n### Phase 4: ìë™í™” ê°œì„  (2025-11-06)\n\n**ì™„ë£Œ í•­ëª©:**\n1. âœ… ì‹ ê·œ ìˆ˜ì§‘ ì‹œ Apify ì§ì ‘ í™œìš©\n2. âœ… ì£¼ê°„ ìë™ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ ì¶”ê°€\n3. âœ… ëˆ„ë½ ë°ì´í„° ìë™ ë³´ì™„ ì‹œìŠ¤í…œ\n4. âœ… ë°°ì¹˜ ì»¤ë°‹ ì•ˆì •ì„± ê°•í™”\n\n**ì£¼ìš” ì„±ê³¼:**\n- ì‹ ê·œ ìˆ˜ì§‘ ì‹œ ë°ì´í„° ì™„ì„±ë„ 100%\n- ì£¼ê°„ ìë™ ê²€ì¦ ì‹œìŠ¤í…œ ê°€ë™\n- ìˆ˜ë™ ê°œì… ìµœì†Œí™”\n\n### ì£¼ìš” ë§ˆì¼ìŠ¤í†¤\n\n| ë‚ ì§œ | ë‚´ìš© | ì„±ê³¼ |\n|------|------|------|\n| 2025-10 | í”„ë¡œì íŠ¸ ì‹œì‘ | ê¸°ë°˜ ì‹œìŠ¤í…œ êµ¬ì¶• |\n| 2025-11-04 | Google Places API í†µí•© | í‰ì /ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹œì‘ |\n| 2025-11-06 | Apify í†µí•© | ë©”ë‰´/ì „í™”ë²ˆí˜¸ 100% ìˆ˜ì§‘ |\n| 2025-11-06 | ìë™í™” ì™„ì„± | 24/7 ë¬´ì¸ ìš´ì˜ ì‹œì‘ |\n\n---\n\n## ğŸ“ˆ ë°ì´í„° í˜„í™©\n\n### ì „ì²´ í†µê³„ (2025-11-06 ê¸°ì¤€)\n\n| í•­ëª© | ìˆ˜ëŸ‰ | ì™„ì„±ë„ |\n|------|------|--------|\n| **ì´ ë ˆìŠ¤í† ë‘ ìˆ˜** | 59ê°œ | - |\n| **Raw ë°ì´í„°** | 59ê°œ | 100% |\n| **Processed ë°ì´í„°** | 59ê°œ | 100% |\n| **ë™ê¸°í™” ì™„ë£Œ** | 0ê°œ | 0% (ëŒ€ê¸° ì¤‘) |\n\n### ë°ì´í„° í’ˆì§ˆ\n\n| í•„ë“œ | ì™„ë£Œ | ì™„ì„±ë„ | ë¹„ê³  |\n|------|------|--------|------|\n| **ì£¼ì†Œ** | 59/59 | 100% | âœ… ì™„ë²½ |\n| **ì„¤ëª…** | 59/59 | 100% | âœ… AI ì •ì œ ì™„ë£Œ (í‰ê·  250ì) |\n| **ë©”ë‰´** | 59/59 | 100% | âœ… í‰ê·  8-10ê°œ |\n| **ì˜ì—…ì‹œê°„** | 59/59 | 100% | âœ… ì™„ë²½ |\n| **ë„¤ì´ë²„ í‰ì ** | 59/59 | 100% | âœ… í‰ê·  ë¦¬ë·° 3,500ê°œ |\n| **Google í‰ì ** | 58/59 | 98% | âœ… í‰ê·  4.38/5.0 |\n| **ì „í™”ë²ˆí˜¸** | 58/59 | 98% | âš ï¸ 1ê°œ ì˜¨ë¼ì¸ ë¯¸ê³µê°œ |\n| **ì´ë¯¸ì§€ URL** | 58/59 | 98% | âœ… í‰ê·  8-10ê°œ |\n| **ì¸ê¸°ë„ ì ìˆ˜** | 59/59 | 100% | âœ… 0-100 ì ìˆ˜í™” |\n\n### í’ˆì§ˆ ì§€í‘œ\n\n**ì„¤ëª… í’ˆì§ˆ:**\n- í‰ê·  ê¸¸ì´: 250ì\n- ìµœì†Œ ê¸¸ì´: 200ì\n- AI ì •ì œìœ¨: 100%\n- í™˜ê° ë°œìƒ: 0ê±´\n\n**í‰ì  ì •ë³´:**\n- Google í‰ê· : 4.38/5.0 â­\n- Naver í‰ê· : 4.5/5.0 â­\n- í‰ê·  ë¦¬ë·° ìˆ˜: 3,500ê°œ (Naver)\n\n**ë°ì´í„° ì™„ì „ì„±:**\n- í•„ìˆ˜ í•„ë“œ: 100%\n- ì„ íƒ í•„ë“œ: 98%\n- ì „ì²´ ì™„ì„±ë„: **99%**\n\n### ë ˆìŠ¤í† ë‘ ìƒ˜í”Œ\n\n| ì´ë¦„ | ì „í™”ë²ˆí˜¸ | ë©”ë‰´ ìˆ˜ | Google í‰ì  | Naver í‰ì  |\n|------|---------|--------|------------|-----------|\n| í•„ë™ë©´ì˜¥ | 02-2266-2611 | 8ê°œ | 4.4â˜… | 4.84â˜… |\n| ë¬´ì›”ì‹íƒ | 0507-1304-8592 | 10ê°œ | 4.6â˜… | 5.85â˜… |\n| ì„ë°€ëŒ€ í‰ì–‘ëƒ‰ë©´ | 02-717-1922 | 8ê°œ | 4.3â˜… | 4.89â˜… |\n| ë‹¤ëª½ì§‘ ì‹ ë…¼í˜„ë³¸ì  | 0507-1468-4245 | 10ê°œ | 4.5â˜… | 6.43â˜… |\n| ìš°ë˜ì˜¥ | 02-2265-0151 | 4ê°œ | 4.2â˜… | 4.87â˜… |\n\n---\n\n## ğŸ’° ìš´ì˜ ë¹„ìš©\n\n### ì›”ê°„ ë¹„ìš© (USD)\n\n| í•­ëª© | ë¹„ìš© | ë¹„ê³  |\n|------|------|------|\n| **Apify êµ¬ë…** | $30/ì›” | Naver Map Scraper |\n| Google Places API | $0 | ë¬´ë£Œ í• ë‹¹ëŸ‰ ë‚´ |\n| Google Gemini API | $0 | ë¬´ë£Œ í• ë‹¹ëŸ‰ ë‚´ |\n| Replit Hosting | $0 | í¬í•¨ |\n| **ì´ ë¹„ìš©** | **$30/ì›”** | **$360/ë…„** |\n\n### ë¹„ìš© íš¨ìœ¨ì„±\n\n**ê¸°ì¡´ ë°©ì‹ (ì§ì ‘ ê°œë°œ) ì˜ˆìƒ ë¹„ìš©:**\n- ì„œë²„ ë¹„ìš©: $100-200/ì›”\n- í”„ë¡ì‹œ/IP: $50-100/ì›”\n- ê°œë°œ/ìœ ì§€ë³´ìˆ˜: $100-200/ì›”\n- **ì´ ì˜ˆìƒ**: $250-500/ì›” ($3,000-6,000/ë…„)\n\n**Apify í™œìš© ì‹œ:**\n- **$30/ì›”** ($360/ë…„)\n- **87-94% ë¹„ìš© ì ˆê°**\n\n### ROI (Return on Investment)\n\n**íˆ¬ì:**\n- ê°œë°œ ì‹œê°„: ì•½ 40ì‹œê°„\n- ì›” ìš´ì˜ ë¹„ìš©: $30\n\n**íš¨ê³¼:**\n- ì›” 990ê°œ ë ˆìŠ¤í† ë‘ ìë™ ìˆ˜ì§‘\n- ì™„ì „í•œ ë°ì´í„° (ë©”ë‰´, ì „í™”ë²ˆí˜¸, ì˜ì—…ì‹œê°„)\n- 24/7 ë¬´ì¸ ìš´ì˜\n- ìˆ˜ë™ ì‘ì—… ì‹œê°„: 0ì‹œê°„/ì›”\n\n**ì‹œê°„ ì ˆì•½:**\n- ìˆ˜ë™ ìˆ˜ì§‘ ì‹œ: ì•½ 200ì‹œê°„/ì›”\n- ìë™í™” í›„: 0ì‹œê°„/ì›”\n- **ì ˆê° ì‹œê°„**: 200ì‹œê°„/ì›”\n\n---\n\n## ğŸ”® í–¥í›„ ê³„íš\n\n### ë‹¨ê¸° ê³„íš (1ê°œì›”)\n\n1. **ë©”ì¸ í”Œë«í¼ ë™ê¸°í™” í™œì„±í™”**\n   - í•œì‹ë‹¹ í”Œë«í¼ API ì—°ë™ ì™„ë£Œ\n   - ì¼ì¼ ë™ê¸°í™” ì‹œì‘\n   - ì‹ ê·œ ë ˆìŠ¤í† ë‘ ìë™ ë“±ë¡\n\n2. **ë°ì´í„° í™•ì¥**\n   - ì¼ 33ê°œ â†’ 50ê°œë¡œ ì¦ëŒ€\n   - ì„œìš¸ ì „ì—­ ì»¤ë²„ë¦¬ì§€ í™•ëŒ€\n   - ì¸ê¸° ì§€ì—­ ì§‘ì¤‘ ìˆ˜ì§‘\n\n3. **í’ˆì§ˆ ê°œì„ **\n   - ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì¦ ê°•í™”\n   - ë©”ë‰´ ê°€ê²© ì •í™•ë„ í–¥ìƒ\n   - ì˜ì—…ì‹œê°„ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸\n\n### ì¤‘ê¸° ê³„íš (3ê°œì›”)\n\n1. **ì§€ì—­ í™•ì¥**\n   - ì„œìš¸ ì™¸ ìˆ˜ë„ê¶Œ í™•ëŒ€\n   - ë¶€ì‚°, ëŒ€êµ¬ ë“± ì£¼ìš” ë„ì‹œ\n   - ì§€ì—­ë³„ ë§ì¶¤ ê²€ìƒ‰ ì¿¼ë¦¬\n\n2. **ë°ì´í„° ê³ ë„í™”**\n   - ë¦¬ë·° ê°ì„± ë¶„ì„\n   - íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ\n   - ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ë°ì´í„° ì œê³µ\n\n3. **ì„±ëŠ¥ ìµœì í™”**\n   - ìˆ˜ì§‘ ì†ë„ 2ë°° í–¥ìƒ\n   - DB ì¸ë±ì‹± ìµœì í™”\n   - API ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•\n\n### ì¥ê¸° ê³„íš (6ê°œì›”)\n\n1. **AI ê³ ë„í™”**\n   - ë‹¤êµ­ì–´ ì§€ì› (ì˜ì–´, ì¼ë³¸ì–´)\n   - ì´ë¯¸ì§€ AI ë¶„ì„\n   - ë©”ë‰´ ìë™ ì¹´í…Œê³ ë¦¬í™”\n\n2. **í”Œë«í¼ í™•ì¥**\n   - ì¹´ì¹´ì˜¤ë§µ ì—°ë™\n   - ì¸ìŠ¤íƒ€ê·¸ë¨ ë°ì´í„° ìˆ˜ì§‘\n   - ë°°ë‹¬ì•± ì •ë³´ í†µí•©\n\n3. **ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤**\n   - ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ\n   - íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸\n   - ê²½ìŸ ë¶„ì„ ê¸°ëŠ¥\n\n---\n\n## ğŸ“ ê²°ë¡ \n\n### í”„ë¡œì íŠ¸ ì„±ê³¼\n\n**í•œì‹ë‹¹ ë°ì´í„° í—ˆë¸Œ**ëŠ” ì›” 990ê°œì˜ ë ˆìŠ¤í† ë‘ ë°ì´í„°ë¥¼ ì™„ì „ ìë™ìœ¼ë¡œ ìˆ˜ì§‘Â·ì •ì œÂ·ë™ê¸°í™”í•˜ëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤:\n\n1. âœ… **ì™„ì „ ìë™í™”**: 24/7 ë¬´ì¸ ìš´ì˜ (ìˆ˜ë™ ê°œì… 0%)\n2. âœ… **ë†’ì€ í’ˆì§ˆ**: AI ì •ì œ + ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ì¦\n3. âœ… **ì™„ì „í•œ ë°ì´í„°**: ë©”ë‰´, ì „í™”ë²ˆí˜¸, ì˜ì—…ì‹œê°„ 100% ìˆ˜ì§‘\n4. âœ… **ë¹„ìš© íš¨ìœ¨**: ì›” $30ë¡œ ìš´ì˜ (87% ë¹„ìš© ì ˆê°)\n5. âœ… **í™•ì¥ì„±**: í•„ìš”ì‹œ ì¦‰ì‹œ ìˆ˜ì§‘ëŸ‰ ì¦ëŒ€ ê°€ëŠ¥\n\n### í•µì‹¬ ê¸°ìˆ \n\n- **Apify Naver Map Scraper**: ì™„ì „í•œ ë°ì´í„° ìˆ˜ì§‘\n- **Google Gemini AI**: ê³ í’ˆì§ˆ ì„¤ëª… ìƒì„±\n- **Google Places API**: í‰ì  ë° ì´ë¯¸ì§€ ë³´ê°•\n- **24/7 ìŠ¤ì¼€ì¤„ëŸ¬**: ì™„ì „ ìë™í™” ìš´ì˜\n- **ì£¼ê°„ ê²€ì¦ ì‹œìŠ¤í…œ**: ë°ì´í„° ì™„ì „ì„± ë³´ì¥\n\n### ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜\n\n1. **ì‹œê°„ ì ˆì•½**: ì›” 200ì‹œê°„ â†’ 0ì‹œê°„\n2. **ë¹„ìš© ì ˆê°**: 87-94% ì ˆê° ($360 vs $3,000-6,000/ë…„)\n3. **í’ˆì§ˆ í–¥ìƒ**: AI ì •ì œ + ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ì¦\n4. **í™•ì¥ ê°€ëŠ¥**: ì›” 990ê°œ â†’ í•„ìš”ì‹œ ë¬´ì œí•œ í™•ì¥\n\n### ë‹¤ìŒ ë‹¨ê³„\n\n1. ë©”ì¸ í”Œë«í¼ ë™ê¸°í™” ì‹œì‘\n2. ìˆ˜ì§‘ëŸ‰ ì ì§„ì  ì¦ëŒ€ (33 â†’ 50 â†’ 100ê°œ/ì¼)\n3. ì„œìš¸ ì™¸ ì§€ì—­ í™•ëŒ€\n4. AI ê¸°ëŠ¥ ê³ ë„í™”\n\n---\n\n**í”„ë¡œì íŠ¸ ë‹´ë‹¹ì:** Replit AI Agent  \n**ê¸°ìˆ  ì§€ì›:** Google Gemini 2.0 Flash, Apify Platform  \n**ë¬¸ì˜:** hansikdang-data-hub@example.com\n\n---\n\n*ë³¸ ë³´ê³ ì„œëŠ” 2025ë…„ 11ì›” 6ì¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*\n","size_bytes":18440},"data-hub/src/monitoring/system_monitor.py":{"content":"\"\"\"\nSystem Monitor - ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§\n\"\"\"\nimport uuid\nimport psutil\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime, timezone, timedelta\nfrom loguru import logger\nfrom sqlalchemy.orm import Session\n\nfrom ..database.models import SystemHealth, RawRestaurantData, ProcessedRestaurant\n\n\nclass SystemMonitor:\n    \"\"\"\n    ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ëª¨ë‹ˆí„°ë§\n    CPU, ë©”ëª¨ë¦¬, ì„±ëŠ¥ ì§€í‘œë¥¼ ì¶”ì í•©ë‹ˆë‹¤.\n    \"\"\"\n    \n    def __init__(self, db: Session):\n        self.db = db\n    \n    def monitor_component(\n        self,\n        component: str,\n        total_operations: int = 0,\n        successful_operations: int = 0,\n        failed_operations: int = 0,\n        response_time_ms: Optional[int] = None\n    ) -> SystemHealth:\n        \"\"\"\n        ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œë¥¼ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.\n        \n        Args:\n            component: êµ¬ì„± ìš”ì†Œ ì´ë¦„ (scraper, processor, sync, api, database)\n            total_operations: ì´ ì‘ì—… ìˆ˜\n            successful_operations: ì„±ê³µí•œ ì‘ì—… ìˆ˜\n            failed_operations: ì‹¤íŒ¨í•œ ì‘ì—… ìˆ˜\n            response_time_ms: ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)\n            \n        Returns:\n            SystemHealth: ê±´ê°• ìƒíƒœ ë ˆì½”ë“œ\n        \"\"\"\n        cpu_usage = psutil.cpu_percent(interval=1)\n        memory = psutil.virtual_memory()\n        disk = psutil.disk_usage('/')\n        \n        error_rate = 0.0\n        success_rate = 100.0\n        \n        if total_operations > 0:\n            error_rate = (failed_operations / total_operations) * 100\n            success_rate = (successful_operations / total_operations) * 100\n        \n        status = self._determine_status(\n            error_rate=error_rate,\n            response_time_ms=response_time_ms,\n            cpu_usage=cpu_usage\n        )\n        \n        alerts = self._check_alerts(\n            component=component,\n            error_rate=error_rate,\n            response_time_ms=response_time_ms,\n            cpu_usage=cpu_usage,\n            memory_usage=memory.percent\n        )\n        \n        health = SystemHealth(\n            id=str(uuid.uuid4()),\n            component=component,\n            component_status=status,\n            response_time_ms=response_time_ms,\n            throughput=total_operations,\n            error_rate=error_rate,\n            success_rate=success_rate,\n            cpu_usage=cpu_usage,\n            memory_usage=memory.percent,\n            disk_usage=disk.percent,\n            total_operations=total_operations,\n            successful_operations=successful_operations,\n            failed_operations=failed_operations,\n            alerts=alerts,\n            last_alert_at=datetime.now(timezone.utc) if alerts else None,\n            details={\n                'memory_available_mb': round(memory.available / 1024 / 1024, 2),\n                'disk_free_gb': round(disk.free / 1024 / 1024 / 1024, 2)\n            }\n        )\n        \n        self.db.add(health)\n        self.db.commit()\n        \n        if status != 'healthy':\n            logger.warning(f\"âš ï¸  {component} ìƒíƒœ: {status} (ì˜¤ë¥˜ìœ¨: {error_rate:.1f}%)\")\n        else:\n            logger.debug(f\"âœ… {component} ì •ìƒ (ì„±ê³µë¥ : {success_rate:.1f}%)\")\n        \n        return health\n    \n    def _determine_status(\n        self,\n        error_rate: float,\n        response_time_ms: Optional[int],\n        cpu_usage: float\n    ) -> str:\n        \"\"\"ì‹œìŠ¤í…œ ìƒíƒœ íŒë‹¨\"\"\"\n        if error_rate > 50:\n            return 'down'\n        elif error_rate > 20 or (response_time_ms and response_time_ms > 5000) or cpu_usage > 90:\n            return 'degraded'\n        else:\n            return 'healthy'\n    \n    def _check_alerts(\n        self,\n        component: str,\n        error_rate: float,\n        response_time_ms: Optional[int],\n        cpu_usage: float,\n        memory_usage: float\n    ) -> List[Dict[str, str]]:\n        \"\"\"ì•Œë¦¼ í™•ì¸\"\"\"\n        alerts = []\n        \n        if error_rate > 30:\n            alerts.append({\n                'severity': 'high',\n                'type': 'error_rate',\n                'message': f'{component} ì˜¤ë¥˜ìœ¨ ë†’ìŒ: {error_rate:.1f}%'\n            })\n        \n        if response_time_ms and response_time_ms > 5000:\n            alerts.append({\n                'severity': 'medium',\n                'type': 'response_time',\n                'message': f'{component} ì‘ë‹µ ì‹œê°„ ëŠë¦¼: {response_time_ms}ms'\n            })\n        \n        if cpu_usage > 85:\n            alerts.append({\n                'severity': 'medium',\n                'type': 'cpu',\n                'message': f'CPU ì‚¬ìš©ë¥  ë†’ìŒ: {cpu_usage:.1f}%'\n            })\n        \n        if memory_usage > 85:\n            alerts.append({\n                'severity': 'medium',\n                'type': 'memory',\n                'message': f'ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {memory_usage:.1f}%'\n            })\n        \n        return alerts\n    \n    def get_system_overview(self) -> Dict[str, Any]:\n        \"\"\"ì‹œìŠ¤í…œ ì „ì²´ ê°œìš”ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n        components = ['scraper', 'processor', 'sync', 'api', 'database']\n        overview = {}\n        \n        for component in components:\n            latest = self.db.query(SystemHealth)\\\n                .filter(SystemHealth.component == component)\\\n                .order_by(SystemHealth.measured_at.desc())\\\n                .first()\n            \n            if latest:\n                overview[component] = {\n                    'status': latest.component_status,\n                    'success_rate': latest.success_rate,\n                    'error_rate': latest.error_rate,\n                    'response_time_ms': latest.response_time_ms,\n                    'last_checked': latest.measured_at.isoformat() if latest.measured_at else None\n                }\n            else:\n                overview[component] = {\n                    'status': 'unknown',\n                    'success_rate': 0,\n                    'error_rate': 0\n                }\n        \n        cpu_usage = psutil.cpu_percent(interval=0.5)\n        memory = psutil.virtual_memory()\n        disk = psutil.disk_usage('/')\n        \n        overview['system_resources'] = {\n            'cpu_usage': cpu_usage,\n            'memory_usage': memory.percent,\n            'disk_usage': disk.percent\n        }\n        \n        return overview\n    \n    def get_performance_trends(\n        self,\n        component: str,\n        hours: int = 24\n    ) -> Dict[str, Any]:\n        \"\"\"ì„±ëŠ¥ íŠ¸ë Œë“œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)\n        \n        records = self.db.query(SystemHealth)\\\n            .filter(\n                SystemHealth.component == component,\n                SystemHealth.measured_at >= cutoff_time\n            )\\\n            .order_by(SystemHealth.measured_at.asc())\\\n            .all()\n        \n        if not records:\n            return {\n                'component': component,\n                'data_points': 0,\n                'avg_success_rate': 0,\n                'avg_response_time': 0\n            }\n        \n        success_rates = [r.success_rate for r in records if r.success_rate is not None]\n        response_times = [r.response_time_ms for r in records if r.response_time_ms]\n        \n        return {\n            'component': component,\n            'data_points': len(records),\n            'avg_success_rate': round(sum(success_rates) / len(success_rates), 2) if success_rates else 0,\n            'avg_response_time': round(sum(response_times) / len(response_times), 2) if response_times else 0,\n            'min_response_time': min(response_times) if response_times else 0,\n            'max_response_time': max(response_times) if response_times else 0,\n            'current_status': records[-1].component_status if records else 'unknown'\n        }\n","size_bytes":7845},"data-hub/src/targeting/popularity_scorer.py":{"content":"\"\"\"\nì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚°ê¸°\nGoogle Trends ê¸°ë°˜ ì§€ì—­ë³„ ì™¸êµ­ì¸ ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° ë° ê´€ë¦¬\n\"\"\"\n\nfrom typing import Dict, List, Optional\nfrom datetime import datetime, timedelta\nfrom loguru import logger\nfrom sqlalchemy import and_\nimport json\n\n\nclass PopularityScorer:\n    \"\"\"ì§€ì—­ë³„ ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° ë° íˆìŠ¤í† ë¦¬ ê´€ë¦¬\"\"\"\n    \n    def __init__(self):\n        \"\"\"ì´ˆê¸°í™”\"\"\"\n        self.logger = logger\n        self.history = {}\n    \n    def calculate_weighted_score(\n        self,\n        current_score: float,\n        historical_avg: Optional[float] = None,\n        weight_current: float = 0.8\n    ) -> float:\n        \"\"\"\n        ê°€ì¤‘ í‰ê·  ì ìˆ˜ ê³„ì‚°\n        \n        Args:\n            current_score: í˜„ì¬ ì ìˆ˜\n            historical_avg: íˆìŠ¤í† ë¦¬ í‰ê·  ì ìˆ˜\n            weight_current: í˜„ì¬ ì ìˆ˜ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.8)\n            \n        Returns:\n            ê°€ì¤‘ í‰ê·  ì ìˆ˜ (0-100)\n        \"\"\"\n        if historical_avg is None:\n            return current_score\n        \n        weighted = (\n            current_score * weight_current +\n            historical_avg * (1 - weight_current)\n        )\n        \n        return round(weighted, 2)\n    \n    def update_history(\n        self,\n        region: str,\n        score: float,\n        date: Optional[datetime] = None\n    ):\n        \"\"\"\n        íˆìŠ¤í† ë¦¬ì— ì ìˆ˜ ì¶”ê°€\n        \n        Args:\n            region: ì§€ì—­ëª…\n            score: ì ìˆ˜\n            date: ë‚ ì§œ (ê¸°ë³¸: í˜„ì¬)\n        \"\"\"\n        if date is None:\n            date = datetime.now()\n        \n        if region not in self.history:\n            self.history[region] = []\n        \n        self.history[region].append({\n            \"date\": date.isoformat(),\n            \"score\": score\n        })\n        \n        # ìµœê·¼ 30ì¼ë§Œ ìœ ì§€\n        cutoff = datetime.now() - timedelta(days=30)\n        self.history[region] = [\n            entry for entry in self.history[region]\n            if datetime.fromisoformat(entry[\"date\"]) > cutoff\n        ]\n    \n    def get_historical_avg(\n        self,\n        region: str,\n        days: int = 7\n    ) -> Optional[float]:\n        \"\"\"\n        íŠ¹ì • ì§€ì—­ì˜ íˆìŠ¤í† ë¦¬ í‰ê·  ë°˜í™˜\n        \n        Args:\n            region: ì§€ì—­ëª…\n            days: ê¸°ê°„ (ì¼)\n            \n        Returns:\n            í‰ê·  ì ìˆ˜ ë˜ëŠ” None\n        \"\"\"\n        if region not in self.history or not self.history[region]:\n            return None\n        \n        cutoff = datetime.now() - timedelta(days=days)\n        recent = [\n            entry[\"score\"] for entry in self.history[region]\n            if datetime.fromisoformat(entry[\"date\"]) > cutoff\n        ]\n        \n        if not recent:\n            return None\n        \n        return round(sum(recent) / len(recent), 2)\n    \n    def get_trend_direction(\n        self,\n        region: str,\n        days: int = 7\n    ) -> str:\n        \"\"\"\n        íŠ¸ë Œë“œ ë°©í–¥ ë°˜í™˜\n        \n        Args:\n            region: ì§€ì—­ëª…\n            days: ê¸°ê°„ (ì¼)\n            \n        Returns:\n            \"up\", \"down\", \"stable\"\n        \"\"\"\n        if region not in self.history or len(self.history[region]) < 2:\n            return \"stable\"\n        \n        cutoff = datetime.now() - timedelta(days=days)\n        recent = [\n            entry for entry in self.history[region]\n            if datetime.fromisoformat(entry[\"date\"]) > cutoff\n        ]\n        \n        if len(recent) < 2:\n            return \"stable\"\n        \n        # ì²« ì ˆë°˜ vs í›„ ì ˆë°˜ ë¹„êµ\n        mid = len(recent) // 2\n        first_half_avg = sum(e[\"score\"] for e in recent[:mid]) / mid\n        second_half_avg = sum(e[\"score\"] for e in recent[mid:]) / (len(recent) - mid)\n        \n        diff = second_half_avg - first_half_avg\n        \n        if diff > 5:\n            return \"up\"\n        elif diff < -5:\n            return \"down\"\n        else:\n            return \"stable\"\n    \n    def get_all_stats(self) -> Dict[str, any]:\n        \"\"\"\n        ì „ì²´ í†µê³„ ë°˜í™˜\n        \n        Returns:\n            í†µê³„ ë”•ì…”ë„ˆë¦¬\n        \"\"\"\n        stats = {\n            \"total_regions\": len(self.history),\n            \"regions\": {}\n        }\n        \n        for region, entries in self.history.items():\n            if entries:\n                scores = [e[\"score\"] for e in entries]\n                stats[\"regions\"][region] = {\n                    \"current_score\": scores[-1] if scores else 0,\n                    \"avg_score\": round(sum(scores) / len(scores), 2),\n                    \"min_score\": min(scores),\n                    \"max_score\": max(scores),\n                    \"trend\": self.get_trend_direction(region),\n                    \"data_points\": len(scores)\n                }\n        \n        return stats\n    \n    def save_to_json(self, filepath: str):\n        \"\"\"\n        íˆìŠ¤í† ë¦¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n        \n        Args:\n            filepath: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ\n        \"\"\"\n        try:\n            with open(filepath, 'w', encoding='utf-8') as f:\n                json.dump(self.history, f, ensure_ascii=False, indent=2)\n            self.logger.info(f\"âœ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {filepath}\")\n        except Exception as e:\n            self.logger.error(f\"âŒ íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}\")\n    \n    def load_from_json(self, filepath: str) -> bool:\n        \"\"\"\n        JSON íŒŒì¼ì—ì„œ íˆìŠ¤í† ë¦¬ ë¡œë“œ\n        \n        Args:\n            filepath: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n            \n        Returns:\n            ì„±ê³µ ì—¬ë¶€\n        \"\"\"\n        try:\n            with open(filepath, 'r', encoding='utf-8') as f:\n                self.history = json.load(f)\n            self.logger.info(f\"âœ… íˆìŠ¤í† ë¦¬ ë¡œë“œ ì™„ë£Œ: {filepath}\")\n            return True\n        except FileNotFoundError:\n            self.logger.warning(f\"âš ï¸  íˆìŠ¤í† ë¦¬ íŒŒì¼ ì—†ìŒ: {filepath}\")\n            return False\n        except Exception as e:\n            self.logger.error(f\"âŒ íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n            return False\n","size_bytes":6018},"data-hub/src/api/dashboard_routes.py":{"content":"\"\"\"\nDashboard API Routes - ìš´ì˜ ëŒ€ì‹œë³´ë“œìš© í†µí•© API\n\"\"\"\nfrom fastapi import APIRouter, Depends\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import func, and_, text\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Dict, Any, List\n\nfrom ..database.connection import get_db\nfrom ..database.models import (\n    ProcessedRestaurant, RawRestaurantData, BackupHistory,\n    SystemHealth, QualityMetrics, MergeHistory\n)\n\nrouter = APIRouter(prefix=\"/api/dashboard\", tags=[\"dashboard\"])\n\n\n@router.get(\"/stats\")\ndef get_dashboard_stats(db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    ëŒ€ì‹œë³´ë“œ ì „ì²´ í†µê³„ ì¡°íšŒ (ë‹¨ì¼ APIë¡œ ëª¨ë“  ë°ì´í„° ì œê³µ)\n    \n    Returns:\n        - system_health: ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ (DB, API, Scheduler, Drive)\n        - yesterday_stats: ì–´ì œ ìˆ˜ì§‘ í†µê³„\n        - backup_status: ë°±ì—… ìƒíƒœ\n        - recent_alerts: ìµœê·¼ ì•Œë¦¼ 3ê±´\n        - weekly_trend: 7ì¼ ì¶”ì´ ë°ì´í„°\n    \"\"\"\n    now = datetime.now(timezone.utc)\n    yesterday_start = now - timedelta(days=1)\n    yesterday_start = yesterday_start.replace(hour=0, minute=0, second=0, microsecond=0)\n    yesterday_end = yesterday_start + timedelta(days=1)\n    \n    # 1. ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬\n    system_health = get_system_health(db)\n    \n    # 2. ì–´ì œ ìˆ˜ì§‘ í†µê³„\n    yesterday_stats = get_yesterday_stats(db, yesterday_start, yesterday_end)\n    \n    # 3. ë°±ì—… ìƒíƒœ\n    backup_status = get_latest_backup_status(db)\n    \n    # 4. ìµœê·¼ ì•Œë¦¼ 3ê±´\n    recent_alerts = get_recent_alerts(db, limit=3)\n    \n    # 5. 7ì¼ ì¶”ì´ ë°ì´í„°\n    weekly_trend = get_weekly_trend(db, days=7)\n    \n    return {\n        \"status\": \"success\",\n        \"timestamp\": now.isoformat(),\n        \"system_health\": system_health,\n        \"yesterday_stats\": yesterday_stats,\n        \"backup_status\": backup_status,\n        \"recent_alerts\": recent_alerts,\n        \"weekly_trend\": weekly_trend\n    }\n\n\ndef get_system_health(db: Session) -> Dict[str, Any]:\n    \"\"\"ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬\"\"\"\n    # ìµœê·¼ 1ì‹œê°„ ì´ë‚´ í—¬ìŠ¤ ë©”íŠ¸ë¦­\n    one_hour_ago = datetime.now(timezone.utc) - timedelta(hours=1)\n    \n    latest_health = db.query(SystemHealth).filter(\n        SystemHealth.measured_at >= one_hour_ago\n    ).order_by(SystemHealth.measured_at.desc()).first()\n    \n    # DB ìƒíƒœ\n    try:\n        db.execute(text(\"SELECT 1\"))\n        db_status = \"healthy\"\n        db_message = \"Database responding\"\n    except Exception as e:\n        db_status = \"unhealthy\"\n        db_message = str(e)\n    \n    # API ìƒíƒœ (í˜„ì¬ ì‘ë‹µ ì¤‘ì´ë¯€ë¡œ healthy)\n    api_status = \"healthy\"\n    api_message = \"API responding\"\n    \n    # Scheduler ìƒíƒœ (ìµœê·¼ 1ì‹œê°„ ë‚´ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€)\n    recent_data = db.query(RawRestaurantData).filter(\n        RawRestaurantData.scraped_at >= one_hour_ago\n    ).count()\n    \n    scheduler_status = \"healthy\" if recent_data > 0 or latest_health else \"idle\"\n    scheduler_message = f\"Last activity: {recent_data} records in 1h\" if recent_data > 0 else \"No recent activity\"\n    \n    # Drive ë°±ì—… ìƒíƒœ (ìµœê·¼ 24ì‹œê°„ ë‚´ ë°±ì—… ì¡´ì¬)\n    yesterday = datetime.now(timezone.utc) - timedelta(days=1)\n    recent_backup = db.query(BackupHistory).filter(\n        BackupHistory.completed_at >= yesterday\n    ).order_by(BackupHistory.completed_at.desc()).first()\n    \n    drive_status = \"healthy\" if recent_backup and recent_backup.status == \"success\" else \"warning\"\n    drive_message = f\"Last backup: {recent_backup.completed_at.strftime('%Y-%m-%d %H:%M')}\" if recent_backup else \"No recent backup\"\n    \n    return {\n        \"database\": {\n            \"status\": db_status,\n            \"message\": db_message\n        },\n        \"api\": {\n            \"status\": api_status,\n            \"message\": api_message\n        },\n        \"scheduler\": {\n            \"status\": scheduler_status,\n            \"message\": scheduler_message\n        },\n        \"drive_backup\": {\n            \"status\": drive_status,\n            \"message\": drive_message\n        },\n        \"overall_status\": \"healthy\" if all([\n            db_status == \"healthy\",\n            api_status == \"healthy\",\n            scheduler_status in [\"healthy\", \"idle\"]\n        ]) else \"warning\"\n    }\n\n\ndef get_yesterday_stats(db: Session, start: datetime, end: datetime) -> Dict[str, Any]:\n    \"\"\"ì–´ì œ ìˆ˜ì§‘ í†µê³„\"\"\"\n    # ì‹ ê·œ ìˆ˜ì§‘ (raw ë°ì´í„°)\n    new_collected = db.query(RawRestaurantData).filter(\n        and_(\n            RawRestaurantData.scraped_at >= start,\n            RawRestaurantData.scraped_at < end\n        )\n    ).count()\n    \n    # ì¤‘ë³µ ì œê±°ëœ ìˆ˜\n    duplicates_removed = db.query(MergeHistory).filter(\n        and_(\n            MergeHistory.merged_at >= start,\n            MergeHistory.merged_at < end\n        )\n    ).count()\n    \n    # ìµœì¢… ì²˜ë¦¬ëœ ìˆ˜\n    final_processed = db.query(ProcessedRestaurant).filter(\n        and_(\n            ProcessedRestaurant.created_at >= start,\n            ProcessedRestaurant.created_at < end\n        )\n    ).count()\n    \n    # ë°ì´í„° ì™„ì „ì„± (ì „í™”ë²ˆí˜¸, ë©”ë‰´, ì˜ì—…ì‹œê°„ ëª¨ë‘ ìˆëŠ” ë¹„ìœ¨)\n    complete_count = db.query(ProcessedRestaurant).filter(\n        and_(\n            ProcessedRestaurant.created_at >= start,\n            ProcessedRestaurant.created_at < end,\n            ProcessedRestaurant.phone.isnot(None),\n            ProcessedRestaurant.menu_summary.isnot(None),\n            ProcessedRestaurant.open_hours.isnot(None)\n        )\n    ).count()\n    \n    completeness_rate = (complete_count / final_processed * 100) if final_processed > 0 else 0\n    \n    # í‰ê·  í’ˆì§ˆ ì ìˆ˜\n    avg_quality = db.query(func.avg(QualityMetrics.overall_quality_score)).filter(\n        and_(\n            QualityMetrics.measured_at >= start,\n            QualityMetrics.measured_at < end\n        )\n    ).scalar() or 0\n    \n    return {\n        \"new_collected\": new_collected,\n        \"duplicates_removed\": duplicates_removed,\n        \"final_processed\": final_processed,\n        \"completeness_rate\": round(completeness_rate, 1),\n        \"average_quality_score\": round(avg_quality, 1),\n        \"date\": start.strftime('%Y-%m-%d')\n    }\n\n\ndef get_latest_backup_status(db: Session) -> Dict[str, Any]:\n    \"\"\"ìµœê·¼ ë°±ì—… ìƒíƒœ\"\"\"\n    latest = db.query(BackupHistory).order_by(\n        BackupHistory.completed_at.desc()\n    ).first()\n    \n    if not latest:\n        return {\n            \"status\": \"no_backup\",\n            \"message\": \"ë°±ì—… ì´ë ¥ ì—†ìŒ\"\n        }\n    \n    return {\n        \"status\": latest.status,\n        \"backup_date\": latest.backup_date,\n        \"file_name\": latest.file_name,\n        \"file_size_mb\": round(latest.file_size_bytes / 1024 / 1024, 2) if latest.file_size_bytes else 0,\n        \"total_records\": latest.total_records,\n        \"completed_at\": latest.completed_at.strftime('%Y-%m-%d %H:%M:%S') if latest.completed_at else None,\n        \"execution_time_seconds\": latest.execution_time_seconds\n    }\n\n\ndef get_recent_alerts(db: Session, limit: int = 3) -> List[Dict[str, Any]]:\n    \"\"\"ìµœê·¼ ì•Œë¦¼ ì¡°íšŒ (ì—ëŸ¬ 3ê±´) - ì‹œìŠ¤í…œ í—¬ìŠ¤ ê¸°ë°˜\"\"\"\n    # SystemHealth í…Œì´ë¸”ì—ì„œ ìµœê·¼ ì´ìŠˆ ì¡°íšŒ\n    one_week_ago = datetime.now(timezone.utc) - timedelta(days=7)\n    \n    health_issues = db.query(SystemHealth).filter(\n        and_(\n            SystemHealth.measured_at >= one_week_ago,\n            SystemHealth.component_status.in_(['degraded', 'down', 'warning'])\n        )\n    ).order_by(SystemHealth.measured_at.desc()).limit(limit).all()\n    \n    alerts = []\n    for health in health_issues:\n        severity = 'high' if health.component_status == 'down' else 'medium'\n        message = f\"{health.component} status: {health.component_status}\"\n        if health.error_rate and health.error_rate > 10:\n            message += f\" (error rate: {health.error_rate:.1f}%)\"\n        \n        alerts.append({\n            \"id\": health.id,\n            \"severity\": severity,\n            \"component\": health.component,\n            \"message\": message,\n            \"triggered_at\": health.measured_at.strftime('%Y-%m-%d %H:%M:%S') if health.measured_at else None,\n            \"status\": health.component_status\n        })\n    \n    return alerts\n\n\ndef get_weekly_trend(db: Session, days: int = 7) -> Dict[str, List]:\n    \"\"\"7ì¼ ì¶”ì´ ë°ì´í„°\"\"\"\n    now = datetime.now(timezone.utc)\n    \n    dates = []\n    new_collected = []\n    final_processed = []\n    quality_scores = []\n    \n    for i in range(days - 1, -1, -1):\n        day_start = (now - timedelta(days=i)).replace(hour=0, minute=0, second=0, microsecond=0)\n        day_end = day_start + timedelta(days=1)\n        \n        # ë‚ ì§œ\n        dates.append(day_start.strftime('%m/%d'))\n        \n        # ì‹ ê·œ ìˆ˜ì§‘\n        new_count = db.query(RawRestaurantData).filter(\n            and_(\n                RawRestaurantData.scraped_at >= day_start,\n                RawRestaurantData.scraped_at < day_end\n            )\n        ).count()\n        new_collected.append(new_count)\n        \n        # ìµœì¢… ì²˜ë¦¬\n        processed_count = db.query(ProcessedRestaurant).filter(\n            and_(\n                ProcessedRestaurant.created_at >= day_start,\n                ProcessedRestaurant.created_at < day_end\n            )\n        ).count()\n        final_processed.append(processed_count)\n        \n        # í‰ê·  í’ˆì§ˆ\n        avg_quality = db.query(func.avg(QualityMetrics.overall_quality_score)).filter(\n            and_(\n                QualityMetrics.measured_at >= day_start,\n                QualityMetrics.measured_at < day_end\n            )\n        ).scalar() or 0\n        quality_scores.append(round(avg_quality, 1))\n    \n    return {\n        \"dates\": dates,\n        \"new_collected\": new_collected,\n        \"final_processed\": final_processed,\n        \"quality_scores\": quality_scores\n    }\n","size_bytes":9767},"data-hub/src/monitoring/__init__.py":{"content":"","size_bytes":0},"data-hub/src/monitoring/alert_manager.py":{"content":"\"\"\"\nAlert Manager - ì•Œë¦¼ ê´€ë¦¬ ì‹œìŠ¤í…œ\n\"\"\"\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime, timezone, timedelta\nfrom loguru import logger\nfrom sqlalchemy.orm import Session\n\nfrom ..database.models import SystemHealth, QualityMetrics\n\n\nclass AlertManager:\n    \"\"\"\n    ì•Œë¦¼ ê´€ë¦¬ ì‹œìŠ¤í…œ\n    í’ˆì§ˆ ì €í•˜, ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë“±ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ì•Œë¦¼ì„ ìƒì„±í•©ë‹ˆë‹¤.\n    \"\"\"\n    \n    def __init__(self, db: Session):\n        self.db = db\n        \n        self.alert_thresholds = {\n            'quality_score': 70.0,\n            'error_rate': 30.0,\n            'response_time_ms': 5000,\n            'cpu_usage': 85.0,\n            'memory_usage': 85.0,\n            'disk_usage': 90.0\n        }\n    \n    def check_quality_alerts(self) -> List[Dict[str, Any]]:\n        \"\"\"í’ˆì§ˆ ê´€ë ¨ ì•Œë¦¼ì„ í™•ì¸í•©ë‹ˆë‹¤.\"\"\"\n        alerts = []\n        \n        recent_metrics = self.db.query(QualityMetrics)\\\n            .filter(\n                QualityMetrics.measured_at >= datetime.now(timezone.utc) - timedelta(hours=1)\n            )\\\n            .all()\n        \n        if not recent_metrics:\n            return alerts\n        \n        low_quality_count = sum(\n            1 for m in recent_metrics \n            if m.overall_quality_score < self.alert_thresholds['quality_score']\n        )\n        \n        if low_quality_count > len(recent_metrics) * 0.3:\n            alerts.append({\n                'type': 'quality_degradation',\n                'severity': 'high',\n                'message': f'í’ˆì§ˆ ì €í•˜ ê°ì§€: {low_quality_count}/{len(recent_metrics)} ë°ì´í„°ê°€ ê¸°ì¤€ ë¯¸ë‹¬',\n                'threshold': self.alert_thresholds['quality_score'],\n                'details': {\n                    'total_checked': len(recent_metrics),\n                    'below_threshold': low_quality_count,\n                    'percentage': round((low_quality_count / len(recent_metrics)) * 100, 2)\n                }\n            })\n        \n        f_grade_count = sum(1 for m in recent_metrics if m.quality_grade == 'F')\n        \n        if f_grade_count > 0:\n            alerts.append({\n                'type': 'critical_quality',\n                'severity': 'critical',\n                'message': f'ì‹¬ê°í•œ í’ˆì§ˆ ë¬¸ì œ: {f_grade_count}ê°œ ë ˆìŠ¤í† ë‘ì´ Fë“±ê¸‰',\n                'details': {\n                    'f_grade_count': f_grade_count\n                }\n            })\n        \n        return alerts\n    \n    def check_system_alerts(self) -> List[Dict[str, Any]]:\n        \"\"\"ì‹œìŠ¤í…œ ê´€ë ¨ ì•Œë¦¼ì„ í™•ì¸í•©ë‹ˆë‹¤.\"\"\"\n        alerts = []\n        \n        components = ['scraper', 'processor', 'sync', 'api', 'database']\n        \n        for component in components:\n            latest = self.db.query(SystemHealth)\\\n                .filter(SystemHealth.component == component)\\\n                .order_by(SystemHealth.measured_at.desc())\\\n                .first()\n            \n            if not latest:\n                continue\n            \n            if latest.component_status == 'down':\n                alerts.append({\n                    'type': 'component_down',\n                    'severity': 'critical',\n                    'message': f'{component} êµ¬ì„± ìš”ì†Œ ë‹¤ìš´',\n                    'component': component,\n                    'details': {\n                        'error_rate': latest.error_rate,\n                        'last_checked': latest.measured_at.isoformat() if latest.measured_at else None\n                    }\n                })\n            \n            elif latest.component_status == 'degraded':\n                alerts.append({\n                    'type': 'component_degraded',\n                    'severity': 'high',\n                    'message': f'{component} ì„±ëŠ¥ ì €í•˜',\n                    'component': component,\n                    'details': {\n                        'error_rate': latest.error_rate,\n                        'response_time_ms': latest.response_time_ms\n                    }\n                })\n            \n            if latest.error_rate > self.alert_thresholds['error_rate']:\n                alerts.append({\n                    'type': 'high_error_rate',\n                    'severity': 'high',\n                    'message': f'{component} ì˜¤ë¥˜ìœ¨ ë†’ìŒ: {latest.error_rate:.1f}%',\n                    'component': component,\n                    'threshold': self.alert_thresholds['error_rate'],\n                    'current_value': latest.error_rate\n                })\n        \n        return alerts\n    \n    def check_all_alerts(self) -> Dict[str, Any]:\n        \"\"\"ëª¨ë“  ì•Œë¦¼ì„ í™•ì¸í•©ë‹ˆë‹¤.\"\"\"\n        quality_alerts = self.check_quality_alerts()\n        system_alerts = self.check_system_alerts()\n        \n        all_alerts = quality_alerts + system_alerts\n        \n        critical_count = sum(1 for a in all_alerts if a.get('severity') == 'critical')\n        high_count = sum(1 for a in all_alerts if a.get('severity') == 'high')\n        \n        status = 'healthy'\n        if critical_count > 0:\n            status = 'critical'\n        elif high_count > 0:\n            status = 'warning'\n        \n        for alert in all_alerts:\n            logger.warning(f\"ğŸš¨ ì•Œë¦¼: [{alert['severity']}] {alert['message']}\")\n        \n        return {\n            'status': status,\n            'total_alerts': len(all_alerts),\n            'critical': critical_count,\n            'high': high_count,\n            'alerts': all_alerts,\n            'checked_at': datetime.now(timezone.utc).isoformat()\n        }\n    \n    def get_alert_history(\n        self,\n        hours: int = 24,\n        severity: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"ì•Œë¦¼ ì´ë ¥ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)\n        \n        health_records = self.db.query(SystemHealth)\\\n            .filter(\n                SystemHealth.measured_at >= cutoff_time,\n                SystemHealth.alerts.isnot(None)\n            )\\\n            .all()\n        \n        all_historical_alerts = []\n        for record in health_records:\n            if record.alerts:\n                for alert in record.alerts:\n                    if severity is None or alert.get('severity') == severity:\n                        alert['component'] = record.component\n                        alert['timestamp'] = record.measured_at.isoformat() if record.measured_at else None\n                        all_historical_alerts.append(alert)\n        \n        return {\n            'total_alerts': len(all_historical_alerts),\n            'time_range_hours': hours,\n            'severity_filter': severity,\n            'alerts': all_historical_alerts\n        }\n","size_bytes":6693},"data-hub/src/governance/__init__.py":{"content":"","size_bytes":0},"data-hub/src/deduplication/detector.py":{"content":"from typing import List, Dict, Tuple, Optional\nfrom fuzzywuzzy import fuzz\nfrom geopy.distance import geodesic\nfrom loguru import logger\n\nfrom src.database.models import ProcessedRestaurant\n\n\nclass DuplicateDetector:\n    def __init__(\n        self,\n        name_threshold: float = 90.0,\n        address_threshold: float = 85.0,\n        distance_threshold_meters: float = 100.0\n    ):\n        self.name_threshold = name_threshold\n        self.address_threshold = address_threshold\n        self.distance_threshold_meters = distance_threshold_meters\n    \n    def detect_duplicates(\n        self,\n        restaurants: List[ProcessedRestaurant]\n    ) -> List[Dict]:\n        logger.info(f\"ğŸ” ì¤‘ë³µ íƒì§€ ì‹œì‘: {len(restaurants)}ê°œ ë ˆìŠ¤í† ë‘\")\n        \n        duplicate_groups = []\n        processed_ids = set()\n        \n        for i, restaurant in enumerate(restaurants):\n            if restaurant.id in processed_ids:\n                continue\n            \n            duplicates = []\n            \n            for j, candidate in enumerate(restaurants):\n                if i >= j or candidate.id in processed_ids:\n                    continue\n                \n                similarity = self._calculate_similarity(restaurant, candidate)\n                \n                if similarity['is_duplicate']:\n                    duplicates.append({\n                        'id': candidate.id,\n                        'name': candidate.name,\n                        'similarity': similarity\n                    })\n                    processed_ids.add(candidate.id)\n            \n            if duplicates:\n                duplicate_groups.append({\n                    'master': {\n                        'id': restaurant.id,\n                        'name': restaurant.name,\n                        'address': restaurant.address\n                    },\n                    'duplicates': duplicates,\n                    'total_duplicates': len(duplicates)\n                })\n                processed_ids.add(restaurant.id)\n        \n        logger.info(f\"âœ… ì¤‘ë³µ íƒì§€ ì™„ë£Œ: {len(duplicate_groups)}ê°œ ê·¸ë£¹ ë°œê²¬\")\n        return duplicate_groups\n    \n    def _calculate_similarity(\n        self,\n        restaurant1: ProcessedRestaurant,\n        restaurant2: ProcessedRestaurant\n    ) -> Dict:\n        name_similarity = self._fuzzy_match_name(\n            restaurant1.name or '',\n            restaurant2.name or ''\n        )\n        \n        address_similarity = self._fuzzy_match_address(\n            restaurant1.address or '',\n            restaurant2.address or ''\n        )\n        \n        distance_meters = self._calculate_distance(\n            restaurant1.latitude,\n            restaurant1.longitude,\n            restaurant2.latitude,\n            restaurant2.longitude\n        )\n        \n        is_duplicate = self._is_duplicate(\n            name_similarity,\n            address_similarity,\n            distance_meters\n        )\n        \n        return {\n            'is_duplicate': is_duplicate,\n            'name_similarity': name_similarity,\n            'address_similarity': address_similarity,\n            'distance_meters': distance_meters,\n            'detection_method': self._get_detection_method(\n                name_similarity,\n                address_similarity,\n                distance_meters\n            )\n        }\n    \n    def _fuzzy_match_name(self, name1: str, name2: str) -> float:\n        if not name1 or not name2:\n            return 0.0\n        \n        name1 = name1.strip().lower()\n        name2 = name2.strip().lower()\n        \n        ratio = fuzz.ratio(name1, name2)\n        partial_ratio = fuzz.partial_ratio(name1, name2)\n        token_sort_ratio = fuzz.token_sort_ratio(name1, name2)\n        \n        return max(ratio, partial_ratio, token_sort_ratio)\n    \n    def _fuzzy_match_address(self, addr1: str, addr2: str) -> float:\n        if not addr1 or not addr2:\n            return 0.0\n        \n        addr1 = addr1.strip().lower()\n        addr2 = addr2.strip().lower()\n        \n        return fuzz.token_sort_ratio(addr1, addr2)\n    \n    def _calculate_distance(\n        self,\n        lat1: Optional[float],\n        lon1: Optional[float],\n        lat2: Optional[float],\n        lon2: Optional[float]\n    ) -> Optional[float]:\n        if not all([lat1, lon1, lat2, lon2]):\n            return None\n        \n        try:\n            distance = geodesic((lat1, lon1), (lat2, lon2)).meters\n            return round(distance, 2)\n        except Exception as e:\n            logger.warning(f\"ê±°ë¦¬ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n            return None\n    \n    def _is_duplicate(\n        self,\n        name_similarity: float,\n        address_similarity: float,\n        distance_meters: Optional[float]\n    ) -> bool:\n        if name_similarity >= self.name_threshold:\n            if address_similarity >= self.address_threshold:\n                return True\n            \n            if distance_meters is not None and distance_meters <= self.distance_threshold_meters:\n                return True\n        \n        if (name_similarity >= 80.0 and \n            distance_meters is not None and \n            distance_meters <= 50.0):\n            return True\n        \n        return False\n    \n    def _get_detection_method(\n        self,\n        name_similarity: float,\n        address_similarity: float,\n        distance_meters: Optional[float]\n    ) -> str:\n        methods = []\n        \n        if name_similarity >= self.name_threshold:\n            methods.append('name')\n        \n        if address_similarity >= self.address_threshold:\n            methods.append('address')\n        \n        if distance_meters is not None and distance_meters <= self.distance_threshold_meters:\n            methods.append('distance')\n        \n        return '+'.join(methods) if methods else 'none'\n","size_bytes":5810},"attached_assets/immediate_action_plan_1762482269383.md":{"content":"# ì¦‰ì‹œ ì‹¤í–‰ ì•¡ì…˜ í”Œëœ: Google Drive ë°±ì—… + íƒ€ì´ë° ìˆ˜ì •\n\n**ì‘ì„±ì¼**: 2025ë…„ 11ì›” 7ì¼, 11:18 AM  \n**ìƒíƒœ**: Replitì— ì§€ì‹œí•  ì¤€ë¹„ ì™„ë£Œ  \n**ëª©í‘œ**: Google Drive ë°±ì—… ê¸°ëŠ¥ ì¶”ê°€ + ì¤‘ë³µ íƒì§€ íƒ€ì´ë° ìˆ˜ì •  \n**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 3-4ì‹œê°„\n\n---\n\n## ğŸ¯ ì™œ ì´ê²Œ í•„ìš”í•œê°€?\n\n### ë‹¹ì‹ ì˜ ì›ë˜ ëª©í‘œ (Q3)\n\n```\n\"ì´ë ‡ê²Œ ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ ë‚˜ì˜ êµ¬ê¸€ë“œë¼ì´ë¸Œë¡œ \nìˆ˜ì§‘í•œ ì¼ìë³„ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ ê³„ì† ëˆ„ì ì‹œí‚¤ê³ ...\"\n\nâ†’ í˜„ì¬: êµ¬í˜„ë˜ì§€ ì•ŠìŒ!\n```\n\n### í˜„ì¬ ìƒíƒœ\n\n```\nPostgreSQL DBì—ë§Œ ë°ì´í„° ì €ì¥\nâ†’ ì„œë²„ ì†ìƒ ì‹œ ëª¨ë“  ë°ì´í„° ì†ì‹¤\nâ†’ ê°ì‹œ ì¶”ì (audit trail) ì—†ìŒ\nâ†’ ì¥ê¸° ë³´ê´€ ì²´ê³„ ì—†ìŒ\n```\n\n### ê°œì„  í›„ ìƒíƒœ\n\n```\në§¤ì¼ 22:00 KST\nâ†’ ë‹¹ì¼ ìˆ˜ì§‘ ë°ì´í„° CSV ìƒì„±\nâ†’ Google Driveì— ìë™ ì—…ë¡œë“œ\nâ†’ /hansikdang-data/2025-11/07-collection.csv\nâ†’ ì˜êµ¬ ë³´ì¡´ + ê°ì‹œ ì¶”ì  + ë³µì› ê°€ëŠ¥\n```\n\n---\n\n## ğŸ“‹ Replitì—ê²Œ ìš”ì²­í•  í”„ë¡¬í”„íŠ¸\n\n### Main Request (ë³µì‚¬-ë¶™ì—¬ë„£ê¸°ìš©)\n\n```\nìš°ë¦¬ Data Hubì— Google Drive ë°±ì—… ê¸°ëŠ¥ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.\n\n**ëª©í‘œ:**\në§¤ì¼ ìˆ˜ì§‘ëœ ëª¨ë“  ë ˆìŠ¤í† ë‘ ë°ì´í„°ë¥¼ CSVë¡œ ë³€í™˜í•˜ì—¬ \nGoogle Driveì— ìë™ìœ¼ë¡œ ì €ì¥í•˜ê³ , ë‚ ì§œë³„ë¡œ ëˆ„ì í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.\n\n**í˜„ì¬ ìƒí™©:**\n- PostgreSQL DBì—ë§Œ ë°ì´í„° ì €ì¥ ì¤‘\n- ê°ì‹œ ì¶”ì (audit trail) ì—†ìŒ\n- ë°±ì—… ì²´ê³„ ì—†ìŒ\n\n**ì›í•˜ëŠ” ê¸°ëŠ¥:**\n\n1. Google Drive API í†µí•©\n   - ê¸°ì¡´ Google Gemini API ì¸ì¦ ì •ë³´ ì¬ì‚¬ìš© ê°€ëŠ¥\n\n2. ì¼ì¼ ìë™ ë°±ì—… (ë§¤ì¼ 22:00 KST)\n   - ë‹¹ì¼ ì‹ ê·œ ìˆ˜ì§‘ ë ˆìŠ¤í† ë‘ ëª¨ë‘ í¬í•¨\n   - CSV í¬ë§·: ID, ì´ë¦„, ì£¼ì†Œ, ì „í™”, í‰ì , ë¦¬ë·°ìˆ˜, ì„¤ëª…, ìƒì„±ì¼, ì¤‘ë³µì—¬ë¶€\n   \n3. í´ë” êµ¬ì¡°\n   - /hansikdang-data/2025-11/ (ì›”ë³„ í´ë”)\n   - íŒŒì¼ëª…: DD-collection.csv (ë‚ ì§œë³„)\n   - ì˜ˆ: 07-collection.csv, 08-collection.csv, ...\n\n4. ë©”íƒ€ë°ì´í„° í¬í•¨\n   - ìˆ˜ì§‘ ê±´ìˆ˜\n   - ì¤‘ë³µ ì œê±° ê±´ìˆ˜\n   - í‰ê·  í’ˆì§ˆ ì ìˆ˜\n   - ë°±ì—… ì‹œê°„\n\n5. API ì—”ë“œí¬ì¸íŠ¸\n   - GET /api/governance/backup/status - ë§ˆì§€ë§‰ ë°±ì—… ìƒíƒœ\n   - GET /api/governance/backup/history - ë°±ì—… ì´ë ¥ (ìµœê·¼ 7ì¼)\n   - POST /api/governance/backup/manual - ìˆ˜ë™ ë°±ì—… (í…ŒìŠ¤íŠ¸ìš©)\n\n6. ì—ëŸ¬ ì²˜ë¦¬\n   - Google Drive API ì‹¤íŒ¨ ì‹œ ë¡œì»¬ í´ë”ì— ë°±ì—…\n   - ì¬ì‹œë„ ë¡œì§ (3íšŒ ìë™ ì¬ì‹œë„)\n   - ì‹¤íŒ¨ ì•Œë¦¼\n\n**ê¸°ìˆ  ê³ ë ¤ì‚¬í•­:**\n- Google Drive API (ë˜ëŠ” google-auth ë¼ì´ë¸ŒëŸ¬ë¦¬)\n- APSchedulerì— ì¶”ê°€ (ë§¤ì¼ 22:00 KST)\n- Pandasë¡œ CSV ë³€í™˜\n- ë©”íƒ€ë°ì´í„°ëŠ” QualityMetrics í…Œì´ë¸” í™œìš©\n\n**ë‹¹ì‹ ì˜ íŒë‹¨:**\n1. ìœ„ì˜ ìŠ¤í™ì´ íƒ€ë‹¹í•œê°€?\n2. ë” ë‚˜ì€ êµ¬í˜„ ë°©ì‹ì´ ìˆëŠ”ê°€?\n3. ì¶”ê°€í•´ì•¼ í•  ê¸°ëŠ¥ì´ ìˆëŠ”ê°€?\n4. ì˜ˆìƒ êµ¬í˜„ ì‹œê°„ì€?\n\nì§„í–‰ ë¶€íƒí•©ë‹ˆë‹¤.\n```\n\n---\n\n## ğŸ”§ Bonus: ì¤‘ë³µ íƒì§€ íƒ€ì´ë° ìˆ˜ì • ìš”ì²­\n\n### ì¶”ê°€ Request (1ì‹œê°„ ì†Œìš”)\n\n```\n**ìˆ˜ì • ìš”ì²­:** Feature 2 (ì¤‘ë³µ íƒì§€) ì‹¤í–‰ íƒ€ì´ë° ë³€ê²½\n\n**í˜„ì¬ ìŠ¤ì¼€ì¤„:**\n- 01:30 KST: Smart Targeting (ë™ì  ì¿¼ë¦¬)\n- 03:00 KST: Naver Scraping (33ê°œ)\n- 05:00 KST: ì¤‘ë³µ íƒì§€ & ë³‘í•© â† ë¬¸ì œ!\n- 06:00 KST: Gemini ì •ì œ\n- 07:00 KST: Google Places\n\n**ë¬¸ì œì :**\nì¤‘ë³µ íƒì§€ê°€ 2ì‹œê°„ ëŠ¦ìŒ (03:00 â†’ 05:00)\nâ†’ ê·¸ ì‚¬ì´ Gemini, Google Places API í˜¸ì¶œ (ë‚­ë¹„)\nâ†’ ì¤‘ë³µ ë°ì´í„°ë„ ì •ì œë¨ (ë¹„íš¨ìœ¨)\n\n**ê°œì„ ì•ˆ:**\n- 03:00 KST: Naver Scraping\n- **03:05 KST**: ì¦‰ì‹œ ì¤‘ë³µ íƒì§€ â† ë³€ê²½\n  (ì‹ ê·œ ë°ì´í„°ë§Œ DB ì €ì¥)\n- 06:00 KST: Gemini ì •ì œ (ì‹ ê·œ ë°ì´í„°ë§Œ)\n- 07:00 KST: Google Places (ì‹ ê·œ ë°ì´í„°ë§Œ)\n\n**íš¨ê³¼:**\n- API í˜¸ì¶œ 20% ê°ì†Œ\n- ë¹„ìš© $6/ì›” ì ˆê°\n- íš¨ìœ¨ì„± í–¥ìƒ\n\n**ìˆ˜ì • ë°©ë²•:**\nscheduler.pyì—ì„œ:\n- generate_smart_targets() â†’ 01:30 (ìœ ì§€)\n- scrape_naver() â†’ 03:00 (ìœ ì§€)\n- detect_duplicates() â†’ 03:05 ë³€ê²½ (05:00ì—ì„œ)\n- quality_validation() â†’ 06:00 (ìœ ì§€)\n\nì§„í–‰ ë¶€íƒí•©ë‹ˆë‹¤.\n```\n\n---\n\n## ğŸ“… ì˜ˆìƒ ì¼ì •\n\n### Day 1 (ë‚´ì¼)\n```\nReplit AIì—ê²Œ ìœ„ í”„ë¡¬í”„íŠ¸ 2ê°œ ì œì‹œ\nì˜ˆìƒ ì‘ë‹µ: 3-4ì‹œê°„ ì‘ì—…\n```\n\n### Day 2 (ëª¨ë ˆ)\n```\nâœ… Google Drive ë°±ì—… êµ¬í˜„ ì™„ë£Œ\nâœ… ì¤‘ë³µ íƒì§€ íƒ€ì´ë° ìˆ˜ì • ì™„ë£Œ\nâœ… ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\nâ†’ ì™„ì„±ë„ 95%+\n```\n\n---\n\n## ğŸ¯ ìµœì¢… ì™„ì„± ì‹œìŠ¤í…œ\n\n### ì „ì²´ ì¼ì¼ ìŠ¤ì¼€ì¤„ (KST)\n\n```\n01:30 - Smart Targeting â† Feature 1\n        (Google Trends ë¶„ì„ + ë™ì  ì¿¼ë¦¬ ìƒì„±)\n        â†’ 33ê°œ ë™ì  ì¿¼ë¦¬ ìƒì„±\n\n03:00 - Naver Scraping\n        (33ê°œ ë ˆìŠ¤í† ë‘ ìŠ¤í¬ë˜í•‘)\n\n03:05 - Duplicate Detection â† Feature 2 (ê°œì„ )\n        (ì‹ ê·œ ë°ì´í„°ë§Œ DB ì €ì¥)\n\n06:00 - Gemini AI Refinement\n        (ì„¤ëª… ìƒì„±)\n\n07:00 - Google Places Enrichment\n        (í‰ì , ì´ë¯¸ì§€, ì˜ì—…ì‹œê°„)\n\n08:00 - Main Platform Sync\n        (í•œì‹ë‹¹ ì•±ê³¼ ë™ê¸°í™”)\n\n22:00 - Google Drive Backup â† NEW (ì¶”ê°€)\n        (ë‹¹ì¼ ë°ì´í„° CSV ì €ì¥)\n\në§¤ì‹œê°„ - System Monitoring â† Feature 3\n        (ê±´ê°• ìƒíƒœ í™•ì¸, ì•Œë¦¼)\n```\n\n### ë°ì´í„° íë¦„\n\n```\nGoogle Trends (ì™¸êµ­ì¸ ê²€ìƒ‰ëŸ‰)\n    â†“\nSmart Targeting (ì ìˆ˜ ê³„ì‚°)\n    â†“\n33ê°œ ë™ì  ì¿¼ë¦¬ ìƒì„±\n    â†“\nNaver Maps ìŠ¤í¬ë˜í•‘\n    â†“\nì¦‰ì‹œ ì¤‘ë³µ íƒì§€ â† ê°œì„ ëœ ì§€ì \n    â†“\nì‹ ê·œ ë°ì´í„°ë§Œ ì €ì¥ (ì¤‘ë³µ ì œê±°)\n    â†“\nGemini ì •ì œ (íš¨ìœ¨ì„± í–¥ìƒ)\n    â†“\nGoogle Places ë³´ê°•\n    â†“\në©”ì¸ ì•± ë™ê¸°í™”\n    â†“\nGoogle Drive ë°±ì—… â† ìƒˆë¡œìš´ ê¸°ëŠ¥\n    â†“\ní’ˆì§ˆ ë©”íŠ¸ë¦­ ê¸°ë¡\n```\n\n---\n\n## âœ… ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n### ë‹¹ì‹ ì˜ ì›ë˜ 3ê°€ì§€ ëª©í‘œ\n\n- [x] **Q1: ì™¸êµ­ì¸ ì¸ê¸°ë„ ê¸°ë°˜ íƒ€ê²ŸíŒ…**\n  - âœ… ì™„ë£Œ (Feature 1)\n  - ìƒíƒœ: 100% ë§Œì¡±\n\n- [ ] **Q2: ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ**\n  - âœ… íƒì§€ ê¸°ëŠ¥ (Feature 2)\n  - âš ï¸ íƒ€ì´ë° ê°œì„  í•„ìš” (ì´ë²ˆì— ìˆ˜ì •)\n  - ìƒíƒœ: 90% â†’ 99%\n\n- [ ] **Q3: Google Drive ëˆ„ì **\n  - âŒ ì•„ì§ ë¯¸êµ¬í˜„ (ì´ë²ˆì— ì¶”ê°€)\n  - ìƒíƒœ: 0% â†’ 100%\n\n---\n\n## ğŸš€ ë‹¤ìŒ ìŠ¤í…\n\n### ì§€ê¸ˆ í•  ì¼ (5ë¶„)\n\n```\n1. ìœ„ì˜ 2ê°œ í”„ë¡¬í”„íŠ¸ ë³µì‚¬\n2. Replit AI ì—´ê¸°\n3. í”„ë¡¬í”„íŠ¸ ë¶™ì—¬ë„£ê¸°\n4. ì‹¤í–‰\n```\n\n### Replitì´ êµ¬í˜„í•˜ëŠ” ë™ì•ˆ (3-4ì‹œê°„)\n\n```\nëª¨ë‹ˆí„°ë§:\n- Replitì˜ ì§„í–‰ ìƒí™© í™•ì¸\n- í•„ìš”ì‹œ ì¶”ê°€ ì„¤ëª… ì œì‹œ\n- êµ¬í˜„ ì™„ë£Œ í›„ í…ŒìŠ¤íŠ¸\n```\n\n### ì™„ë£Œ í›„ (1ì‹œê°„)\n\n```\nì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸:\n- /api/governance/backup/status í™•ì¸\n- Google Driveì— íŒŒì¼ ìƒì„± í™•ì¸\n- ìŠ¤ì¼€ì¤„ ìë™ ì‹¤í–‰ í™•ì¸\n```\n\n---\n\n## ğŸ’¡ ì¶”ê°€ ì •ë³´\n\n### Google Drive API ì¸ì¦\n\n```\nê¸°ì¡´ì— Google Gemini APIë¥¼ ì‚¬ìš© ì¤‘ì´ë¯€ë¡œ\ncredentials.jsonì´ ì´ë¯¸ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n\ní•„ìš”í•œ ê²ƒ:\n1. Google Drive API ìŠ¤ì½”í”„ ì¶”ê°€\n   - https://www.googleapis.com/auth/drive.file\n\n2. í´ë” ìë™ ìƒì„± ë¡œì§\n   - /hansikdang-data í´ë” ìë™ ìƒì„±\n   - ì›”ë³„ í•˜ìœ„í´ë” ìë™ ìƒì„±\n```\n\n### CSV í¬ë§· ì˜ˆì‹œ\n\n```\nid,name,address,phone,naver_place_id,rating,reviews,description,created_at,duplicate_status,quality_score\n1,\"ê°•ë‚¨ ê°ˆë¹„íƒ•\",\"ì„œìš¸ ê°•ë‚¨êµ¬ ì—­ì‚¼ë™ 123\",\"02-1234-5678\",\"12345678\",\"4.5\",\"150\",\"...ì„¤ëª…...\",\"2025-11-07 03:15\",\"new\",\"92.5\"\n2,\"ëª…ë™ í•œì‹ë‹¹\",\"ì„œìš¸ ì¤‘êµ¬ ëª…ë™ 456\",\"02-9876-5432\",\"87654321\",\"4.3\",\"200\",\"...ì„¤ëª…...\",\"2025-11-07 03:20\",\"duplicate\",\"88.0\"\n```\n\n---\n\n**ì¤€ë¹„ ì™„ë£Œ! ë‚´ì¼ ë°”ë¡œ Replitì— ìš”ì²­í•˜ì„¸ìš”.** ğŸš€\n\n","size_bytes":7127},"data-hub/src/governance/quality_validator.py":{"content":"\"\"\"\nQuality Validator - 7ê°€ì§€ í’ˆì§ˆ ì§€í‘œ ì¸¡ì • ì‹œìŠ¤í…œ\n\"\"\"\nimport uuid\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime, timezone\nfrom loguru import logger\nfrom sqlalchemy.orm import Session\n\nfrom ..database.models import ProcessedRestaurant, QualityMetrics\n\n\nclass QualityValidator:\n    \"\"\"\n    ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì—”ì§„\n    7ê°€ì§€ í’ˆì§ˆ ì§€í‘œë¥¼ ì¸¡ì •í•˜ê³  ë“±ê¸‰ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.\n    \"\"\"\n    \n    def __init__(self, db: Session):\n        self.db = db\n        \n        self.required_fields = [\n            'name', 'address', 'category', 'description'\n        ]\n        \n        self.recommended_fields = [\n            'phone', 'hours', 'menu', 'images', \n            'google_rating', 'google_reviews'\n        ]\n    \n    def validate_restaurant(\n        self, \n        restaurant: ProcessedRestaurant\n    ) -> QualityMetrics:\n        \"\"\"\n        ë ˆìŠ¤í† ë‘ ë°ì´í„°ì˜ í’ˆì§ˆì„ 7ê°€ì§€ ì§€í‘œë¡œ ì¸¡ì •í•©ë‹ˆë‹¤.\n        \n        Args:\n            restaurant: ê²€ì¦í•  ë ˆìŠ¤í† ë‘\n            \n        Returns:\n            QualityMetrics: í’ˆì§ˆ ë©”íŠ¸ë¦­ ê°ì²´\n        \"\"\"\n        logger.info(f\"í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {restaurant.name}\")\n        \n        completeness = self._measure_completeness(restaurant)\n        accuracy = self._measure_accuracy(restaurant)\n        consistency = self._measure_consistency(restaurant)\n        timeliness = self._measure_timeliness(restaurant)\n        validity = self._measure_validity(restaurant)\n        uniqueness = self._measure_uniqueness(restaurant)\n        relevance = self._measure_relevance(restaurant)\n        \n        overall_score = (\n            completeness * 0.20 +\n            accuracy * 0.20 +\n            consistency * 0.15 +\n            timeliness * 0.10 +\n            validity * 0.15 +\n            uniqueness * 0.10 +\n            relevance * 0.10\n        )\n        \n        grade = self._calculate_grade(overall_score)\n        issues = self._identify_issues(restaurant)\n        recommendations = self._generate_recommendations(issues)\n        \n        metrics = QualityMetrics(\n            id=str(uuid.uuid4()),\n            restaurant_id=restaurant.id,\n            data_type='restaurant',\n            completeness_score=completeness,\n            accuracy_score=accuracy,\n            consistency_score=consistency,\n            timeliness_score=timeliness,\n            validity_score=validity,\n            uniqueness_score=uniqueness,\n            relevance_score=relevance,\n            overall_quality_score=overall_score,\n            quality_grade=grade,\n            issues=issues,\n            recommendations=recommendations,\n            measured_by='system'\n        )\n        \n        self.db.add(metrics)\n        self.db.commit()\n        \n        logger.info(f\"âœ… í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {restaurant.name} - {grade}ë“±ê¸‰ ({overall_score:.1f}ì )\")\n        \n        return metrics\n    \n    def _measure_completeness(self, restaurant: ProcessedRestaurant) -> float:\n        \"\"\"\n        ì™„ì „ì„± ì¸¡ì •: í•„ìˆ˜ í•„ë“œê°€ ëª¨ë‘ ì±„ì›Œì ¸ ìˆëŠ”ê°€?\n        \"\"\"\n        total_fields = len(self.required_fields) + len(self.recommended_fields)\n        filled_fields = 0\n        \n        for field in self.required_fields:\n            value = getattr(restaurant, field, None)\n            if value and str(value).strip():\n                filled_fields += 1\n        \n        for field in self.recommended_fields:\n            value = getattr(restaurant, field, None)\n            if value:\n                if isinstance(value, list) and len(value) > 0:\n                    filled_fields += 1\n                elif isinstance(value, (str, int, float)) and str(value).strip():\n                    filled_fields += 1\n        \n        score = (filled_fields / total_fields) * 100\n        return min(100, score)\n    \n    def _measure_accuracy(self, restaurant: ProcessedRestaurant) -> float:\n        \"\"\"\n        ì •í™•ì„± ì¸¡ì •: ë°ì´í„°ê°€ ì •í™•í•œê°€?\n        \"\"\"\n        score = 100.0\n        \n        if restaurant.description and len(restaurant.description) < 200:\n            score -= 20\n        \n        if restaurant.google_rating and (restaurant.google_rating < 0 or restaurant.google_rating > 5):\n            score -= 30\n        \n        if restaurant.phone and len(restaurant.phone.replace('-', '').replace(' ', '')) < 9:\n            score -= 10\n        \n        return max(0, score)\n    \n    def _measure_consistency(self, restaurant: ProcessedRestaurant) -> float:\n        \"\"\"\n        ì¼ê´€ì„± ì¸¡ì •: ë°ì´í„°ê°€ ë‹¤ë¥¸ ì†ŒìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ”ê°€?\n        \"\"\"\n        score = 100.0\n        \n        if restaurant.naver_rating and restaurant.google_rating:\n            rating_diff = abs(restaurant.naver_rating - restaurant.google_rating)\n            if rating_diff > 1.0:\n                score -= 30\n            elif rating_diff > 0.5:\n                score -= 15\n        \n        if not restaurant.address or not restaurant.latitude or not restaurant.longitude:\n            score -= 20\n        \n        return max(0, score)\n    \n    def _measure_timeliness(self, restaurant: ProcessedRestaurant) -> float:\n        \"\"\"\n        ì ì‹œì„± ì¸¡ì •: ë°ì´í„°ê°€ ìµœì‹ ì¸ê°€?\n        \"\"\"\n        if not restaurant.created_at:\n            return 50.0\n        \n        now = datetime.now(timezone.utc)\n        age_days = (now - restaurant.created_at).days\n        \n        if age_days <= 1:\n            return 100.0\n        elif age_days <= 7:\n            return 90.0\n        elif age_days <= 30:\n            return 70.0\n        elif age_days <= 90:\n            return 50.0\n        else:\n            return 30.0\n    \n    def _measure_validity(self, restaurant: ProcessedRestaurant) -> float:\n        \"\"\"\n        ìœ íš¨ì„± ì¸¡ì •: ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€?\n        \"\"\"\n        score = 100.0\n        \n        if restaurant.latitude:\n            if not (33 <= restaurant.latitude <= 43):\n                score -= 30\n        \n        if restaurant.longitude:\n            if not (124 <= restaurant.longitude <= 132):\n                score -= 30\n        \n        if restaurant.category:\n            valid_categories = ['í•œì‹', 'ì¼ì‹', 'ì¤‘ì‹', 'ì–‘ì‹', 'ì¹´í˜', 'ì£¼ì ', 'ë¶„ì‹', 'ì•„ì‹œì•ˆ']\n            if not any(cat in restaurant.category for cat in valid_categories):\n                score -= 10\n        \n        return max(0, score)\n    \n    def _measure_uniqueness(self, restaurant: ProcessedRestaurant) -> float:\n        \"\"\"\n        ê³ ìœ ì„± ì¸¡ì •: ì¤‘ë³µë˜ì§€ ì•Šì€ ë°ì´í„°ì¸ê°€?\n        \"\"\"\n        duplicates_count = self.db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.name == restaurant.name,\n            ProcessedRestaurant.id != restaurant.id\n        ).count()\n        \n        if duplicates_count == 0:\n            return 100.0\n        elif duplicates_count == 1:\n            return 70.0\n        else:\n            return 40.0\n    \n    def _measure_relevance(self, restaurant: ProcessedRestaurant) -> float:\n        \"\"\"\n        ê´€ë ¨ì„± ì¸¡ì •: í•œì‹ë‹¹ í”Œë«í¼ì— ì í•©í•œê°€?\n        \"\"\"\n        score = 100.0\n        \n        if restaurant.category:\n            korean_keywords = ['í•œì‹', 'í•œì •ì‹', 'êµ­ë°¥', 'ì°Œê°œ', 'ì‚¼ê²¹ì‚´', 'ê°ˆë¹„', 'ë¹„ë¹”ë°¥']\n            if not any(keyword in restaurant.category for keyword in korean_keywords):\n                score -= 30\n        \n        if not restaurant.description:\n            score -= 20\n        elif len(restaurant.description) < 200:\n            score -= 10\n        \n        if not restaurant.google_rating or restaurant.google_rating < 3.5:\n            score -= 20\n        \n        return max(0, score)\n    \n    def _calculate_grade(self, score: float) -> str:\n        \"\"\"ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜\"\"\"\n        if score >= 90:\n            return 'A'\n        elif score >= 80:\n            return 'B'\n        elif score >= 70:\n            return 'C'\n        elif score >= 60:\n            return 'D'\n        else:\n            return 'F'\n    \n    def _identify_issues(self, restaurant: ProcessedRestaurant) -> List[Dict[str, str]]:\n        \"\"\"í’ˆì§ˆ ì´ìŠˆ ì‹ë³„\"\"\"\n        issues = []\n        \n        for field in self.required_fields:\n            value = getattr(restaurant, field, None)\n            if not value or not str(value).strip():\n                issues.append({\n                    'field': field,\n                    'severity': 'critical',\n                    'message': f'í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}'\n                })\n        \n        if restaurant.description and len(restaurant.description) < 200:\n            issues.append({\n                'field': 'description',\n                'severity': 'warning',\n                'message': f'ì„¤ëª…ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(restaurant.description)}ì, ê¶Œì¥: 200ì ì´ìƒ)'\n            })\n        \n        if not restaurant.google_rating:\n            issues.append({\n                'field': 'google_rating',\n                'severity': 'warning',\n                'message': 'Google í‰ì  ëˆ„ë½'\n            })\n        \n        return issues\n    \n    def _generate_recommendations(self, issues: List[Dict[str, str]]) -> List[str]:\n        \"\"\"ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±\"\"\"\n        recommendations = []\n        \n        for issue in issues:\n            if issue['field'] == 'description':\n                recommendations.append('Gemini AIë¡œ ì„¤ëª… ì¬ìƒì„± í•„ìš”')\n            elif issue['field'] == 'google_rating':\n                recommendations.append('Google Places APIë¡œ í‰ì  ìˆ˜ì§‘ í•„ìš”')\n            elif issue['severity'] == 'critical':\n                recommendations.append(f'{issue[\"field\"]} í•„ë“œ ë°ì´í„° ë³´ì™„ í•„ìš”')\n        \n        return recommendations\n    \n    def validate_batch(\n        self,\n        batch_size: int = 100\n    ) -> Dict[str, Any]:\n        \"\"\"\n        ì¼ê´„ í’ˆì§ˆ ê²€ì¦\n        \n        Args:\n            batch_size: í•œ ë²ˆì— ê²€ì¦í•  ë ˆìŠ¤í† ë‘ ìˆ˜\n            \n        Returns:\n            í†µê³„ ì •ë³´\n        \"\"\"\n        logger.info(f\"ì¼ê´„ í’ˆì§ˆ ê²€ì¦ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})\")\n        \n        restaurants = self.db.query(ProcessedRestaurant).limit(batch_size).all()\n        \n        total = len(restaurants)\n        grades = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'F': 0}\n        avg_score = 0.0\n        \n        for restaurant in restaurants:\n            metrics = self.validate_restaurant(restaurant)\n            grades[metrics.quality_grade] += 1\n            avg_score += metrics.overall_quality_score\n        \n        if total > 0:\n            avg_score /= total\n        \n        stats = {\n            'total_validated': total,\n            'average_score': round(avg_score, 2),\n            'grade_distribution': grades,\n            'a_grade_percentage': round((grades['A'] / total * 100) if total > 0 else 0, 2)\n        }\n        \n        logger.info(f\"âœ… ì¼ê´„ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {total}ê°œ, í‰ê·  {avg_score:.1f}ì \")\n        \n        return stats\n","size_bytes":10942},"data-hub/src/api/deduplication_routes.py":{"content":"from fastapi import APIRouter, Depends, HTTPException, Query\nfrom sqlalchemy.orm import Session\nfrom typing import Optional\nfrom loguru import logger\n\nfrom src.database.connection import get_db\nfrom src.deduplication.service import DeduplicationService\n\nrouter = APIRouter(prefix=\"/api/duplicates\", tags=[\"duplicates\"])\n\n\n@router.post(\"/detect\")\nasync def detect_duplicates(\n    auto_merge: bool = Query(False, description=\"ìë™ ë³‘í•© ì—¬ë¶€\"),\n    name_threshold: float = Query(90.0, ge=0, le=100, description=\"ì´ë¦„ ìœ ì‚¬ë„ ì„ê³„ê°’\"),\n    address_threshold: float = Query(85.0, ge=0, le=100, description=\"ì£¼ì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’\"),\n    distance_threshold: float = Query(100.0, ge=0, description=\"ê±°ë¦¬ ì„ê³„ê°’ (ë¯¸í„°)\"),\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    ë ˆìŠ¤í† ë‘ ì¤‘ë³µ íƒì§€ ë° ìë™ ë³‘í•©\n    \n    - **auto_merge**: Trueì‹œ ì¤‘ë³µ ìë™ ë³‘í•© (ê¸°ë³¸ê°’: False)\n    - **name_threshold**: ì´ë¦„ ìœ ì‚¬ë„ ì„ê³„ê°’ (0-100, ê¸°ë³¸ê°’: 90)\n    - **address_threshold**: ì£¼ì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0-100, ê¸°ë³¸ê°’: 85)\n    - **distance_threshold**: GPS ê±°ë¦¬ ì„ê³„ê°’ (ë¯¸í„°, ê¸°ë³¸ê°’: 100)\n    \"\"\"\n    try:\n        logger.info(f\"ğŸ” ì¤‘ë³µ íƒì§€ ìš”ì²­: auto_merge={auto_merge}\")\n        \n        service = DeduplicationService(\n            db=db,\n            name_threshold=name_threshold,\n            address_threshold=address_threshold,\n            distance_threshold_meters=distance_threshold\n        )\n        \n        result = service.detect_and_merge_duplicates(\n            auto_merge=auto_merge,\n            merge_type='auto' if auto_merge else 'manual'\n        )\n        \n        return {\n            \"status\": \"success\",\n            \"data\": result,\n            \"message\": f\"ì¤‘ë³µ íƒì§€ ì™„ë£Œ: {result['duplicate_groups_found']}ê°œ ê·¸ë£¹ ë°œê²¬\"\n        }\n        \n    except Exception as e:\n        logger.error(f\"âŒ ì¤‘ë³µ íƒì§€ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/groups\")\nasync def get_duplicate_groups(\n    status: Optional[str] = Query(None, description=\"ìƒíƒœ í•„í„° (detected, merged, ignored)\"),\n    limit: int = Query(100, ge=1, le=1000, description=\"ì¡°íšŒ ê°œìˆ˜\"),\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    ì¤‘ë³µ ê·¸ë£¹ ëª©ë¡ ì¡°íšŒ\n    \n    - **status**: ìƒíƒœ í•„í„° (detected: íƒì§€ë¨, merged: ë³‘í•©ë¨, ignored: ë¬´ì‹œë¨)\n    - **limit**: ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)\n    \"\"\"\n    try:\n        service = DeduplicationService(db=db)\n        groups = service.get_duplicate_groups(status=status, limit=limit)\n        \n        result = []\n        for group in groups:\n            result.append({\n                \"id\": group.id,\n                \"master_id\": group.master_id,\n                \"duplicate_ids\": group.duplicate_ids,\n                \"total_duplicates\": len(group.duplicate_ids) if group.duplicate_ids else 0,\n                \"similarity_scores\": group.similarity_scores,\n                \"detection_method\": group.detection_method,\n                \"status\": group.status,\n                \"created_at\": group.created_at.isoformat() if group.created_at else None,\n                \"merged_at\": group.merged_at.isoformat() if group.merged_at else None\n            })\n        \n        return {\n            \"status\": \"success\",\n            \"data\": {\n                \"total\": len(result),\n                \"groups\": result\n            }\n        }\n        \n    except Exception as e:\n        logger.error(f\"âŒ ì¤‘ë³µ ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/history\")\nasync def get_merge_history(\n    limit: int = Query(100, ge=1, le=1000, description=\"ì¡°íšŒ ê°œìˆ˜\"),\n    merge_type: Optional[str] = Query(None, description=\"ë³‘í•© íƒ€ì… (auto, manual)\"),\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    ë ˆìŠ¤í† ë‘ ë³‘í•© ì´ë ¥ ì¡°íšŒ\n    \n    - **limit**: ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)\n    - **merge_type**: ë³‘í•© íƒ€ì… (auto: ìë™, manual: ìˆ˜ë™)\n    \"\"\"\n    try:\n        service = DeduplicationService(db=db)\n        history = service.get_merge_history(limit=limit, merge_type=merge_type)\n        \n        result = []\n        for record in history:\n            result.append({\n                \"id\": record.id,\n                \"duplicate_group_id\": record.duplicate_group_id,\n                \"master_id\": record.master_id,\n                \"merged_ids\": record.merged_ids,\n                \"total_merged\": len(record.merged_ids) if record.merged_ids else 0,\n                \"merge_reason\": record.merge_reason,\n                \"similarity_details\": record.similarity_details,\n                \"merge_type\": record.merge_type,\n                \"merged_by\": record.merged_by,\n                \"merged_at\": record.merged_at.isoformat() if record.merged_at else None\n            })\n        \n        return {\n            \"status\": \"success\",\n            \"data\": {\n                \"total\": len(result),\n                \"history\": result\n            }\n        }\n        \n    except Exception as e:\n        logger.error(f\"âŒ ë³‘í•© ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/stats\")\nasync def get_deduplication_stats(db: Session = Depends(get_db)):\n    \"\"\"\n    ì¤‘ë³µ ì œê±° í†µê³„ ì¡°íšŒ\n    \n    ì „ì²´ ì¤‘ë³µ ê·¸ë£¹, ë³‘í•© ì´ë ¥, ì²˜ë¦¬ëœ ë ˆìŠ¤í† ë‘ ìˆ˜ ë“±\n    \"\"\"\n    try:\n        service = DeduplicationService(db=db)\n        stats = service.get_deduplication_stats()\n        \n        return {\n            \"status\": \"success\",\n            \"data\": stats\n        }\n        \n    except Exception as e:\n        logger.error(f\"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n","size_bytes":5703},"data-hub/src/api/monitoring_routes.py":{"content":"\"\"\"\nMonitoring API Routes - ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì—”ë“œí¬ì¸íŠ¸\n\"\"\"\nfrom typing import Optional\nfrom fastapi import APIRouter, Depends\nfrom sqlalchemy.orm import Session\n\nfrom ..database.connection import get_db\nfrom ..monitoring.system_monitor import SystemMonitor\nfrom ..monitoring.alert_manager import AlertManager\n\n\nrouter = APIRouter(prefix=\"/api/monitoring\", tags=[\"monitoring\"])\n\n\n@router.get(\"/health\")\ndef get_system_health(db: Session = Depends(get_db)):\n    \"\"\"ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    monitor = SystemMonitor(db)\n    overview = monitor.get_system_overview()\n    \n    return {\n        \"status\": \"success\",\n        \"overview\": overview\n    }\n\n\n@router.get(\"/health/{component}\")\ndef get_component_health(\n    component: str,\n    hours: int = 24,\n    db: Session = Depends(get_db)\n):\n    \"\"\"íŠ¹ì • êµ¬ì„± ìš”ì†Œì˜ ê±´ê°• ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    monitor = SystemMonitor(db)\n    trends = monitor.get_performance_trends(component=component, hours=hours)\n    \n    return {\n        \"status\": \"success\",\n        \"component\": component,\n        \"trends\": trends\n    }\n\n\n@router.get(\"/alerts\")\ndef get_alerts(db: Session = Depends(get_db)):\n    \"\"\"í˜„ì¬ í™œì„± ì•Œë¦¼ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    alert_manager = AlertManager(db)\n    alerts = alert_manager.check_all_alerts()\n    \n    return {\n        \"status\": \"success\",\n        \"alerts\": alerts\n    }\n\n\n@router.get(\"/alerts/history\")\ndef get_alert_history(\n    hours: int = 24,\n    severity: Optional[str] = None,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ì•Œë¦¼ ì´ë ¥ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    alert_manager = AlertManager(db)\n    history = alert_manager.get_alert_history(hours=hours, severity=severity)\n    \n    return {\n        \"status\": \"success\",\n        \"history\": history\n    }\n\n\n@router.get(\"/dashboard\")\ndef get_dashboard_data(db: Session = Depends(get_db)):\n    \"\"\"ëŒ€ì‹œë³´ë“œìš© ì¢…í•© ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\"\"\n    monitor = SystemMonitor(db)\n    alert_manager = AlertManager(db)\n    \n    system_overview = monitor.get_system_overview()\n    alerts = alert_manager.check_all_alerts()\n    \n    from ..database.models import RawRestaurantData, ProcessedRestaurant, QualityMetrics\n    \n    total_raw = db.query(RawRestaurantData).count()\n    total_processed = db.query(ProcessedRestaurant).count()\n    \n    recent_metrics = db.query(QualityMetrics).order_by(\n        QualityMetrics.measured_at.desc()\n    ).limit(100).all()\n    \n    avg_quality = 0\n    if recent_metrics:\n        avg_quality = sum(m.overall_quality_score for m in recent_metrics) / len(recent_metrics)\n    \n    return {\n        \"status\": \"success\",\n        \"dashboard\": {\n            \"system\": system_overview,\n            \"alerts\": {\n                \"status\": alerts['status'],\n                \"total\": alerts['total_alerts'],\n                \"critical\": alerts['critical'],\n                \"high\": alerts['high']\n            },\n            \"data\": {\n                \"total_raw\": total_raw,\n                \"total_processed\": total_processed,\n                \"processing_rate\": round((total_processed / total_raw * 100) if total_raw > 0 else 0, 2),\n                \"average_quality\": round(avg_quality, 2)\n            }\n        }\n    }\n","size_bytes":3218},"data-hub/src/targeting/__init__.py":{"content":"\"\"\"\nìŠ¤ë§ˆíŠ¸ íƒ€ê²ŸíŒ… ì‹œìŠ¤í…œ\nSmart Targeting System for Dynamic Query Generation\n\"\"\"\n\nfrom .trends_analyzer import TrendsAnalyzer\nfrom .query_generator import QueryGenerator\nfrom .popularity_scorer import PopularityScorer\n\n__all__ = ['TrendsAnalyzer', 'QueryGenerator', 'PopularityScorer']\n","size_bytes":295},"data-hub/src/governance/lineage_tracker.py":{"content":"\"\"\"\nData Lineage Tracker - ë°ì´í„° ê³„ë³´ ì¶”ì  ì‹œìŠ¤í…œ\n\"\"\"\nimport uuid\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime, timezone\nfrom loguru import logger\nfrom sqlalchemy.orm import Session\n\nfrom ..database.models import DataLineage\n\n\nclass DataLineageTracker:\n    \"\"\"\n    ë°ì´í„° ë³€í™˜ ì´ë ¥ ì¶”ì  ì‹œìŠ¤í…œ\n    ìŠ¤í¬ë˜í•‘ â†’ ì •ì œ â†’ ë³‘í•© â†’ ë™ê¸°í™” ì „ ê³¼ì •ì„ ê¸°ë¡í•©ë‹ˆë‹¤.\n    \"\"\"\n    \n    def __init__(self, db: Session):\n        self.db = db\n    \n    def track_operation(\n        self,\n        entity_id: str,\n        operation: str,\n        source_system: str,\n        input_data: Optional[Dict] = None,\n        output_data: Optional[Dict] = None,\n        transformation_rules: Optional[Dict] = None,\n        quality_before: Optional[float] = None,\n        quality_after: Optional[float] = None,\n        execution_time_ms: Optional[int] = None,\n        status: str = 'success',\n        executed_by: str = 'system'\n    ) -> DataLineage:\n        \"\"\"\n        ë°ì´í„° ë³€í™˜ ì‘ì—…ì„ ì¶”ì í•©ë‹ˆë‹¤.\n        \n        Args:\n            entity_id: ë ˆìŠ¤í† ë‘ ID\n            operation: ì‘ì—… ìœ í˜• (scraped, processed, merged, synced)\n            source_system: ì†ŒìŠ¤ ì‹œìŠ¤í…œ (naver, google, gemini, system)\n            input_data: ì…ë ¥ ë°ì´í„° (ìƒ˜í”Œ)\n            output_data: ì¶œë ¥ ë°ì´í„° (ìƒ˜í”Œ)\n            transformation_rules: ì ìš©ëœ ê·œì¹™\n            quality_before: ë³€í™˜ ì „ í’ˆì§ˆ ì ìˆ˜\n            quality_after: ë³€í™˜ í›„ í’ˆì§ˆ ì ìˆ˜\n            execution_time_ms: ì‹¤í–‰ ì‹œê°„ (ë°€ë¦¬ì´ˆ)\n            status: ì‘ì—… ìƒíƒœ\n            executed_by: ì‹¤í–‰ ì£¼ì²´\n            \n        Returns:\n            DataLineage: ê³„ë³´ ë ˆì½”ë“œ\n        \"\"\"\n        quality_delta = None\n        if quality_before is not None and quality_after is not None:\n            quality_delta = quality_after - quality_before\n        \n        lineage = DataLineage(\n            id=str(uuid.uuid4()),\n            entity_id=entity_id,\n            entity_type='restaurant',\n            operation=operation,\n            operation_status=status,\n            source_system=source_system,\n            source_id=entity_id,\n            input_data=self._sample_data(input_data),\n            output_data=self._sample_data(output_data),\n            transformation_rules=transformation_rules,\n            quality_before=quality_before,\n            quality_after=quality_after,\n            quality_delta=quality_delta,\n            executed_by=executed_by,\n            execution_time_ms=execution_time_ms\n        )\n        \n        self.db.add(lineage)\n        self.db.commit()\n        \n        logger.debug(f\"ğŸ“Š ê³„ë³´ ì¶”ì : {entity_id} - {operation} by {source_system}\")\n        \n        return lineage\n    \n    def _sample_data(self, data: Optional[Dict], max_fields: int = 5) -> Optional[Dict]:\n        \"\"\"ë°ì´í„° ìƒ˜í”Œë§ (ì €ì¥ ê³µê°„ ì ˆì•½)\"\"\"\n        if not data:\n            return None\n        \n        if isinstance(data, dict):\n            keys = list(data.keys())[:max_fields]\n            return {k: data[k] for k in keys}\n        \n        return data\n    \n    def get_lineage(\n        self,\n        entity_id: str,\n        limit: int = 100\n    ) -> List[DataLineage]:\n        \"\"\"\n        íŠ¹ì • ë ˆìŠ¤í† ë‘ì˜ ê³„ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\n        \n        Args:\n            entity_id: ë ˆìŠ¤í† ë‘ ID\n            limit: ìµœëŒ€ ì¡°íšŒ ìˆ˜\n            \n        Returns:\n            ê³„ë³´ ë ˆì½”ë“œ ëª©ë¡\n        \"\"\"\n        return self.db.query(DataLineage)\\\n            .filter(DataLineage.entity_id == entity_id)\\\n            .order_by(DataLineage.executed_at.desc())\\\n            .limit(limit)\\\n            .all()\n    \n    def get_operation_stats(\n        self,\n        operation: Optional[str] = None,\n        hours: int = 24\n    ) -> Dict[str, Any]:\n        \"\"\"\n        ì‘ì—… í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\n        \n        Args:\n            operation: ì‘ì—… ìœ í˜• í•„í„°\n            hours: ì¡°íšŒ ì‹œê°„ ë²”ìœ„ (ì‹œê°„)\n            \n        Returns:\n            í†µê³„ ì •ë³´\n        \"\"\"\n        from datetime import timedelta\n        \n        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)\n        \n        query = self.db.query(DataLineage)\\\n            .filter(DataLineage.executed_at >= cutoff_time)\n        \n        if operation:\n            query = query.filter(DataLineage.operation == operation)\n        \n        records = query.all()\n        \n        total = len(records)\n        success = sum(1 for r in records if r.operation_status == 'success')\n        failed = sum(1 for r in records if r.operation_status == 'failed')\n        \n        avg_exec_time = 0\n        if records:\n            exec_times = [r.execution_time_ms for r in records if r.execution_time_ms]\n            if exec_times:\n                avg_exec_time = sum(exec_times) / len(exec_times)\n        \n        avg_quality_delta = 0\n        if records:\n            deltas = [r.quality_delta for r in records if r.quality_delta is not None]\n            if deltas:\n                avg_quality_delta = sum(deltas) / len(deltas)\n        \n        return {\n            'total_operations': total,\n            'successful': success,\n            'failed': failed,\n            'success_rate': round((success / total * 100) if total > 0 else 0, 2),\n            'avg_execution_time_ms': round(avg_exec_time, 2),\n            'avg_quality_improvement': round(avg_quality_delta, 2)\n        }\n    \n    def trace_entity_journey(\n        self,\n        entity_id: str\n    ) -> Dict[str, Any]:\n        \"\"\"\n        ë ˆìŠ¤í† ë‘ì˜ ì „ì²´ ì—¬ì •ì„ ì¶”ì í•©ë‹ˆë‹¤.\n        \n        Args:\n            entity_id: ë ˆìŠ¤í† ë‘ ID\n            \n        Returns:\n            ì—¬ì • ì •ë³´\n        \"\"\"\n        lineage = self.get_lineage(entity_id)\n        \n        if not lineage:\n            return {\n                'entity_id': entity_id,\n                'total_operations': 0,\n                'journey': []\n            }\n        \n        journey = []\n        for record in reversed(lineage):\n            journey.append({\n                'timestamp': record.executed_at.isoformat() if record.executed_at else None,\n                'operation': record.operation,\n                'source': record.source_system,\n                'status': record.operation_status,\n                'quality_change': record.quality_delta,\n                'execution_time_ms': record.execution_time_ms\n            })\n        \n        initial_quality = lineage[-1].quality_before if lineage else None\n        final_quality = lineage[0].quality_after if lineage else None\n        \n        return {\n            'entity_id': entity_id,\n            'total_operations': len(lineage),\n            'initial_quality': initial_quality,\n            'final_quality': final_quality,\n            'total_improvement': final_quality - initial_quality if (initial_quality and final_quality) else None,\n            'journey': journey\n        }\n","size_bytes":6958},"data-hub/src/targeting/query_generator.py":{"content":"\"\"\"\në™ì  ì¿¼ë¦¬ ìƒì„±ê¸°\nì™¸êµ­ì¸ ì¸ê¸°ë„ ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¼ 33ê°œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë™ì  ìƒì„±\n\"\"\"\n\nfrom typing import List, Dict\nfrom loguru import logger\nimport random\n\n\nclass QueryGenerator:\n    \"\"\"ì§€ì—­ë³„ ì¸ê¸°ë„ ê¸°ë°˜ ë™ì  ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±\"\"\"\n    \n    FOOD_CATEGORIES = [\n        \"í•œì‹\", \"ëƒ‰ë©´\", \"ì‚¼ê²¹ì‚´\", \"ë¶ˆê³ ê¸°\", \"ë¹„ë¹”ë°¥\", \"ê°ˆë¹„\",\n        \"ì°Œê°œ\", \"ê¹€ì¹˜ì°Œê°œ\", \"ëœì¥ì°Œê°œ\", \"ìˆœë‘ë¶€ì°Œê°œ\",\n        \"í•œì •ì‹\", \"ë³´ìŒˆ\", \"ì¡±ë°œ\", \"ê³±ì°½\", \"ì‚¼ê³„íƒ•\",\n        \"ì„¤ë íƒ•\", \"ì¹¼êµ­ìˆ˜\", \"ê¹€ë°¥\", \"ë–¡ë³¶ì´\", \"ìˆœëŒ€\"\n    ]\n    \n    GENERAL_KEYWORDS = [\n        \"ë§›ì§‘\", \"í•œì‹ë‹¹\", \"ì „í†µìŒì‹\", \"ë¡œì»¬ë§›ì§‘\", \"ì¸ê¸°ë§›ì§‘\"\n    ]\n    \n    def __init__(self):\n        \"\"\"ì´ˆê¸°í™”\"\"\"\n        self.logger = logger\n    \n    async def generate_daily_queries(\n        self,\n        top_regions: List[tuple],\n        target_count: int = 33\n    ) -> List[str]:\n        \"\"\"\n        ìƒìœ„ ì§€ì—­ ê¸°ë°˜ 33ê°œ ë™ì  ì¿¼ë¦¬ ìƒì„±\n        \n        Args:\n            top_regions: [(ì§€ì—­ëª…, ì ìˆ˜), ...] í˜•ì‹ì˜ ìƒìœ„ ì§€ì—­ ë¦¬ìŠ¤íŠ¸\n            target_count: ëª©í‘œ ì¿¼ë¦¬ ê°œìˆ˜ (ê¸°ë³¸: 33)\n            \n        Returns:\n            ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸\n        \"\"\"\n        try:\n            self.logger.info(f\"ğŸ¯ ë™ì  ì¿¼ë¦¬ ìƒì„± ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)\")\n            \n            queries = []\n            regions = [region for region, score in top_regions]\n            \n            # ì§€ì—­ë‹¹ ì¿¼ë¦¬ ê°œìˆ˜ ê³„ì‚°\n            queries_per_region = target_count // len(regions)\n            remainder = target_count % len(regions)\n            \n            for i, (region, score) in enumerate(top_regions):\n                # ìƒìœ„ ì§€ì—­ì¼ìˆ˜ë¡ +1ê°œ ë” ìƒì„±\n                count = queries_per_region + (1 if i < remainder else 0)\n                \n                region_queries = self._generate_region_queries(region, count)\n                queries.extend(region_queries)\n                \n                self.logger.debug(f\"  âœ“ {region}: {len(region_queries)}ê°œ ìƒì„±\")\n            \n            # ëª©í‘œ ê°œìˆ˜ ì •í™•íˆ ë§ì¶”ê¸°\n            if len(queries) > target_count:\n                queries = queries[:target_count]\n            elif len(queries) < target_count:\n                # ë¶€ì¡±í•˜ë©´ ëœë¤ ì¶”ê°€\n                additional = self._generate_random_queries(target_count - len(queries))\n                queries.extend(additional)\n            \n            # ì¤‘ë³µ ì œê±°\n            queries = list(dict.fromkeys(queries))\n            \n            # ë‹¤ì‹œ ë¶€ì¡±í•˜ë©´ ì±„ìš°ê¸°\n            while len(queries) < target_count:\n                additional = self._generate_random_queries(1)\n                if additional[0] not in queries:\n                    queries.extend(additional)\n            \n            self.logger.info(f\"âœ… ë™ì  ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {len(queries)}ê°œ\")\n            return queries[:target_count]\n            \n        except Exception as e:\n            self.logger.error(f\"âŒ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}\")\n            return self._get_default_queries(target_count)\n    \n    def _generate_region_queries(\n        self,\n        region: str,\n        count: int\n    ) -> List[str]:\n        \"\"\"\n        íŠ¹ì • ì§€ì—­ì˜ ë‹¤ì–‘í•œ ì¿¼ë¦¬ ìƒì„±\n        \n        Args:\n            region: ì§€ì—­ëª… (ì˜ˆ: \"ê°•ë‚¨êµ¬\")\n            count: ìƒì„±í•  ì¿¼ë¦¬ ê°œìˆ˜\n            \n        Returns:\n            ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸\n        \"\"\"\n        queries = []\n        region_short = region.replace(\"êµ¬\", \"\")  # \"ê°•ë‚¨êµ¬\" -> \"ê°•ë‚¨\"\n        \n        # 1. ìŒì‹ ì¹´í…Œê³ ë¦¬ ì¿¼ë¦¬ (60%)\n        food_count = int(count * 0.6)\n        selected_foods = random.sample(self.FOOD_CATEGORIES, min(food_count, len(self.FOOD_CATEGORIES)))\n        for food in selected_foods:\n            queries.append(f\"{region_short} {food}\")\n        \n        # 2. ì¼ë°˜ í‚¤ì›Œë“œ ì¿¼ë¦¬ (40%)\n        general_count = count - food_count\n        selected_general = random.sample(self.GENERAL_KEYWORDS, min(general_count, len(self.GENERAL_KEYWORDS)))\n        for keyword in selected_general:\n            queries.append(f\"{region_short} {keyword}\")\n        \n        # ë¶€ì¡±í•˜ë©´ ì¡°í•©ìœ¼ë¡œ ì±„ìš°ê¸°\n        while len(queries) < count:\n            food = random.choice(self.FOOD_CATEGORIES)\n            queries.append(f\"{region_short} {food}\")\n        \n        return queries[:count]\n    \n    def _generate_random_queries(self, count: int) -> List[str]:\n        \"\"\"\n        ëœë¤ ì¿¼ë¦¬ ìƒì„± (Fallback)\n        \n        Args:\n            count: ìƒì„±í•  ì¿¼ë¦¬ ê°œìˆ˜\n            \n        Returns:\n            ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸\n        \"\"\"\n        queries = []\n        regions = [\"í™ëŒ€\", \"ê°•ë‚¨\", \"ëª…ë™\", \"ì´íƒœì›\", \"ì—¬ì˜ë„\", \"ì ì‹¤\", \"ì¢…ë¡œ\"]\n        \n        for _ in range(count):\n            region = random.choice(regions)\n            category = random.choice(self.FOOD_CATEGORIES + self.GENERAL_KEYWORDS)\n            queries.append(f\"{region} {category}\")\n        \n        return queries\n    \n    def _get_default_queries(self, count: int = 33) -> List[str]:\n        \"\"\"\n        ê¸°ë³¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (ì™„ì „ Fallback)\n        \n        Args:\n            count: ì¿¼ë¦¬ ê°œìˆ˜\n            \n        Returns:\n            ê¸°ë³¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸\n        \"\"\"\n        default = [\n            \"í™ëŒ€ í•œì‹\", \"ê°•ë‚¨ í•œì‹ë‹¹\", \"ëª…ë™ í•œì‹\", \"ì—¬ì˜ë„ ë§›ì§‘\",\n            \"ì´íƒœì› í•œì‹\", \"ì„œìš¸ ì‚¼ê³„íƒ•\", \"ì„œìš¸ ë¶ˆê³ ê¸°\", \"ì„œìš¸ ë¹„ë¹”ë°¥\",\n            \"ì„œìš¸ ê°ˆë¹„\", \"ì„œìš¸ ëƒ‰ë©´\", \"ì„œìš¸ ì°Œê°œ\", \"ê°•ë‚¨ ëƒ‰ë©´\",\n            \"í™ëŒ€ ì‚¼ê²¹ì‚´\", \"ëª…ë™ í•œì •ì‹\", \"ì´íƒœì› ë¶ˆê³ ê¸°\", \"ì—¬ì˜ë„ í•œì‹\",\n            \"ì ì‹¤ ë§›ì§‘\", \"ì¢…ë¡œ í•œì‹\", \"ê°•ë‚¨ ì¡±ë°œ\", \"í™ëŒ€ ë³´ìŒˆ\",\n            \"ëª…ë™ ê°ˆë¹„\", \"ì´íƒœì› ì°Œê°œ\", \"ì—¬ì˜ë„ ëƒ‰ë©´\", \"ì ì‹¤ í•œì‹\",\n            \"ì¢…ë¡œ ì‚¼ê³„íƒ•\", \"ê°•ë‚¨ ê³±ì°½\", \"í™ëŒ€ ê¹€ì¹˜ì°Œê°œ\", \"ëª…ë™ ìˆœë‘ë¶€\",\n            \"ì´íƒœì› ì„¤ë íƒ•\", \"ì—¬ì˜ë„ ì¹¼êµ­ìˆ˜\", \"ì ì‹¤ ë¹„ë¹”ë°¥\", \"ì¢…ë¡œ ë¶ˆê³ ê¸°\",\n            \"ê°•ë‚¨ ë³´ìŒˆ\"\n        ]\n        \n        return default[:count]\n    \n    def get_query_diversity_score(self, queries: List[str]) -> Dict[str, any]:\n        \"\"\"\n        ì¿¼ë¦¬ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°\n        \n        Args:\n            queries: ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸\n            \n        Returns:\n            ë‹¤ì–‘ì„± í†µê³„\n        \"\"\"\n        regions = set()\n        categories = set()\n        \n        for query in queries:\n            parts = query.split()\n            if len(parts) >= 2:\n                regions.add(parts[0])\n                categories.add(parts[1])\n        \n        return {\n            \"total_queries\": len(queries),\n            \"unique_regions\": len(regions),\n            \"unique_categories\": len(categories),\n            \"diversity_score\": round((len(regions) * len(categories)) / len(queries) * 100, 2)\n        }\n","size_bytes":6860},"data-hub/src/api/governance_routes.py":{"content":"\"\"\"\nGovernance API Routes - ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ì—”ë“œí¬ì¸íŠ¸\n\"\"\"\nfrom typing import Optional\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm import Session\nfrom pydantic import BaseModel\n\nfrom ..database.connection import get_db\nfrom ..database.models import QualityMetrics, DataLineage, ProcessedRestaurant\nfrom ..governance.quality_validator import QualityValidator\nfrom ..governance.lineage_tracker import DataLineageTracker\nfrom ..governance.drive_backup import DriveBackupManager\n\n\nrouter = APIRouter(prefix=\"/api/governance\", tags=[\"governance\"])\n\n\nclass QualityValidationRequest(BaseModel):\n    restaurant_id: Optional[str] = None\n    batch_size: Optional[int] = 100\n\n\n@router.post(\"/quality/validate\")\ndef validate_quality(\n    request: QualityValidationRequest,\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.\n    \n    - restaurant_idê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë ˆìŠ¤í† ë‘ë§Œ ê²€ì¦\n    - ì—†ìœ¼ë©´ ì¼ê´„ ê²€ì¦ (batch_size ê°œìˆ˜ë§Œí¼)\n    \"\"\"\n    validator = QualityValidator(db)\n    \n    if request.restaurant_id:\n        restaurant = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.id == request.restaurant_id\n        ).first()\n        \n        if not restaurant:\n            raise HTTPException(status_code=404, detail=\"ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        metrics = validator.validate_restaurant(restaurant)\n        \n        return {\n            \"status\": \"success\",\n            \"restaurant_id\": request.restaurant_id,\n            \"quality_score\": metrics.overall_quality_score,\n            \"quality_grade\": metrics.quality_grade,\n            \"metrics\": {\n                \"completeness\": metrics.completeness_score,\n                \"accuracy\": metrics.accuracy_score,\n                \"consistency\": metrics.consistency_score,\n                \"timeliness\": metrics.timeliness_score,\n                \"validity\": metrics.validity_score,\n                \"uniqueness\": metrics.uniqueness_score,\n                \"relevance\": metrics.relevance_score\n            },\n            \"issues\": metrics.issues,\n            \"recommendations\": metrics.recommendations\n        }\n    else:\n        stats = validator.validate_batch(batch_size=request.batch_size)\n        \n        return {\n            \"status\": \"success\",\n            \"type\": \"batch\",\n            \"statistics\": stats\n        }\n\n\n@router.get(\"/quality/metrics\")\ndef get_quality_metrics(\n    restaurant_id: Optional[str] = None,\n    limit: int = 100,\n    db: Session = Depends(get_db)\n):\n    \"\"\"í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    query = db.query(QualityMetrics)\n    \n    if restaurant_id:\n        query = query.filter(QualityMetrics.restaurant_id == restaurant_id)\n    \n    metrics = query.order_by(QualityMetrics.measured_at.desc()).limit(limit).all()\n    \n    return {\n        \"status\": \"success\",\n        \"total\": len(metrics),\n        \"metrics\": [\n            {\n                \"id\": m.id,\n                \"restaurant_id\": m.restaurant_id,\n                \"overall_score\": m.overall_quality_score,\n                \"grade\": m.quality_grade,\n                \"measured_at\": m.measured_at.isoformat() if m.measured_at else None\n            }\n            for m in metrics\n        ]\n    }\n\n\n@router.get(\"/quality/stats\")\ndef get_quality_stats(db: Session = Depends(get_db)):\n    \"\"\"í’ˆì§ˆ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    metrics = db.query(QualityMetrics).all()\n    \n    if not metrics:\n        return {\n            \"status\": \"success\",\n            \"total_validated\": 0,\n            \"average_score\": 0,\n            \"grade_distribution\": {}\n        }\n    \n    total = len(metrics)\n    avg_score = sum(m.overall_quality_score for m in metrics) / total\n    \n    grade_dist = {}\n    for grade in ['A', 'B', 'C', 'D', 'F']:\n        count = sum(1 for m in metrics if m.quality_grade == grade)\n        grade_dist[grade] = count\n    \n    return {\n        \"status\": \"success\",\n        \"total_validated\": total,\n        \"average_score\": round(avg_score, 2),\n        \"grade_distribution\": grade_dist,\n        \"a_grade_percentage\": round((grade_dist.get('A', 0) / total * 100) if total > 0 else 0, 2)\n    }\n\n\n@router.get(\"/lineage/stats\")\ndef get_lineage_stats(\n    operation: Optional[str] = None,\n    hours: int = 24,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ê³„ë³´ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    tracker = DataLineageTracker(db)\n    stats = tracker.get_operation_stats(operation=operation, hours=hours)\n    \n    return {\n        \"status\": \"success\",\n        \"statistics\": stats\n    }\n\n\n@router.get(\"/lineage/{entity_id}\")\ndef get_lineage(\n    entity_id: str,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ë°ì´í„° ê³„ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    tracker = DataLineageTracker(db)\n    journey = tracker.trace_entity_journey(entity_id)\n    \n    if journey['total_operations'] == 0:\n        raise HTTPException(status_code=404, detail=\"ê³„ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n    \n    return {\n        \"status\": \"success\",\n        \"lineage\": journey\n    }\n\n\n@router.get(\"/backup/status\")\ndef get_backup_status(db: Session = Depends(get_db)):\n    \"\"\"ìµœê·¼ ë°±ì—… ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    backup_manager = DriveBackupManager(db)\n    status = backup_manager.get_backup_status()\n    \n    return {\n        \"status\": \"success\",\n        \"backup\": status\n    }\n\n\n@router.get(\"/backup/history\")\ndef get_backup_history(\n    days: int = 7,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ë°±ì—… ì´ë ¥ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    backup_manager = DriveBackupManager(db)\n    history = backup_manager.get_backup_history(days=days)\n    \n    return {\n        \"status\": \"success\",\n        \"total\": len(history),\n        \"history\": history\n    }\n\n\nclass BackupRequest(BaseModel):\n    backup_date: Optional[str] = None\n    backup_type: str = 'manual'\n\n\n@router.post(\"/backup/manual\")\ndef manual_backup(\n    request: BackupRequest,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ë™ ë°±ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸ìš©).\"\"\"\n    backup_manager = DriveBackupManager(db)\n    \n    try:\n        backup = backup_manager.backup_daily(\n            backup_date=request.backup_date,\n            backup_type=request.backup_type\n        )\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"ë°±ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"backup\": {\n                \"backup_date\": backup.backup_date,\n                \"file_path\": backup.file_path,\n                \"total_records\": backup.total_records,\n                \"file_size_mb\": round(backup.file_size_bytes / 1024 / 1024, 2) if backup.file_size_bytes else 0,\n                \"execution_time_seconds\": backup.execution_time_seconds\n            }\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ë°±ì—… ì‹¤íŒ¨: {str(e)}\")\n","size_bytes":6797},"data-hub/src/targeting/trends_analyzer.py":{"content":"\"\"\"\nGoogle Trends ë¶„ì„ê¸°\nì™¸êµ­ì¸ ê´€ê´‘ê°ì˜ ì„œìš¸ ì§€ì—­ë³„ í•œì‹ ê´€ì‹¬ë„ë¥¼ ë¶„ì„\n\"\"\"\n\nfrom typing import Dict, List, Optional\nfrom datetime import datetime, timedelta\nfrom loguru import logger\nfrom pytrends.request import TrendReq\nimport pandas as pd\nimport asyncio\n\n\nclass TrendsAnalyzer:\n    \"\"\"Google Trends ë°ì´í„°ë¥¼ í™œìš©í•œ ì§€ì—­ë³„ ì¸ê¸°ë„ ë¶„ì„\"\"\"\n    \n    SEOUL_REGIONS = [\n        \"ê°•ë‚¨êµ¬\",\n        \"ì¤‘êµ¬\",\n        \"ì¢…ë¡œêµ¬\",\n        \"ìš©ì‚°êµ¬\",\n        \"ë§ˆí¬êµ¬\",\n        \"ì˜ë“±í¬êµ¬\",\n        \"ì†¡íŒŒêµ¬\",\n        \"ì„œì´ˆêµ¬\"\n    ]\n    \n    KEYWORDS = [\n        \"Korean food Seoul\",\n        \"Korean restaurant Seoul\",\n        \"Seoul food tour\",\n        \"Korean BBQ Seoul\",\n        \"Traditional Korean food\"\n    ]\n    \n    def __init__(self):\n        \"\"\"ì´ˆê¸°í™”\"\"\"\n        self.pytrends = None\n        self.logger = logger\n        \n    def _get_pytrends(self) -> TrendReq:\n        \"\"\"PyTrends ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì¬ì‚¬ìš©)\"\"\"\n        if self.pytrends is None:\n            self.pytrends = TrendReq(hl='en-US', tz=540)\n        return self.pytrends\n    \n    async def get_regional_popularity(\n        self, \n        regions: Optional[List[str]] = None,\n        days: int = 7\n    ) -> Dict[str, float]:\n        \"\"\"\n        ì§€ì—­ë³„ ì™¸êµ­ì¸ ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚°\n        \n        Args:\n            regions: ë¶„ì„í•  ì§€ì—­ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: SEOUL_REGIONS)\n            days: ë¶„ì„ ê¸°ê°„ (ì¼ ë‹¨ìœ„, ê¸°ë³¸: 7ì¼)\n            \n        Returns:\n            {ì§€ì—­ëª…: ì¸ê¸°ë„ ì ìˆ˜(0-100)}\n        \"\"\"\n        if regions is None:\n            regions = self.SEOUL_REGIONS\n            \n        try:\n            self.logger.info(f\"ğŸ” Google Trends ë¶„ì„ ì‹œì‘ (ì§€ì—­: {len(regions)}ê°œ, ê¸°ê°„: {days}ì¼)\")\n            \n            # ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•´ ë™ê¸° í•¨ìˆ˜ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰\n            loop = asyncio.get_event_loop()\n            scores = await loop.run_in_executor(\n                None,\n                self._fetch_trends_data,\n                regions,\n                days\n            )\n            \n            self.logger.info(f\"âœ… Google Trends ë¶„ì„ ì™„ë£Œ: {len(scores)}ê°œ ì§€ì—­\")\n            return scores\n            \n        except Exception as e:\n            self.logger.error(f\"âŒ Google Trends ë¶„ì„ ì‹¤íŒ¨: {e}\")\n            # Fallback: ëª¨ë“  ì§€ì—­ì— ë™ì¼ ì ìˆ˜\n            return {region: 50.0 for region in regions}\n    \n    def _fetch_trends_data(self, regions: List[str], days: int) -> Dict[str, float]:\n        \"\"\"\n        ì‹¤ì œ Google Trends ë°ì´í„° ìˆ˜ì§‘ (ë™ê¸° í•¨ìˆ˜)\n        \n        Args:\n            regions: ì§€ì—­ ë¦¬ìŠ¤íŠ¸\n            days: ë¶„ì„ ê¸°ê°„\n            \n        Returns:\n            ì§€ì—­ë³„ ì ìˆ˜\n        \"\"\"\n        pytrends = self._get_pytrends()\n        regional_scores = {region: 0.0 for region in regions}\n        \n        try:\n            # ê° í‚¤ì›Œë“œë³„ë¡œ ì§€ì—­ ê²€ìƒ‰ëŸ‰ ìˆ˜ì§‘\n            for keyword in self.KEYWORDS:\n                try:\n                    # ì‹œê°„ ë²”ìœ„ ì„¤ì •\n                    timeframe = f'now {days}-d'\n                    \n                    # í‚¤ì›Œë“œ + ì§€ì—­ ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰\n                    search_terms = [f\"{keyword} {region}\" for region in regions[:5]]\n                    \n                    pytrends.build_payload(\n                        search_terms,\n                        timeframe=timeframe,\n                        geo='KR'\n                    )\n                    \n                    # Interest over time ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n                    data = pytrends.interest_over_time()\n                    \n                    if not data.empty and len(data.columns) > 0:\n                        # ê° ì§€ì—­ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°\n                        for i, region in enumerate(regions[:5]):\n                            if i < len(data.columns) - 1:  # 'isPartial' ì»¬ëŸ¼ ì œì™¸\n                                col_name = search_terms[i]\n                                if col_name in data.columns:\n                                    avg_score = data[col_name].mean()\n                                    regional_scores[region] += avg_score\n                        \n                        self.logger.debug(f\"  âœ“ {keyword}: ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ\")\n                    \n                    # Rate limiting ë°©ì§€\n                    import time\n                    time.sleep(2)\n                    \n                except Exception as e:\n                    self.logger.warning(f\"  âš ï¸  {keyword} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")\n                    continue\n            \n            # ì •ê·œí™” (0-100)\n            if regional_scores:\n                max_score = max(regional_scores.values()) if max(regional_scores.values()) > 0 else 1\n                regional_scores = {\n                    region: round((score / max_score) * 100, 2)\n                    for region, score in regional_scores.items()\n                }\n            \n            # ë‚˜ë¨¸ì§€ ì§€ì—­ì€ í‰ê· ê°’ í• ë‹¹\n            avg_score = sum(regional_scores.values()) / len(regional_scores) if regional_scores else 50.0\n            for region in regions:\n                if regional_scores[region] == 0.0:\n                    regional_scores[region] = avg_score * 0.8\n            \n            return regional_scores\n            \n        except Exception as e:\n            self.logger.error(f\"âŒ Trends ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}\")\n            # Fallback\n            return {region: 50.0 for region in regions}\n    \n    async def get_top_regions(\n        self,\n        count: int = 7,\n        days: int = 7\n    ) -> List[tuple]:\n        \"\"\"\n        ìƒìœ„ Nê°œ ì¸ê¸° ì§€ì—­ ë°˜í™˜\n        \n        Args:\n            count: ë°˜í™˜í•  ì§€ì—­ ê°œìˆ˜\n            days: ë¶„ì„ ê¸°ê°„\n            \n        Returns:\n            [(ì§€ì—­ëª…, ì ìˆ˜), ...] (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ)\n        \"\"\"\n        scores = await self.get_regional_popularity(days=days)\n        sorted_regions = sorted(\n            scores.items(),\n            key=lambda x: x[1],\n            reverse=True\n        )\n        return sorted_regions[:count]\n    \n    async def get_trend_history(\n        self,\n        region: str,\n        days: int = 30\n    ) -> List[Dict[str, any]]:\n        \"\"\"\n        íŠ¹ì • ì§€ì—­ì˜ íŠ¸ë Œë“œ íˆìŠ¤í† ë¦¬ ë°˜í™˜\n        \n        Args:\n            region: ì§€ì—­ëª…\n            days: ë¶„ì„ ê¸°ê°„\n            \n        Returns:\n            [{\"date\": \"2025-11-01\", \"score\": 75.5}, ...]\n        \"\"\"\n        try:\n            pytrends = self._get_pytrends()\n            timeframe = f'now {days}-d'\n            \n            # ëŒ€í‘œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰\n            keyword = f\"Korean food {region}\"\n            pytrends.build_payload([keyword], timeframe=timeframe, geo='KR')\n            \n            data = pytrends.interest_over_time()\n            \n            if data.empty:\n                return []\n            \n            history = []\n            for date, row in data.iterrows():\n                if keyword in data.columns:\n                    history.append({\n                        \"date\": date.strftime(\"%Y-%m-%d\"),\n                        \"score\": float(row[keyword])\n                    })\n            \n            return history\n            \n        except Exception as e:\n            self.logger.error(f\"âŒ íŠ¸ë Œë“œ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n            return []\n","size_bytes":7385},"data-hub/src/api/targeting_routes.py":{"content":"\"\"\"\nìŠ¤ë§ˆíŠ¸ íƒ€ê²ŸíŒ… API ì—”ë“œí¬ì¸íŠ¸\nSmart Targeting System API Routes\n\"\"\"\n\nfrom fastapi import APIRouter, HTTPException, Depends\nfrom typing import List, Dict, Optional\nfrom datetime import datetime, timedelta\nfrom loguru import logger\nfrom sqlalchemy import desc\n\nfrom ..targeting.trends_analyzer import TrendsAnalyzer\nfrom ..targeting.query_generator import QueryGenerator\nfrom ..targeting.popularity_scorer import PopularityScorer\nfrom ..database.connection import db_session\nfrom ..database.models import ScrapingTarget\n\nrouter = APIRouter(prefix=\"/api/targeting\", tags=[\"targeting\"])\n\n# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤\ntrends_analyzer = TrendsAnalyzer()\nquery_generator = QueryGenerator()\npopularity_scorer = PopularityScorer()\n\n\n@router.get(\"/popularity\")\nasync def get_regional_popularity(days: int = 7):\n    \"\"\"\n    ì§€ì—­ë³„ ì™¸êµ­ì¸ ì¸ê¸°ë„ ì ìˆ˜ ì¡°íšŒ\n    \n    Args:\n        days: ë¶„ì„ ê¸°ê°„ (ì¼ ë‹¨ìœ„, ê¸°ë³¸: 7ì¼)\n        \n    Returns:\n        {ì§€ì—­ëª…: ì ìˆ˜(0-100)}\n    \"\"\"\n    try:\n        logger.info(f\"ğŸ“Š ì§€ì—­ë³„ ì¸ê¸°ë„ ì¡°íšŒ ìš”ì²­ (ê¸°ê°„: {days}ì¼)\")\n        \n        scores = await trends_analyzer.get_regional_popularity(days=days)\n        \n        # íˆìŠ¤í† ë¦¬ì— ì €ì¥\n        for region, score in scores.items():\n            popularity_scorer.update_history(region, score)\n        \n        # íŠ¸ë Œë“œ ë°©í–¥ ì¶”ê°€\n        result = {}\n        for region, score in scores.items():\n            result[region] = {\n                \"score\": score,\n                \"trend\": popularity_scorer.get_trend_direction(region, days),\n                \"historical_avg\": popularity_scorer.get_historical_avg(region, days)\n            }\n        \n        return {\n            \"status\": \"success\",\n            \"data\": result,\n            \"analyzed_at\": datetime.now().isoformat()\n        }\n        \n    except Exception as e:\n        logger.error(f\"âŒ ì¸ê¸°ë„ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/top-regions\")\nasync def get_top_regions(count: int = 7, days: int = 7):\n    \"\"\"\n    ìƒìœ„ Nê°œ ì¸ê¸° ì§€ì—­ ì¡°íšŒ\n    \n    Args:\n        count: ë°˜í™˜í•  ì§€ì—­ ê°œìˆ˜ (ê¸°ë³¸: 7)\n        days: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸: 7ì¼)\n        \n    Returns:\n        ìƒìœ„ ì§€ì—­ ë¦¬ìŠ¤íŠ¸\n    \"\"\"\n    try:\n        logger.info(f\"ğŸ† ìƒìœ„ {count}ê°œ ì§€ì—­ ì¡°íšŒ\")\n        \n        top_regions = await trends_analyzer.get_top_regions(count=count, days=days)\n        \n        result = [\n            {\n                \"rank\": i + 1,\n                \"region\": region,\n                \"score\": score,\n                \"trend\": popularity_scorer.get_trend_direction(region, days)\n            }\n            for i, (region, score) in enumerate(top_regions)\n        ]\n        \n        return {\n            \"status\": \"success\",\n            \"data\": result,\n            \"total\": len(result)\n        }\n        \n    except Exception as e:\n        logger.error(f\"âŒ ìƒìœ„ ì§€ì—­ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/regenerate\")\nasync def regenerate_queries(target_count: int = 33, days: int = 7):\n    \"\"\"\n    ë™ì  ì¿¼ë¦¬ ìˆ˜ë™ ì¬ìƒì„±\n    \n    Args:\n        target_count: ìƒì„±í•  ì¿¼ë¦¬ ê°œìˆ˜ (ê¸°ë³¸: 33)\n        days: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸: 7ì¼)\n        \n    Returns:\n        ìƒì„±ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸\n    \"\"\"\n    try:\n        logger.info(f\"ğŸ”„ ì¿¼ë¦¬ ìˆ˜ë™ ì¬ìƒì„± ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)\")\n        \n        # 1. ìƒìœ„ ì§€ì—­ ë¶„ì„\n        top_regions = await trends_analyzer.get_top_regions(count=7, days=days)\n        \n        # 2. ë™ì  ì¿¼ë¦¬ ìƒì„±\n        queries = await query_generator.generate_daily_queries(top_regions, target_count)\n        \n        # 3. DBì— ì €ì¥\n        with db_session() as db:\n            # ê¸°ì¡´ ìë™ ìƒì„± ì¿¼ë¦¬ ì‚­ì œ\n            db.query(ScrapingTarget).filter_by(created_by='auto').delete()\n            \n            # ìƒˆ ì¿¼ë¦¬ ì €ì¥\n            for query in queries:\n                target = ScrapingTarget(\n                    id=f\"auto_{datetime.now().strftime('%Y%m%d')}_{queries.index(query)}\",\n                    keyword=query,\n                    region=query.split()[0] if query.split() else \"\",\n                    priority=5,\n                    status='active',\n                    created_by='auto'\n                )\n                db.add(target)\n            \n            db.commit()\n        \n        # 4. ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°\n        diversity = query_generator.get_query_diversity_score(queries)\n        \n        logger.info(f\"âœ… ì¿¼ë¦¬ ì¬ìƒì„± ì™„ë£Œ: {len(queries)}ê°œ\")\n        \n        return {\n            \"status\": \"success\",\n            \"data\": {\n                \"queries\": queries,\n                \"top_regions\": [\n                    {\"region\": r, \"score\": s} for r, s in top_regions\n                ],\n                \"diversity\": diversity,\n                \"generated_at\": datetime.now().isoformat()\n            }\n        }\n        \n    except Exception as e:\n        logger.error(f\"âŒ ì¿¼ë¦¬ ì¬ìƒì„± ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/stats\")\nasync def get_targeting_stats():\n    \"\"\"\n    íƒ€ê²ŸíŒ… ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ\n    \n    Returns:\n        ì „ì²´ í†µê³„\n    \"\"\"\n    try:\n        # ì¸ê¸°ë„ í†µê³„\n        popularity_stats = popularity_scorer.get_all_stats()\n        \n        # DB ì¿¼ë¦¬ í†µê³„\n        with db_session() as db:\n            total_targets = db.query(ScrapingTarget).count()\n            active_targets = db.query(ScrapingTarget).filter_by(status='active').count()\n            auto_targets = db.query(ScrapingTarget).filter_by(created_by='auto').count()\n        \n        return {\n            \"status\": \"success\",\n            \"data\": {\n                \"popularity\": popularity_stats,\n                \"targets\": {\n                    \"total\": total_targets,\n                    \"active\": active_targets,\n                    \"auto_generated\": auto_targets\n                },\n                \"updated_at\": datetime.now().isoformat()\n            }\n        }\n        \n    except Exception as e:\n        logger.error(f\"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/queries/today\")\nasync def get_today_queries():\n    \"\"\"\n    ì˜¤ëŠ˜ì˜ ë™ì  ì¿¼ë¦¬ ì¡°íšŒ\n    \n    Returns:\n        ì˜¤ëŠ˜ ìƒì„±ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸\n    \"\"\"\n    try:\n        with db_session() as db:\n            today = datetime.now().date()\n            \n            # ì˜¤ëŠ˜ ìƒì„±ëœ ìë™ ì¿¼ë¦¬ ì¡°íšŒ\n            targets = db.query(ScrapingTarget).filter(\n                ScrapingTarget.created_by == 'auto',\n                ScrapingTarget.status == 'active'\n            ).order_by(desc(ScrapingTarget.created_at)).limit(33).all()\n            \n            queries = [\n                {\n                    \"keyword\": t.keyword,\n                    \"region\": t.region,\n                    \"priority\": t.priority,\n                    \"created_at\": t.created_at.isoformat() if t.created_at else None\n                }\n                for t in targets\n            ]\n            \n            return {\n                \"status\": \"success\",\n                \"data\": {\n                    \"queries\": queries,\n                    \"total\": len(queries),\n                    \"date\": today.isoformat()\n                }\n            }\n            \n    except Exception as e:\n        logger.error(f\"âŒ ì˜¤ëŠ˜ì˜ ì¿¼ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/queries/history\")\nasync def get_query_history(days: int = 7):\n    \"\"\"\n    ì¿¼ë¦¬ íˆìŠ¤í† ë¦¬ ì¡°íšŒ\n    \n    Args:\n        days: ì¡°íšŒ ê¸°ê°„ (ì¼ ë‹¨ìœ„, ê¸°ë³¸: 7ì¼)\n        \n    Returns:\n        íˆìŠ¤í† ë¦¬ ë°ì´í„°\n    \"\"\"\n    try:\n        with db_session() as db:\n            cutoff = datetime.now() - timedelta(days=days)\n            \n            targets = db.query(ScrapingTarget).filter(\n                ScrapingTarget.created_by == 'auto',\n                ScrapingTarget.created_at >= cutoff\n            ).order_by(desc(ScrapingTarget.created_at)).all()\n            \n            # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”\n            history_by_date = {}\n            for target in targets:\n                if target.created_at:\n                    date_key = target.created_at.date().isoformat()\n                    if date_key not in history_by_date:\n                        history_by_date[date_key] = []\n                    history_by_date[date_key].append({\n                        \"keyword\": target.keyword,\n                        \"region\": target.region\n                    })\n            \n            return {\n                \"status\": \"success\",\n                \"data\": {\n                    \"history\": history_by_date,\n                    \"period_days\": days,\n                    \"total_dates\": len(history_by_date)\n                }\n            }\n            \n    except Exception as e:\n        logger.error(f\"âŒ ì¿¼ë¦¬ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/health\")\nasync def targeting_health_check():\n    \"\"\"íƒ€ê²ŸíŒ… ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬\"\"\"\n    try:\n        # Google Trends ì—°ê²° í…ŒìŠ¤íŠ¸\n        test_scores = await trends_analyzer.get_regional_popularity(\n            regions=[\"ê°•ë‚¨êµ¬\"],\n            days=1\n        )\n        \n        trends_ok = \"ê°•ë‚¨êµ¬\" in test_scores\n        \n        return {\n            \"status\": \"healthy\" if trends_ok else \"degraded\",\n            \"components\": {\n                \"google_trends\": \"ok\" if trends_ok else \"error\",\n                \"query_generator\": \"ok\",\n                \"popularity_scorer\": \"ok\"\n            },\n            \"checked_at\": datetime.now().isoformat()\n        }\n        \n    except Exception as e:\n        logger.error(f\"âŒ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}\")\n        return {\n            \"status\": \"unhealthy\",\n            \"error\": str(e),\n            \"checked_at\": datetime.now().isoformat()\n        }\n","size_bytes":9993},"data-hub/src/deduplication/merger.py":{"content":"import uuid\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\nfrom sqlalchemy.orm import Session\nfrom loguru import logger\n\nfrom src.database.models import (\n    ProcessedRestaurant,\n    DuplicateGroup,\n    MergeHistory\n)\n\n\nclass MergeManager:\n    def __init__(self, db: Session):\n        self.db = db\n    \n    def merge_duplicates(\n        self,\n        duplicate_group: Dict,\n        merge_type: str = 'auto',\n        merged_by: str = 'system'\n    ) -> Optional[str]:\n        try:\n            master_id = duplicate_group['master']['id']\n            duplicate_ids = [d['id'] for d in duplicate_group['duplicates']]\n            \n            master = self.db.query(ProcessedRestaurant).filter(\n                ProcessedRestaurant.id == master_id\n            ).first()\n            \n            if not master:\n                logger.error(f\"Master ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {master_id}\")\n                return None\n            \n            duplicates = self.db.query(ProcessedRestaurant).filter(\n                ProcessedRestaurant.id.in_(duplicate_ids)\n            ).all()\n            \n            if not duplicates:\n                logger.warning(f\"ì¤‘ë³µ ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {duplicate_ids}\")\n                return None\n            \n            logger.info(f\"ğŸ”€ ë³‘í•© ì‹œì‘: {master.name} + {len(duplicates)}ê°œ\")\n            \n            merged_master = self._merge_restaurant_data(master, duplicates)\n            \n            merged_data_backup = []\n            for dup in duplicates:\n                merged_data_backup.append({\n                    'id': dup.id,\n                    'name': dup.name,\n                    'address': dup.address,\n                    'rating': dup.rating,\n                    'review_count': dup.review_count\n                })\n            \n            group_id = str(uuid.uuid4())\n            duplicate_group_record = DuplicateGroup(\n                id=group_id,\n                master_id=master.id,\n                duplicate_ids=duplicate_ids,\n                similarity_scores={\n                    d['id']: d['similarity']\n                    for d in duplicate_group['duplicates']\n                },\n                detection_method=duplicate_group['duplicates'][0]['similarity']['detection_method'],\n                status='merged',\n                merged_at=datetime.utcnow()\n            )\n            self.db.add(duplicate_group_record)\n            \n            merge_history = MergeHistory(\n                id=str(uuid.uuid4()),\n                duplicate_group_id=group_id,\n                master_id=master.id,\n                merged_ids=duplicate_ids,\n                merge_reason=f\"ì¤‘ë³µ íƒì§€: {len(duplicates)}ê°œ ë ˆìŠ¤í† ë‘ ë³‘í•©\",\n                similarity_details={\n                    d['id']: {\n                        'name': d['similarity']['name_similarity'],\n                        'address': d['similarity']['address_similarity'],\n                        'distance': d['similarity']['distance_meters']\n                    }\n                    for d in duplicate_group['duplicates']\n                },\n                merged_data=merged_data_backup,\n                merge_type=merge_type,\n                merged_by=merged_by,\n                merged_at=datetime.utcnow()\n            )\n            self.db.add(merge_history)\n            \n            for dup in duplicates:\n                self.db.delete(dup)\n            \n            self.db.commit()\n            \n            logger.info(f\"âœ… ë³‘í•© ì™„ë£Œ: {master.name} (ê·¸ë£¹ ID: {group_id})\")\n            return group_id\n            \n        except Exception as e:\n            self.db.rollback()\n            logger.error(f\"âŒ ë³‘í•© ì‹¤íŒ¨: {e}\")\n            return None\n    \n    def _merge_restaurant_data(\n        self,\n        master: ProcessedRestaurant,\n        duplicates: List[ProcessedRestaurant]\n    ) -> ProcessedRestaurant:\n        all_restaurants = [master] + duplicates\n        \n        master.rating = self._choose_best_value(\n            [r.rating for r in all_restaurants],\n            prefer='max'\n        ) or master.rating\n        \n        master.review_count = sum(\n            r.review_count for r in all_restaurants if r.review_count\n        ) or master.review_count\n        \n        master.naver_rating = self._choose_best_value(\n            [r.naver_rating for r in all_restaurants],\n            prefer='max'\n        ) or master.naver_rating\n        \n        master.naver_review_count = sum(\n            r.naver_review_count for r in all_restaurants if r.naver_review_count\n        ) or master.naver_review_count\n        \n        master.google_rating = self._choose_best_value(\n            [r.google_rating for r in all_restaurants],\n            prefer='max'\n        ) or master.google_rating\n        \n        master.google_review_count = sum(\n            r.google_review_count for r in all_restaurants if r.google_review_count\n        ) or master.google_review_count\n        \n        if not master.phone:\n            master.phone = self._choose_best_value(\n                [r.phone for r in all_restaurants],\n                prefer='first'\n            )\n        \n        if not master.website:\n            master.website = self._choose_best_value(\n                [r.website for r in all_restaurants],\n                prefer='first'\n            )\n        \n        if not master.description or len(master.description or '') < 50:\n            master.description = self._choose_best_value(\n                [r.description for r in all_restaurants],\n                prefer='longest'\n            )\n        \n        if not master.description_en or len(master.description_en or '') < 50:\n            master.description_en = self._choose_best_value(\n                [r.description_en for r in all_restaurants],\n                prefer='longest'\n            )\n        \n        all_image_urls = []\n        for r in all_restaurants:\n            if r.image_url:\n                all_image_urls.append(r.image_url)\n            if r.image_urls:\n                all_image_urls.extend(r.image_urls)\n        \n        unique_images = list(set(all_image_urls))[:10]\n        if unique_images:\n            master.image_url = unique_images[0]\n            master.image_urls = unique_images[1:] if len(unique_images) > 1 else []\n        \n        if not master.menu_summary:\n            all_menus = []\n            for r in all_restaurants:\n                if r.menu_summary:\n                    all_menus.extend(r.menu_summary)\n            if all_menus:\n                seen = set()\n                unique_menus = []\n                for menu in all_menus:\n                    menu_key = menu.get('name', '').lower()\n                    if menu_key and menu_key not in seen:\n                        seen.add(menu_key)\n                        unique_menus.append(menu)\n                master.menu_summary = unique_menus[:10]\n        \n        master.updated_at = datetime.utcnow()\n        \n        return master\n    \n    def _choose_best_value(\n        self,\n        values: List,\n        prefer: str = 'first'\n    ):\n        valid_values = [v for v in values if v is not None]\n        \n        if not valid_values:\n            return None\n        \n        if prefer == 'max':\n            return max(valid_values)\n        elif prefer == 'longest':\n            return max(valid_values, key=lambda x: len(str(x)))\n        else:\n            return valid_values[0]\n","size_bytes":7423},"data-hub/src/governance/drive_backup.py":{"content":"\"\"\"\nGoogle Drive Backup Manager - ì¼ì¼ ë°ì´í„° ë°±ì—… ì‹œìŠ¤í…œ\n\"\"\"\nimport os\nimport uuid\nimport csv\nimport json\nimport requests\nfrom io import StringIO\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime, timezone, timedelta\nfrom loguru import logger\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import func\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaInMemoryUpload\nfrom google.oauth2.credentials import Credentials\n\nfrom ..database.models import ProcessedRestaurant, QualityMetrics, BackupHistory, MergeHistory\n\n\nclass DriveBackupManager:\n    \"\"\"\n    Google Drive ë°±ì—… ê´€ë¦¬ì\n    ë§¤ì¼ ìˆ˜ì§‘ëœ ë ˆìŠ¤í† ë‘ ë°ì´í„°ë¥¼ CSVë¡œ ë³€í™˜í•˜ì—¬ Google Driveì— ì €ì¥\n    \"\"\"\n    \n    def __init__(self, db: Session):\n        self.db = db\n        self.root_folder_name = \"hansikdang-data\"\n        self.access_token = None\n        self.drive_service = None\n    \n    def _get_access_token(self) -> str:\n        \"\"\"Replit Google Drive í†µí•©ì—ì„œ access token ê°€ì ¸ì˜¤ê¸°\"\"\"\n        hostname = os.getenv('REPLIT_CONNECTORS_HOSTNAME')\n        x_replit_token = os.getenv('REPL_IDENTITY')\n        \n        if x_replit_token:\n            x_replit_token = f'repl {x_replit_token}'\n        else:\n            x_replit_token = os.getenv('WEB_REPL_RENEWAL')\n            if x_replit_token:\n                x_replit_token = f'depl {x_replit_token}'\n        \n        if not x_replit_token:\n            raise ValueError('X_REPLIT_TOKEN not found for repl/depl')\n        \n        url = f'https://{hostname}/api/v2/connection?include_secrets=true&connector_names=google-drive'\n        headers = {\n            'Accept': 'application/json',\n            'X_REPLIT_TOKEN': x_replit_token\n        }\n        \n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        \n        data = response.json()\n        items = data.get('items', [])\n        \n        if not items:\n            raise ValueError('Google Drive not connected')\n        \n        connection_settings = items[0]\n        access_token = connection_settings.get('settings', {}).get('access_token')\n        \n        if not access_token:\n            oauth = connection_settings.get('settings', {}).get('oauth', {})\n            access_token = oauth.get('credentials', {}).get('access_token')\n        \n        if not access_token:\n            raise ValueError('Access token not found')\n        \n        return access_token\n    \n    def _get_drive_service(self):\n        \"\"\"Google Drive API ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„±\"\"\"\n        if self.drive_service:\n            return self.drive_service\n        \n        access_token = self._get_access_token()\n        \n        credentials = Credentials(token=access_token)\n        self.drive_service = build('drive', 'v3', credentials=credentials)\n        \n        return self.drive_service\n    \n    def _find_or_create_folder(self, folder_name: str, parent_id: Optional[str] = None) -> str:\n        \"\"\"í´ë” ì°¾ê¸° ë˜ëŠ” ìƒì„±\"\"\"\n        service = self._get_drive_service()\n        \n        query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n        if parent_id:\n            query += f\" and '{parent_id}' in parents\"\n        \n        results = service.files().list(\n            q=query,\n            spaces='drive',\n            fields='files(id, name)'\n        ).execute()\n        \n        files = results.get('files', [])\n        \n        if files:\n            return files[0]['id']\n        \n        file_metadata = {\n            'name': folder_name,\n            'mimeType': 'application/vnd.google-apps.folder'\n        }\n        \n        if parent_id:\n            file_metadata['parents'] = [parent_id]\n        \n        folder = service.files().create(\n            body=file_metadata,\n            fields='id'\n        ).execute()\n        \n        logger.info(f\"ğŸ“ Created folder: {folder_name} (ID: {folder['id']})\")\n        \n        return folder['id']\n    \n    def _prepare_folder_structure(self, backup_date: datetime) -> str:\n        \"\"\"í´ë” êµ¬ì¡° ì¤€ë¹„: /hansikdang-data/2025-11/\"\"\"\n        root_folder_id = self._find_or_create_folder(self.root_folder_name)\n        \n        year_month = backup_date.strftime('%Y-%m')\n        month_folder_id = self._find_or_create_folder(year_month, parent_id=root_folder_id)\n        \n        return month_folder_id\n    \n    def _generate_csv_data(self, backup_date: str) -> tuple[str, Dict[str, Any]]:\n        \"\"\"CSV ë°ì´í„° ìƒì„± ë° í†µê³„ ê³„ì‚°\"\"\"\n        backup_day = datetime.strptime(backup_date, '%Y-%m-%d')\n        next_day = backup_day + timedelta(days=1)\n        \n        restaurants = self.db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.created_at >= backup_day,\n            ProcessedRestaurant.created_at < next_day\n        ).all()\n        \n        total_merged = self.db.query(MergeHistory).filter(\n            MergeHistory.merged_at >= backup_day,\n            MergeHistory.merged_at < next_day\n        ).count()\n        \n        quality_metrics = self.db.query(QualityMetrics).filter(\n            QualityMetrics.measured_at >= backup_day,\n            QualityMetrics.measured_at < next_day\n        ).all()\n        \n        avg_quality = 0\n        if quality_metrics:\n            avg_quality = sum(m.overall_quality_score for m in quality_metrics) / len(quality_metrics)\n        \n        output = StringIO()\n        writer = csv.writer(output)\n        \n        writer.writerow([\n            'id', 'name', 'address', 'phone', 'category',\n            'naver_place_id', 'naver_rating', 'naver_reviews',\n            'google_rating', 'google_reviews',\n            'description', 'latitude', 'longitude',\n            'created_at', 'synced_to_hansikdang'\n        ])\n        \n        for r in restaurants:\n            writer.writerow([\n                r.id,\n                r.name,\n                r.address,\n                r.phone or '',\n                r.category or '',\n                r.naver_place_id or '',\n                r.naver_rating or '',\n                r.naver_review_count or '',\n                r.google_rating or '',\n                r.google_review_count or '',\n                r.description or '',\n                r.latitude or '',\n                r.longitude or '',\n                r.created_at.isoformat() if r.created_at else '',\n                r.synced_to_hansikdang or False\n            ])\n        \n        stats = {\n            'total_records': len(restaurants),\n            'new_records': len(restaurants),\n            'duplicate_removed': total_merged,\n            'average_quality_score': round(avg_quality, 2)\n        }\n        \n        return output.getvalue(), stats\n    \n    def _upload_to_drive(\n        self,\n        csv_content: str,\n        file_name: str,\n        folder_id: str\n    ) -> Dict[str, Any]:\n        \"\"\"Google Driveì— CSV íŒŒì¼ ì—…ë¡œë“œ\"\"\"\n        service = self._get_drive_service()\n        \n        file_metadata = {\n            'name': file_name,\n            'parents': [folder_id]\n        }\n        \n        media = MediaInMemoryUpload(\n            csv_content.encode('utf-8'),\n            mimetype='text/csv',\n            resumable=True\n        )\n        \n        file = service.files().create(\n            body=file_metadata,\n            media_body=media,\n            fields='id, name, size, webViewLink'\n        ).execute()\n        \n        logger.info(f\"âœ… Uploaded to Google Drive: {file_name} (ID: {file['id']})\")\n        \n        return {\n            'file_id': file['id'],\n            'file_name': file['name'],\n            'file_size_bytes': int(file.get('size', 0)),\n            'web_view_link': file.get('webViewLink', '')\n        }\n    \n    def backup_daily(\n        self,\n        backup_date: Optional[str] = None,\n        backup_type: str = 'daily',\n        max_retries: int = 3\n    ) -> BackupHistory:\n        \"\"\"\n        ì¼ì¼ ë°±ì—… ì‹¤í–‰\n        \n        Args:\n            backup_date: ë°±ì—… ë‚ ì§œ (YYYY-MM-DD, Noneì´ë©´ ì–´ì œ)\n            backup_type: ë°±ì—… ìœ í˜• ('daily', 'weekly', 'manual')\n            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜\n            \n        Returns:\n            BackupHistory: ë°±ì—… ì´ë ¥ ë ˆì½”ë“œ\n        \"\"\"\n        if not backup_date:\n            yesterday = datetime.now(timezone.utc) - timedelta(days=1)\n            backup_date = yesterday.strftime('%Y-%m-%d')\n        \n        backup_id = str(uuid.uuid4())\n        started_at = datetime.now(timezone.utc)\n        \n        logger.info(f\"ğŸ”„ Starting {backup_type} backup for {backup_date}\")\n        \n        backup_history = BackupHistory(\n            id=backup_id,\n            backup_date=backup_date,\n            backup_type=backup_type,\n            file_name='',\n            status='started',\n            started_at=started_at,\n            retry_count=0\n        )\n        \n        self.db.add(backup_history)\n        self.db.commit()\n        \n        retry_count = 0\n        last_error = None\n        \n        while retry_count < max_retries:\n            try:\n                csv_content, stats = self._generate_csv_data(backup_date)\n                \n                backup_datetime = datetime.strptime(backup_date, '%Y-%m-%d')\n                folder_id = self._prepare_folder_structure(backup_datetime)\n                \n                day = backup_datetime.strftime('%d')\n                file_name = f\"{day}-collection.csv\"\n                \n                upload_result = self._upload_to_drive(csv_content, file_name, folder_id)\n                \n                completed_at = datetime.now(timezone.utc)\n                execution_time = int((completed_at - started_at).total_seconds())\n                \n                month = backup_datetime.strftime('%Y-%m')\n                file_path = f\"/{self.root_folder_name}/{month}/{file_name}\"\n                \n                backup_history.file_name = file_name\n                backup_history.file_path = file_path\n                backup_history.drive_file_id = upload_result['file_id']\n                backup_history.file_size_bytes = upload_result['file_size_bytes']\n                backup_history.total_records = stats['total_records']\n                backup_history.new_records = stats['new_records']\n                backup_history.duplicate_removed = stats['duplicate_removed']\n                backup_history.average_quality_score = stats['average_quality_score']\n                backup_history.status = 'success'\n                backup_history.completed_at = completed_at\n                backup_history.execution_time_seconds = execution_time\n                backup_history.retry_count = retry_count\n                \n                self.db.commit()\n                \n                logger.info(f\"âœ… Backup completed successfully: {file_path}\")\n                logger.info(f\"   Records: {stats['total_records']}, Duplicates removed: {stats['duplicate_removed']}\")\n                logger.info(f\"   Avg quality: {stats['average_quality_score']}, Time: {execution_time}s\")\n                \n                return backup_history\n                \n            except Exception as e:\n                retry_count += 1\n                last_error = str(e)\n                logger.error(f\"âŒ Backup attempt {retry_count} failed: {e}\")\n                \n                if retry_count < max_retries:\n                    logger.info(f\"   Retrying in 5 seconds...\")\n                    import time\n                    time.sleep(5)\n        \n        completed_at = datetime.now(timezone.utc)\n        execution_time = int((completed_at - started_at).total_seconds())\n        \n        backup_history.status = 'failed'\n        backup_history.error_message = last_error\n        backup_history.retry_count = retry_count\n        backup_history.completed_at = completed_at\n        backup_history.execution_time_seconds = execution_time\n        \n        self.db.commit()\n        \n        logger.error(f\"âŒ Backup failed after {retry_count} attempts: {last_error}\")\n        \n        return backup_history\n    \n    def get_backup_status(self) -> Dict[str, Any]:\n        \"\"\"ìµœê·¼ ë°±ì—… ìƒíƒœ ì¡°íšŒ\"\"\"\n        latest = self.db.query(BackupHistory).order_by(\n            BackupHistory.completed_at.desc()\n        ).first()\n        \n        if not latest:\n            return {\n                'status': 'no_backups',\n                'message': 'ë°±ì—… ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤'\n            }\n        \n        return {\n            'status': latest.status,\n            'backup_date': latest.backup_date,\n            'backup_type': latest.backup_type,\n            'file_path': latest.file_path,\n            'total_records': latest.total_records,\n            'file_size_mb': round(latest.file_size_bytes / 1024 / 1024, 2) if latest.file_size_bytes else 0,\n            'completed_at': latest.completed_at.isoformat() if latest.completed_at else None,\n            'execution_time_seconds': latest.execution_time_seconds,\n            'error_message': latest.error_message if latest.status == 'failed' else None\n        }\n    \n    def get_backup_history(self, days: int = 7) -> List[Dict[str, Any]]:\n        \"\"\"ìµœê·¼ ë°±ì—… ì´ë ¥ ì¡°íšŒ\"\"\"\n        cutoff = datetime.now(timezone.utc) - timedelta(days=days)\n        \n        backups = self.db.query(BackupHistory).filter(\n            BackupHistory.completed_at >= cutoff\n        ).order_by(BackupHistory.completed_at.desc()).all()\n        \n        return [\n            {\n                'backup_date': b.backup_date,\n                'backup_type': b.backup_type,\n                'status': b.status,\n                'file_path': b.file_path,\n                'total_records': b.total_records,\n                'file_size_mb': round(b.file_size_bytes / 1024 / 1024, 2) if b.file_size_bytes else 0,\n                'completed_at': b.completed_at.isoformat() if b.completed_at else None\n            }\n            for b in backups\n        ]\n","size_bytes":13891},"data-hub/src/deduplication/__init__.py":{"content":"","size_bytes":0},"data-hub/src/deduplication/service.py":{"content":"from typing import List, Dict, Optional\nfrom sqlalchemy.orm import Session\nfrom loguru import logger\n\nfrom src.database.models import (\n    ProcessedRestaurant,\n    DuplicateGroup,\n    MergeHistory\n)\nfrom src.deduplication.detector import DuplicateDetector\nfrom src.deduplication.merger import MergeManager\n\n\nclass DeduplicationService:\n    def __init__(\n        self,\n        db: Session,\n        name_threshold: float = 90.0,\n        address_threshold: float = 85.0,\n        distance_threshold_meters: float = 100.0\n    ):\n        self.db = db\n        self.detector = DuplicateDetector(\n            name_threshold=name_threshold,\n            address_threshold=address_threshold,\n            distance_threshold_meters=distance_threshold_meters\n        )\n        self.merger = MergeManager(db)\n    \n    def detect_and_merge_duplicates(\n        self,\n        auto_merge: bool = False,\n        merge_type: str = 'auto'\n    ) -> Dict:\n        logger.info(\"=\" * 70)\n        logger.info(\"ğŸ” ì¤‘ë³µ íƒì§€ ë° ë³‘í•© í”„ë¡œì„¸ìŠ¤ ì‹œì‘\")\n        logger.info(\"=\" * 70)\n        \n        restaurants = self.db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.name.isnot(None)\n        ).all()\n        \n        if not restaurants:\n            logger.warning(\"âš ï¸  ë ˆìŠ¤í† ë‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤\")\n            return {\n                'total_restaurants': 0,\n                'duplicate_groups_found': 0,\n                'merged_groups': 0,\n                'total_merged_restaurants': 0\n            }\n        \n        logger.info(f\"ğŸ“Š ëŒ€ìƒ ë ˆìŠ¤í† ë‘: {len(restaurants)}ê°œ\")\n        \n        duplicate_groups = self.detector.detect_duplicates(restaurants)\n        \n        logger.info(f\"ğŸ” ë°œê²¬ëœ ì¤‘ë³µ ê·¸ë£¹: {len(duplicate_groups)}ê°œ\")\n        \n        merged_count = 0\n        total_merged_restaurants = 0\n        \n        if auto_merge and duplicate_groups:\n            logger.info(\"ğŸ”€ ìë™ ë³‘í•© ì‹œì‘...\")\n            \n            for group in duplicate_groups:\n                group_id = self.merger.merge_duplicates(\n                    group,\n                    merge_type=merge_type,\n                    merged_by='system'\n                )\n                \n                if group_id:\n                    merged_count += 1\n                    total_merged_restaurants += len(group['duplicates'])\n            \n            logger.info(f\"âœ… ìë™ ë³‘í•© ì™„ë£Œ: {merged_count}ê°œ ê·¸ë£¹, {total_merged_restaurants}ê°œ ë ˆìŠ¤í† ë‘\")\n        else:\n            logger.info(\"â„¹ï¸  ìë™ ë³‘í•© ë¹„í™œì„±í™” - íƒì§€ë§Œ ìˆ˜í–‰\")\n        \n        result = {\n            'total_restaurants': len(restaurants),\n            'duplicate_groups_found': len(duplicate_groups),\n            'merged_groups': merged_count,\n            'total_merged_restaurants': total_merged_restaurants,\n            'duplicate_groups': duplicate_groups if not auto_merge else []\n        }\n        \n        logger.info(\"=\" * 70)\n        logger.info(\"âœ… ì¤‘ë³µ íƒì§€ ë° ë³‘í•© í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ\")\n        logger.info(f\"   ì´ ë ˆìŠ¤í† ë‘: {result['total_restaurants']}ê°œ\")\n        logger.info(f\"   ì¤‘ë³µ ê·¸ë£¹: {result['duplicate_groups_found']}ê°œ\")\n        logger.info(f\"   ë³‘í•©ëœ ê·¸ë£¹: {result['merged_groups']}ê°œ\")\n        logger.info(f\"   ë³‘í•©ëœ ë ˆìŠ¤í† ë‘: {result['total_merged_restaurants']}ê°œ\")\n        logger.info(\"=\" * 70)\n        \n        return result\n    \n    def get_duplicate_groups(\n        self,\n        status: Optional[str] = None,\n        limit: int = 100\n    ) -> List[DuplicateGroup]:\n        query = self.db.query(DuplicateGroup)\n        \n        if status:\n            query = query.filter(DuplicateGroup.status == status)\n        \n        return query.order_by(DuplicateGroup.created_at.desc()).limit(limit).all()\n    \n    def get_merge_history(\n        self,\n        limit: int = 100,\n        merge_type: Optional[str] = None\n    ) -> List[MergeHistory]:\n        query = self.db.query(MergeHistory)\n        \n        if merge_type:\n            query = query.filter(MergeHistory.merge_type == merge_type)\n        \n        return query.order_by(MergeHistory.merged_at.desc()).limit(limit).all()\n    \n    def get_deduplication_stats(self) -> Dict:\n        total_groups = self.db.query(DuplicateGroup).count()\n        merged_groups = self.db.query(DuplicateGroup).filter(\n            DuplicateGroup.status == 'merged'\n        ).count()\n        pending_groups = self.db.query(DuplicateGroup).filter(\n            DuplicateGroup.status == 'detected'\n        ).count()\n        \n        total_merges = self.db.query(MergeHistory).count()\n        auto_merges = self.db.query(MergeHistory).filter(\n            MergeHistory.merge_type == 'auto'\n        ).count()\n        manual_merges = self.db.query(MergeHistory).filter(\n            MergeHistory.merge_type == 'manual'\n        ).count()\n        \n        total_merged_restaurants = 0\n        merge_histories = self.db.query(MergeHistory).all()\n        for history in merge_histories:\n            if history.merged_ids:\n                total_merged_restaurants += len(history.merged_ids)\n        \n        return {\n            'duplicate_groups': {\n                'total': total_groups,\n                'merged': merged_groups,\n                'pending': pending_groups\n            },\n            'merge_history': {\n                'total_merges': total_merges,\n                'auto_merges': auto_merges,\n                'manual_merges': manual_merges,\n                'total_merged_restaurants': total_merged_restaurants\n            }\n        }\n","size_bytes":5555},"data-hub/src/api/sync_routes.py":{"content":"\"\"\"\nSync API Routes\ní•œì‹ë‹¹ í”Œë«í¼ê³¼ì˜ ë™ê¸°í™” ê´€ë ¨ API\n\"\"\"\nimport asyncio\nfrom typing import Optional\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm import Session\nfrom datetime import datetime, timedelta\n\nfrom src.database.connection import get_db\nfrom src.database.models import SyncLog, ProcessedRestaurant\nfrom src.workflows.sync import SyncWorkflow\n\nrouter = APIRouter(prefix=\"/api/sync\", tags=[\"Sync\"])\n\n\n@router.post(\"/start\")\nasync def start_sync(db: Session = Depends(get_db)):\n    \"\"\"ìˆ˜ë™ìœ¼ë¡œ ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\"\"\"\n    try:\n        pending_count = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.sync_status == 'pending'\n        ).count()\n        \n        if pending_count == 0:\n            return {\n                \"status\": \"success\",\n                \"message\": \"ë™ê¸°í™”í•  ë ˆìŠ¤í† ë‘ì´ ì—†ìŠµë‹ˆë‹¤\",\n                \"pending_count\": 0\n            }\n        \n        workflow = SyncWorkflow()\n        await workflow.sync_to_hansikdang()\n        \n        latest_log = db.query(SyncLog).order_by(\n            SyncLog.started_at.desc()\n        ).first()\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"ë™ê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"log\": {\n                \"id\": latest_log.id,\n                \"total_sent\": latest_log.total_sent,\n                \"success_count\": latest_log.success_count,\n                \"error_count\": latest_log.error_count,\n                \"status\": latest_log.status\n            } if latest_log else None\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/logs\")\ndef get_sync_logs(\n    limit: int = 10,\n    status: Optional[str] = None,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ë™ê¸°í™” ë¡œê·¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    query = db.query(SyncLog).order_by(SyncLog.started_at.desc())\n    \n    if status:\n        query = query.filter(SyncLog.status == status)\n    \n    logs = query.limit(limit).all()\n    \n    return {\n        \"status\": \"success\",\n        \"total\": len(logs),\n        \"logs\": [\n            {\n                \"id\": log.id,\n                \"status\": log.status,\n                \"total_sent\": log.total_sent,\n                \"success_count\": log.success_count,\n                \"error_count\": log.error_count,\n                \"started_at\": log.started_at.isoformat() if log.started_at else None,\n                \"completed_at\": log.completed_at.isoformat() if log.completed_at else None,\n                \"error_details\": log.error_details\n            }\n            for log in logs\n        ]\n    }\n\n\n@router.get(\"/stats\")\ndef get_sync_stats(db: Session = Depends(get_db)):\n    \"\"\"ë™ê¸°í™” í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    total_restaurants = db.query(ProcessedRestaurant).count()\n    synced = db.query(ProcessedRestaurant).filter(\n        ProcessedRestaurant.sync_status == 'synced'\n    ).count()\n    pending = db.query(ProcessedRestaurant).filter(\n        ProcessedRestaurant.sync_status == 'pending'\n    ).count()\n    \n    last_24h = datetime.now() - timedelta(hours=24)\n    recent_logs = db.query(SyncLog).filter(\n        SyncLog.started_at >= last_24h\n    ).all()\n    \n    total_synced_24h = sum(log.success_count or 0 for log in recent_logs)\n    total_failed_24h = sum(log.error_count or 0 for log in recent_logs)\n    \n    latest_log = db.query(SyncLog).order_by(\n        SyncLog.started_at.desc()\n    ).first()\n    \n    return {\n        \"status\": \"success\",\n        \"overview\": {\n            \"total_restaurants\": total_restaurants,\n            \"synced\": synced,\n            \"pending\": pending,\n            \"sync_rate\": round((synced / total_restaurants * 100) if total_restaurants > 0 else 0, 2)\n        },\n        \"last_24h\": {\n            \"total_synced\": total_synced_24h,\n            \"total_failed\": total_failed_24h,\n            \"sync_runs\": len(recent_logs)\n        },\n        \"latest_sync\": {\n            \"id\": latest_log.id,\n            \"status\": latest_log.status,\n            \"total_sent\": latest_log.total_sent,\n            \"success_count\": latest_log.success_count,\n            \"error_count\": latest_log.error_count,\n            \"started_at\": latest_log.started_at.isoformat() if latest_log.started_at else None,\n            \"completed_at\": latest_log.completed_at.isoformat() if latest_log.completed_at else None\n        } if latest_log else None\n    }\n\n\n@router.get(\"/pending\")\ndef get_pending_restaurants(\n    limit: int = 20,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ë™ê¸°í™” ëŒ€ê¸° ì¤‘ì¸ ë ˆìŠ¤í† ë‘ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    restaurants = db.query(ProcessedRestaurant).filter(\n        ProcessedRestaurant.sync_status == 'pending'\n    ).limit(limit).all()\n    \n    return {\n        \"status\": \"success\",\n        \"total\": db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.sync_status == 'pending'\n        ).count(),\n        \"showing\": len(restaurants),\n        \"restaurants\": [\n            {\n                \"id\": r.id,\n                \"name\": r.name,\n                \"district\": r.district,\n                \"quality_score\": r.quality_score,\n                \"created_at\": r.created_at.isoformat() if r.created_at else None\n            }\n            for r in restaurants\n        ]\n    }\n","size_bytes":5307},"attached_assets/dashboard_upgrade_action_plan_1762581009870.md":{"content":"# Data Hub ì–´ë“œë¯¼ ì¦‰ì‹œ ì—…ê·¸ë ˆì´ë“œ ì•¡ì…˜ í”Œëœ\n\n**ê¸´ê¸‰ë„**: ğŸ”´ **ì¦‰ì‹œ í•„ìš”**  \n**í˜„ì¬ ìƒíƒœ**: 40% ì™„ì„± (ëª¨ë‹ˆí„°ë§ë§Œ)  \n**ëª©í‘œ ìƒíƒœ**: 100% (ëª¨ë‹ˆí„°ë§ + ê´€ë¦¬ + ì‹¤í–‰)  \n**ì˜ˆìƒ ì†Œìš”**: 10-13ì‹œê°„\n\n---\n\n## ğŸ¯ í•µì‹¬ ë¬¸ì œ\n\n### ë‹¹ì‹ ì´ ì›í•œ ê²ƒ\n\n```\n\"ì´ data-hub ì‹œìŠ¤í…œì„ í†µí•´ \nì§ì ‘ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì¸, ê²€ì¦, ìˆ˜ì •í•  ìˆ˜ ìˆìœ¼ë©°, \nì–´ì œ ë„ˆì™€ í•¨ê»˜ ë§Œë“  ê¸°ëŠ¥ë“¤ì„ ì§ì ‘ ì‹¤í–‰ë„ ì‹œí‚¤ê³  ê´€ë¦¬í–ˆìœ¼ë©´ ì¢‹ê² ì–´.\"\n```\n\n### í˜„ì¬ êµ¬í˜„\n\n```\nâœ… ë°ì´í„° ë¦¬ìŠ¤íŠ¸ í™•ì¸ - âŒ ì—†ìŒ\nâœ… ë°ì´í„° ê²€ì¦ - âŒ ì—†ìŒ\nâœ… ë°ì´í„° ìˆ˜ì • - âŒ ì—†ìŒ\nâœ… ê¸°ëŠ¥ ì‹¤í–‰ - âš ï¸ ë°±ì—…ë§Œ ê°€ëŠ¥ (1/7)\nâœ… ê´€ë¦¬ ê¸°ëŠ¥ - âŒ ì—†ìŒ\n```\n\n**ê²°ë¡ **: ë‹¹ì‹ ì´ ì›í•œ ê¸°ëŠ¥ì˜ **10%ë§Œ êµ¬í˜„ë¨**\n\n---\n\n## ğŸ“‹ ì¦‰ì‹œ ì¶”ê°€í•´ì•¼ í•  3ê°€ì§€\n\n### 1ï¸âƒ£ ë°ì´í„° ê´€ë¦¬ í˜ì´ì§€ (í•„ìˆ˜)\n\n**ë¬´ì—‡ì„ ì¶”ê°€**:\n```\nìƒˆ í˜ì´ì§€: /dashboard/data\n\në ˆìŠ¤í† ë‘ ë°ì´í„° í…Œì´ë¸”:\n- ì „ì²´ 59ê°œ ë ˆì½”ë“œ í‘œì‹œ\n- ê²€ìƒ‰, í•„í„°, ì •ë ¬ ê¸°ëŠ¥\n- [ë³´ê¸°] [ìˆ˜ì •] [ì‚­ì œ] ë²„íŠ¼\n- í˜ì´ì§€ë„¤ì´ì…˜ (10ê°œì”©)\n\nìƒì„¸ ëª¨ë‹¬:\n- ëª¨ë“  í•„ë“œ í¸ì§‘ ê°€ëŠ¥\n- [ì €ì¥] [ì·¨ì†Œ] [ì‚­ì œ]\n```\n\n**ì†Œìš” ì‹œê°„**: 4-6ì‹œê°„\n\n---\n\n### 2ï¸âƒ£ ì‹¤í–‰ ì œì–´ ì„¼í„° (í•„ìˆ˜)\n\n**ë¬´ì—‡ì„ ì¶”ê°€**:\n```\nëŒ€ì‹œë³´ë“œ í•˜ë‹¨ì— ì¶”ê°€:\n\nì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—…:\n[ğŸ¯ Smart Targeting ì‹¤í–‰]\n[ğŸ” Naver ìŠ¤í¬ë˜í•‘ ì‹¤í–‰]\n[ğŸ”„ ì¤‘ë³µ íƒì§€ ì‹¤í–‰]\n[ğŸ¤– Gemini ì •ì œ ì‹¤í–‰]\n[ğŸ“ Google Places ë³´ê°• ì‹¤í–‰]\n[ğŸ”„ ë©”ì¸ í”Œë«í¼ ë™ê¸°í™” ì‹¤í–‰]\n[ğŸ’¾ Google Drive ë°±ì—… ì‹¤í–‰] (ê¸°ì¡´)\n\nìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´:\n[â¸ï¸ ì¼ì‹œì •ì§€] [â–¶ï¸ ì¬ê°œ] [ğŸ”„ ì¬ì‹œì‘]\n```\n\n**ì†Œìš” ì‹œê°„**: 3-4ì‹œê°„\n\n---\n\n### 3ï¸âƒ£ ë°ì´í„° ê²€ì¦ ë„êµ¬ (ì„ íƒ)\n\n**ë¬´ì—‡ì„ ì¶”ê°€**:\n```\nìƒˆ íƒ­: /dashboard/validation\n\nìë™ ê²€ì¦:\n- [ì „ì²´ ë°ì´í„° ê²€ì¦ ì‹¤í–‰]\n- ë¬¸ì œ í•­ëª© ëª©ë¡\n- [ë³‘í•©] [ìˆ˜ì •] ë²„íŠ¼\n\nì¼ê´„ ì‘ì—…:\n- [ëª¨ë“  ì¤‘ë³µ ìë™ ë³‘í•©]\n- [í’ˆì§ˆ ì ìˆ˜ ì¬ê³„ì‚°]\n```\n\n**ì†Œìš” ì‹œê°„**: 2-3ì‹œê°„\n\n---\n\n## ğŸš€ Replitì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸\n\n### Phase 1: ë°ì´í„° ê´€ë¦¬ í˜ì´ì§€\n\n```\nData Hub ëŒ€ì‹œë³´ë“œì— \"ë°ì´í„° ê´€ë¦¬\" í˜ì´ì§€ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.\n\n**ëª©í‘œ:**\nìš´ì˜ìê°€ ë ˆìŠ¤í† ë‘ ë°ì´í„°ë¥¼ ì§ì ‘ í™•ì¸, ê²€ìƒ‰, ìˆ˜ì •, ì‚­ì œí•  ìˆ˜ ìˆë„ë¡.\n\n**ìƒˆ í˜ì´ì§€: /dashboard/data**\n\ní•„ìˆ˜ ê¸°ëŠ¥:\n1. ë°ì´í„° í…Œì´ë¸”\n   - ì „ì²´ ë ˆìŠ¤í† ë‘ í‘œì‹œ (processed_restaurants í…Œì´ë¸”)\n   - ì»¬ëŸ¼: ID, ì´ë¦„, ì£¼ì†Œ, ì „í™”, í‰ì , ë¦¬ë·°ìˆ˜, ìƒíƒœ\n   - ë°˜ì‘í˜• ë””ìì¸\n\n2. ê²€ìƒ‰ & í•„í„°\n   - ê²€ìƒ‰: ì´ë¦„, ì£¼ì†Œ ê²€ìƒ‰\n   - í•„í„°: ì§€ì—­ë³„, í‰ì ë³„, ìƒíƒœë³„\n   - ì •ë ¬: ìµœì‹ ìˆœ, í‰ì ìˆœ, ì´ë¦„ìˆœ\n\n3. í˜ì´ì§€ë„¤ì´ì…˜\n   - 10ê°œì”© í‘œì‹œ\n   - [ì´ì „] [ë‹¤ìŒ] ë²„íŠ¼\n\n4. ì•¡ì…˜ ë²„íŠ¼ (ê° í–‰ë§ˆë‹¤)\n   - [ë³´ê¸°]: ìƒì„¸ ì •ë³´ ëª¨ë‹¬\n   - [ìˆ˜ì •]: í¸ì§‘ ëª¨ë‹¬\n   - [ì‚­ì œ]: í™•ì¸ í›„ ì‚­ì œ\n\n5. ìƒì„¸/í¸ì§‘ ëª¨ë‹¬\n   - ëª¨ë“  í•„ë“œ í‘œì‹œ ë° í¸ì§‘\n   - [ì €ì¥] [ì·¨ì†Œ] [ì‚­ì œ]\n\n**ìƒˆ API ì—”ë“œí¬ì¸íŠ¸:**\n- GET /api/restaurants?page=1&limit=10&search=&filter=&sort=\n- GET /api/restaurants/{id}\n- PUT /api/restaurants/{id}\n- DELETE /api/restaurants/{id}\n\n**ê¸°ìˆ :**\n- Vue.js (ê¸°ì¡´ ëŒ€ì‹œë³´ë“œ í™•ì¥)\n- FastAPI (ìƒˆ ì—”ë“œí¬ì¸íŠ¸)\n- ëª¨ë‹¬, í™•ì¸ ë‹¤ì´ì–¼ë¡œê·¸\n\n**ë‹¹ì‹ ì˜ íŒë‹¨:**\në” ë‚˜ì€ ë°©ì‹ì´ë‚˜ ì¶”ê°€ ê¸°ëŠ¥ì´ ìˆë‚˜ìš”?\n\nì§„í–‰ ë¶€íƒí•©ë‹ˆë‹¤.\n```\n\n### Phase 2: ì‹¤í–‰ ì œì–´ ì„¼í„°\n\n```\nëŒ€ì‹œë³´ë“œì— \"ì‹¤í–‰ ì œì–´ ì„¼í„°\"ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.\n\n**ëª©í‘œ:**\nëª¨ë“  ìë™í™” ì‘ì—…ì„ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡.\n\n**ì¶”ê°€ ìœ„ì¹˜:**\nê¸°ì¡´ ëŒ€ì‹œë³´ë“œ í•˜ë‹¨ (7ì¼ ì¶”ì´ ê·¸ë˜í”„ ì•„ë˜)\n\n**ë²„íŠ¼ ëª©ë¡:**\n1. [Smart Targeting ì‹¤í–‰]\n2. [Naver ìŠ¤í¬ë˜í•‘ ì‹¤í–‰]\n3. [ì¤‘ë³µ íƒì§€ ì‹¤í–‰]\n4. [Gemini ì •ì œ ì‹¤í–‰]\n5. [Google Places ë³´ê°• ì‹¤í–‰]\n6. [ë©”ì¸ í”Œë«í¼ ë™ê¸°í™” ì‹¤í–‰]\n7. [Google Drive ë°±ì—… ì‹¤í–‰] (ê¸°ì¡´ ë²„íŠ¼ ì´ë™)\n\nê° ë²„íŠ¼ í‘œì‹œ:\n- ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„\n- í˜„ì¬ ìƒíƒœ (ì™„ë£Œ/ì§„í–‰ì¤‘/ì‹¤íŒ¨)\n- ê²°ê³¼ ìš”ì•½\n\nìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´:\n- [â¸ï¸ ì¼ì‹œì •ì§€] [â–¶ï¸ ì¬ê°œ] [ğŸ”„ ì¬ì‹œì‘]\n\n**ìƒˆ API ì—”ë“œí¬ì¸íŠ¸:**\n- POST /api/jobs/targeting/run\n- POST /api/jobs/scraping/run\n- POST /api/jobs/deduplication/run\n- POST /api/jobs/gemini/run\n- POST /api/jobs/places/run\n- POST /api/jobs/sync/run\n- POST /api/scheduler/pause\n- POST /api/scheduler/resume\n- POST /api/scheduler/restart\n\n**UX:**\n- í´ë¦­ ì‹œ í™•ì¸ ë‹¤ì´ì–¼ë¡œê·¸\n- ì‹¤í–‰ ì¤‘ ë¡œë”© í‘œì‹œ\n- ì™„ë£Œ ì‹œ ê²°ê³¼ ì•Œë¦¼\n\nì§„í–‰ ë¶€íƒí•©ë‹ˆë‹¤.\n```\n\n---\n\n## âœ… ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n### ì˜¤ëŠ˜ (11ì›” 8ì¼)\n\n```\n[ ] ì´ ì•¡ì…˜ í”Œëœ ê²€í† \n[ ] Phase 1 í”„ë¡¬í”„íŠ¸ Replitì—ê²Œ ì „ë‹¬\n[ ] ê°œë°œ ì§„í–‰ ëª¨ë‹ˆí„°ë§ (4-6ì‹œê°„)\n[ ] ì €ë…: ë°ì´í„° ê´€ë¦¬ í˜ì´ì§€ í…ŒìŠ¤íŠ¸\n```\n\n### ë‚´ì¼ (11ì›” 9ì¼)\n\n```\n[ ] Phase 2 í”„ë¡¬í”„íŠ¸ Replitì—ê²Œ ì „ë‹¬\n[ ] ê°œë°œ ì§„í–‰ ëª¨ë‹ˆí„°ë§ (3-4ì‹œê°„)\n[ ] ì €ë…: ì‹¤í–‰ ì œì–´ ì„¼í„° í…ŒìŠ¤íŠ¸\n```\n\n### 11ì›” 10ì¼\n\n```\n[ ] Phase 3 (ì„ íƒ) í”„ë¡¬í”„íŠ¸ ì „ë‹¬\n[ ] ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸\n[ ] ì™„ì„±ëœ ì–´ë“œë¯¼ ëŒ€ì‹œë³´ë“œ í™•ì¸\n```\n\n---\n\n## ğŸ’¡ ì™„ì„± í›„ ë‹¹ì‹ ì´ í•  ìˆ˜ ìˆëŠ” ê²ƒ\n\n### Before (í˜„ì¬)\n\n```\n08:00 - ëŒ€ì‹œë³´ë“œ ì—´ê¸°\n        â†’ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ë§Œ ê°€ëŠ¥\n        â†’ ë°ì´í„° ë³´ê¸° ë¶ˆê°€\n        â†’ ìˆ˜ì • ë¶ˆê°€\n        â†’ ì‹¤í–‰ ë¶ˆê°€\n        \ní™œìš©ë„: 20% (ê±°ì˜ ì“¸ëª¨ì—†ìŒ)\n```\n\n### After (ì™„ì„± í›„)\n\n```\n08:00 - ëŒ€ì‹œë³´ë“œ ì—´ê¸°\n        â†’ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ âœ…\n        â†’ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ í™•ì¸ âœ…\n        â†’ ê²€ìƒ‰ ë° í•„í„°ë§ âœ…\n        â†’ ë¬¸ì œ ìˆëŠ” ë°ì´í„° ìˆ˜ì • âœ…\n        â†’ ê¸´ê¸‰ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ âœ…\n        â†’ ì¤‘ë³µ íƒì§€ ì¦‰ì‹œ ì‹¤í–‰ âœ…\n        â†’ ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´ âœ…\n        \ní™œìš©ë„: 100% (ì™„ë²½í•œ ê´€ë¦¬ ë„êµ¬)\n```\n\n---\n\n## ğŸ“Š íˆ¬ì vs íš¨ê³¼\n\n### íˆ¬ì\n\n```\nê°œë°œ ì‹œê°„: 10-13ì‹œê°„\në¹„ìš©: $0 (ê¸°ì¡´ ì¸í”„ë¼)\nì¼ì •: 3ì¼\n```\n\n### íš¨ê³¼\n\n```\në°ì´í„° ê´€ë¦¬: ìˆ˜ë™ â†’ ì§ì ‘ ê°€ëŠ¥\nì‹¤í–‰ ì œì–´: ë¶ˆê°€ëŠ¥ â†’ ëª¨ë“  ê²ƒ ê°€ëŠ¥\nê²€ì¦ ì‘ì—…: ë¶ˆê°€ëŠ¥ â†’ ìë™í™”\nìš´ì˜ íš¨ìœ¨: +300%\nì˜ì‚¬ê²°ì •: ì¦‰ì‹œ ê°€ëŠ¥\n```\n\n**ROI**: ê·¹ë„ë¡œ ë†’ìŒ\n\n---\n\n## ğŸ”¥ ìµœì¢… ê¶Œì¥ì‚¬í•­\n\n### ì¦‰ì‹œ ì‹¤í–‰\n\n```\n1. Phase 1 (ë°ì´í„° ê´€ë¦¬) ì˜¤ëŠ˜ ì‹œì‘\n2. Phase 2 (ì‹¤í–‰ ì œì–´) ë‚´ì¼ ì‹œì‘\n3. Phase 3 (ê²€ì¦ ë„êµ¬) ì„ íƒì‚¬í•­\n```\n\n### ì™„ì„± í›„\n\n```\nì™„ë²½í•œ Data Hub ì–´ë“œë¯¼:\n- ëª¨ë‹ˆí„°ë§ âœ…\n- ë°ì´í„° ê´€ë¦¬ âœ…\n- ì‹¤í–‰ ì œì–´ âœ…\n- ê²€ì¦ ë„êµ¬ âœ…\n\në‹¹ì‹ ì˜ í™œìš©ë„: 100%\nì‹¤ìš©ì„±: ì™„ë²½\n```\n\n---\n\n**ì§€ê¸ˆ ë°”ë¡œ Replitì—ê²Œ Phase 1 í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ì„¸ìš”!** ğŸš€\n\n","size_bytes":6528},"data-hub/src/api/restaurant_routes.py":{"content":"from fastapi import APIRouter, Depends, HTTPException, Query\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import or_, desc, asc, func\nfrom typing import Optional, List, Dict, Any\nfrom datetime import datetime, timezone\n\nfrom src.database.connection import get_db\nfrom src.database.models import ProcessedRestaurant, RawRestaurantData\nfrom pydantic import BaseModel\n\nrouter = APIRouter(prefix=\"/api/restaurants\", tags=[\"restaurants\"])\n\n\nclass RestaurantUpdate(BaseModel):\n    name: Optional[str] = None\n    name_en: Optional[str] = None\n    category: Optional[str] = None\n    address: Optional[str] = None\n    district: Optional[str] = None\n    phone: Optional[str] = None\n    description: Optional[str] = None\n    description_en: Optional[str] = None\n    menu_summary: Optional[str] = None\n    open_hours: Optional[str] = None\n    price_range: Optional[int] = None\n    google_rating: Optional[float] = None\n    google_review_count: Optional[int] = None\n    image_urls: Optional[List[str]] = None\n    latitude: Optional[float] = None\n    longitude: Optional[float] = None\n    quality_score: Optional[float] = None\n\n\n@router.get(\"\")\ndef get_restaurants(\n    page: int = Query(1, ge=1, description=\"í˜ì´ì§€ ë²ˆí˜¸\"),\n    limit: int = Query(10, ge=1, le=100, description=\"í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜\"),\n    search: Optional[str] = Query(None, description=\"ê²€ìƒ‰ì–´ (ì´ë¦„, ì£¼ì†Œ)\"),\n    district: Optional[str] = Query(None, description=\"ì§€ì—­ í•„í„°\"),\n    min_rating: Optional[float] = Query(None, ge=0, le=5, description=\"ìµœì†Œ í‰ì \"),\n    status: Optional[str] = Query(None, description=\"ìƒíƒœ í•„í„° (synced, pending)\"),\n    sort_by: str = Query(\"created_at\", description=\"ì •ë ¬ ê¸°ì¤€ (created_at, name, rating, quality_score)\"),\n    sort_order: str = Query(\"desc\", description=\"ì •ë ¬ ìˆœì„œ (asc, desc)\"),\n    db: Session = Depends(get_db)\n) -> Dict[str, Any]:\n    \"\"\"\n    ë ˆìŠ¤í† ë‘ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ (ê²€ìƒ‰, í•„í„°, ì •ë ¬, í˜ì´ì§€ë„¤ì´ì…˜)\n    \"\"\"\n    query = db.query(ProcessedRestaurant)\n    \n    # ê²€ìƒ‰ í•„í„°\n    if search:\n        search_pattern = f\"%{search}%\"\n        query = query.filter(\n            or_(\n                ProcessedRestaurant.name.ilike(search_pattern),\n                ProcessedRestaurant.address.ilike(search_pattern),\n                ProcessedRestaurant.district.ilike(search_pattern)\n            )\n        )\n    \n    # ì§€ì—­ í•„í„°\n    if district:\n        query = query.filter(ProcessedRestaurant.district == district)\n    \n    # í‰ì  í•„í„°\n    if min_rating is not None:\n        query = query.filter(ProcessedRestaurant.google_rating >= min_rating)\n    \n    # ìƒíƒœ í•„í„°\n    if status:\n        if status == \"synced\":\n            query = query.filter(ProcessedRestaurant.synced_to_hansikdang == True)\n        elif status == \"pending\":\n            query = query.filter(ProcessedRestaurant.synced_to_hansikdang == False)\n    \n    # ì •ë ¬\n    sort_column = {\n        \"created_at\": ProcessedRestaurant.created_at,\n        \"name\": ProcessedRestaurant.name,\n        \"rating\": ProcessedRestaurant.google_rating,\n        \"quality_score\": ProcessedRestaurant.quality_score\n    }.get(sort_by, ProcessedRestaurant.created_at)\n    \n    if sort_order == \"desc\":\n        query = query.order_by(desc(sort_column))\n    else:\n        query = query.order_by(asc(sort_column))\n    \n    # ì „ì²´ ê°œìˆ˜\n    total_count = query.count()\n    \n    # í˜ì´ì§€ë„¤ì´ì…˜\n    offset = (page - 1) * limit\n    restaurants = query.offset(offset).limit(limit).all()\n    \n    # ì‘ë‹µ ë°ì´í„°\n    items = []\n    for r in restaurants:\n        items.append({\n            \"id\": r.id,\n            \"name\": r.name,\n            \"name_en\": r.name_en,\n            \"category\": r.category,\n            \"address\": r.address,\n            \"district\": r.district,\n            \"phone\": r.phone,\n            \"google_rating\": r.google_rating,\n            \"google_review_count\": r.google_review_count,\n            \"quality_score\": r.quality_score,\n            \"sync_status\": \"synced\" if r.synced_to_hansikdang else \"pending\",\n            \"created_at\": r.created_at.isoformat() if r.created_at else None,\n            \"image_url\": r.image_urls[0] if r.image_urls and len(r.image_urls) > 0 else None\n        })\n    \n    return {\n        \"status\": \"success\",\n        \"total\": total_count,\n        \"page\": page,\n        \"limit\": limit,\n        \"total_pages\": (total_count + limit - 1) // limit,\n        \"items\": items\n    }\n\n\n@router.get(\"/{restaurant_id}\")\ndef get_restaurant(\n    restaurant_id: str,\n    db: Session = Depends(get_db)\n) -> Dict[str, Any]:\n    \"\"\"\n    ë ˆìŠ¤í† ë‘ ìƒì„¸ ì¡°íšŒ\n    \"\"\"\n    restaurant = db.query(ProcessedRestaurant).filter(\n        ProcessedRestaurant.id == restaurant_id\n    ).first()\n    \n    if not restaurant:\n        raise HTTPException(status_code=404, detail=\"ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n    \n    return {\n        \"status\": \"success\",\n        \"data\": {\n            \"id\": restaurant.id,\n            \"name\": restaurant.name,\n            \"name_en\": restaurant.name_en,\n            \"category\": restaurant.category,\n            \"address\": restaurant.address,\n            \"district\": restaurant.district,\n            \"phone\": restaurant.phone,\n            \"description\": restaurant.description,\n            \"description_en\": restaurant.description_en,\n            \"menu_summary\": restaurant.menu_summary,\n            \"open_hours\": restaurant.open_hours,\n            \"price_range\": restaurant.price_range,\n            \"google_rating\": restaurant.google_rating,\n            \"google_review_count\": restaurant.google_review_count,\n            \"google_place_id\": restaurant.google_place_id,\n            \"image_urls\": restaurant.image_urls or [],\n            \"latitude\": restaurant.latitude,\n            \"longitude\": restaurant.longitude,\n            \"naver_place_id\": restaurant.naver_place_id,\n            \"website\": restaurant.website,\n            \"quality_score\": restaurant.quality_score,\n            \"popularity_score\": restaurant.popularity_score,\n            \"popularity_tier\": restaurant.popularity_tier,\n            \"sync_status\": \"synced\" if restaurant.synced_to_hansikdang else \"pending\",\n            \"synced_to_hansikdang\": restaurant.synced_to_hansikdang,\n            \"created_at\": restaurant.created_at.isoformat() if restaurant.created_at else None,\n            \"updated_at\": restaurant.updated_at.isoformat() if restaurant.updated_at else None\n        }\n    }\n\n\n@router.put(\"/{restaurant_id}\")\ndef update_restaurant(\n    restaurant_id: str,\n    update_data: RestaurantUpdate,\n    db: Session = Depends(get_db)\n) -> Dict[str, Any]:\n    \"\"\"\n    ë ˆìŠ¤í† ë‘ ì •ë³´ ìˆ˜ì •\n    \"\"\"\n    restaurant = db.query(ProcessedRestaurant).filter(\n        ProcessedRestaurant.id == restaurant_id\n    ).first()\n    \n    if not restaurant:\n        raise HTTPException(status_code=404, detail=\"ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n    \n    # ì—…ë°ì´íŠ¸í•  í•„ë“œë§Œ ì ìš©\n    update_dict = update_data.dict(exclude_unset=True)\n    \n    for field, value in update_dict.items():\n        setattr(restaurant, field, value)\n    \n    restaurant.updated_at = datetime.now(timezone.utc)\n    \n    try:\n        db.commit()\n        db.refresh(restaurant)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"ë ˆìŠ¤í† ë‘ ì •ë³´ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"data\": {\n                \"id\": restaurant.id,\n                \"name\": restaurant.name,\n                \"updated_at\": restaurant.updated_at.isoformat()\n            }\n        }\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.delete(\"/{restaurant_id}\")\ndef delete_restaurant(\n    restaurant_id: str,\n    db: Session = Depends(get_db)\n) -> Dict[str, Any]:\n    \"\"\"\n    ë ˆìŠ¤í† ë‘ ì‚­ì œ\n    \"\"\"\n    restaurant = db.query(ProcessedRestaurant).filter(\n        ProcessedRestaurant.id == restaurant_id\n    ).first()\n    \n    if not restaurant:\n        raise HTTPException(status_code=404, detail=\"ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n    \n    restaurant_name = restaurant.name\n    \n    try:\n        db.delete(restaurant)\n        db.commit()\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"'{restaurant_name}' ë ˆìŠ¤í† ë‘ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"deleted_id\": restaurant_id\n        }\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì‚­ì œ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/districts/list\")\ndef get_districts(db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì—­ ëª©ë¡ ì¡°íšŒ\n    \"\"\"\n    districts = db.query(ProcessedRestaurant.district).distinct().filter(\n        ProcessedRestaurant.district.isnot(None)\n    ).all()\n    \n    district_list = [d[0] for d in districts if d[0]]\n    district_list.sort()\n    \n    return {\n        \"status\": \"success\",\n        \"districts\": district_list\n    }\n","size_bytes":8979},"data-hub/src/api/jobs_routes.py":{"content":"from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks\nfrom sqlalchemy.orm import Session\nfrom typing import Dict, Any\nfrom datetime import datetime, timezone\nimport asyncio\n\nfrom src.database.connection import get_db\nfrom src.workflows.scraping import ScrapingWorkflow\nfrom src.workflows.sync import SyncWorkflow\nfrom src.api.governance_routes import BackupRequest\nimport logging\n\nrouter = APIRouter(prefix=\"/api/jobs\", tags=[\"jobs\"])\nlogger = logging.getLogger(__name__)\n\n\n@router.post(\"/targeting/run\")\nasync def run_targeting(background_tasks: BackgroundTasks, db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    Smart Targeting ì‹¤í–‰ (Google Trends ë¶„ì„ + ë™ì  ì¿¼ë¦¬ ìƒì„±)\n    \"\"\"\n    try:\n        from src.workflows.smart_targeting import SmartTargetingWorkflow\n        \n        workflow = SmartTargetingWorkflow()\n        \n        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n        background_tasks.add_task(workflow.run_smart_targeting)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"Smart Targetingì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤\",\n            \"started_at\": datetime.now(timezone.utc).isoformat()\n        }\n    except Exception as e:\n        logger.error(f\"Smart Targeting ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/scraping/run\")\nasync def run_scraping(background_tasks: BackgroundTasks, db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    Naver Maps ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ (33ê°œ ìŠ¤ë§ˆíŠ¸ ì¿¼ë¦¬)\n    \"\"\"\n    try:\n        workflow = ScrapingWorkflow()\n        \n        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n        background_tasks.add_task(workflow.run_daily_scraping)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"Naver ìŠ¤í¬ë˜í•‘ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤\",\n            \"started_at\": datetime.now(timezone.utc).isoformat()\n        }\n    except Exception as e:\n        logger.error(f\"ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/deduplication/run\")\nasync def run_deduplication(background_tasks: BackgroundTasks, db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    ì¤‘ë³µ íƒì§€ & ë³‘í•© ì‹¤í–‰\n    \"\"\"\n    try:\n        from src.workflows.deduplication import DeduplicationWorkflow\n        \n        workflow = DeduplicationWorkflow()\n        \n        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n        background_tasks.add_task(workflow.detect_and_merge_duplicates)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"ì¤‘ë³µ íƒì§€ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤\",\n            \"started_at\": datetime.now(timezone.utc).isoformat()\n        }\n    except Exception as e:\n        logger.error(f\"ì¤‘ë³µ íƒì§€ ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/gemini/run\")\nasync def run_gemini_processing(background_tasks: BackgroundTasks, db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    Gemini AI ë°ì´í„° ì •ì œ ì‹¤í–‰\n    \"\"\"\n    try:\n        from src.processors.gemini import GeminiProcessor\n        from src.database.models import RawRestaurantData, ProcessedRestaurant\n        \n        async def process_with_gemini():\n            \"\"\"Gemini ì²˜ë¦¬ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…\"\"\"\n            gemini = GeminiProcessor()\n            \n            with db as session:\n                # pending ìƒíƒœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n                raw_data = session.query(RawRestaurantData).filter(\n                    RawRestaurantData.status == 'pending'\n                ).limit(100).all()\n                \n                logger.info(f\"Found {len(raw_data)} pending records for Gemini processing\")\n                \n                processed_count = 0\n                for raw in raw_data:\n                    try:\n                        refined = await gemini.refine_restaurant_data(raw.raw_data)\n                        quality = await gemini.calculate_quality_score(raw.raw_data)\n                        \n                        # ProcessedRestaurant ìƒì„±\n                        processed = ProcessedRestaurant(\n                            id=raw.id,\n                            name=refined.get('name'),\n                            description=refined.get('description'),\n                            quality_score=quality,\n                            **refined\n                        )\n                        \n                        session.add(processed)\n                        raw.status = 'processed'\n                        processed_count += 1\n                        \n                    except Exception as e:\n                        logger.error(f\"Failed to process {raw.id}: {e}\")\n                        continue\n                \n                session.commit()\n                logger.info(f\"Gemini processing completed: {processed_count} restaurants\")\n        \n        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n        background_tasks.add_task(process_with_gemini)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"Gemini AI ì •ì œê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤\",\n            \"started_at\": datetime.now(timezone.utc).isoformat()\n        }\n    except Exception as e:\n        logger.error(f\"Gemini ì²˜ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/places/run\")\nasync def run_places_enrichment(background_tasks: BackgroundTasks, db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    Google Places API ë°ì´í„° ë³´ê°• ì‹¤í–‰ (í‰ì , ë¦¬ë·°, ì´ë¯¸ì§€)\n    \"\"\"\n    try:\n        from src.workflows.google_places import GooglePlacesWorkflow\n        \n        workflow = GooglePlacesWorkflow()\n        \n        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n        background_tasks.add_task(workflow.enrich_restaurants)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"Google Places ë³´ê°•ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤\",\n            \"started_at\": datetime.now(timezone.utc).isoformat()\n        }\n    except Exception as e:\n        logger.error(f\"Google Places ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/sync/run\")\nasync def run_sync(background_tasks: BackgroundTasks, db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    ë©”ì¸ í”Œë«í¼ ë™ê¸°í™” ì‹¤í–‰ (í•œì‹ë‹¹ ì•±)\n    \"\"\"\n    try:\n        workflow = SyncWorkflow()\n        \n        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n        background_tasks.add_task(workflow.sync_to_hansikdang)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"ë©”ì¸ í”Œë«í¼ ë™ê¸°í™”ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤\",\n            \"started_at\": datetime.now(timezone.utc).isoformat()\n        }\n    except Exception as e:\n        logger.error(f\"ë™ê¸°í™” ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/backup/run\")\nasync def run_backup(background_tasks: BackgroundTasks, db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    Google Drive ë°±ì—… ì‹¤í–‰\n    \"\"\"\n    try:\n        from src.governance.drive_backup import DriveBackupManager\n        \n        backup_manager = DriveBackupManager(db)\n        \n        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ (manual backup)\n        background_tasks.add_task(backup_manager.backup_daily, None, 'manual')\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"Google Drive ë°±ì—…ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤\",\n            \"started_at\": datetime.now(timezone.utc).isoformat()\n        }\n    except Exception as e:\n        logger.error(f\"ë°±ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/status\")\nasync def get_jobs_status(db: Session = Depends(get_db)) -> Dict[str, Any]:\n    \"\"\"\n    ëª¨ë“  ì‘ì—…ì˜ ë§ˆì§€ë§‰ ì‹¤í–‰ ìƒíƒœ ì¡°íšŒ\n    \"\"\"\n    try:\n        from src.database.models import (\n            ScrapingTarget, ScrapingLog, DuplicateGroup, MergeHistory,\n            SyncLog, BackupHistory\n        )\n        \n        # ê° ì‘ì—…ì˜ ë§ˆì§€ë§‰ ì‹¤í–‰ ì •ë³´\n        latest_targeting = db.query(ScrapingTarget).filter(\n            ScrapingTarget.created_by == 'auto'\n        ).order_by(\n            ScrapingTarget.created_at.desc()\n        ).first()\n        \n        latest_scraping = db.query(ScrapingLog).order_by(\n            ScrapingLog.started_at.desc()\n        ).first()\n        \n        latest_dedup = db.query(DuplicateGroup).order_by(\n            DuplicateGroup.created_at.desc()\n        ).first()\n        \n        latest_merge = db.query(MergeHistory).order_by(\n            MergeHistory.merged_at.desc()\n        ).first()\n        \n        latest_sync = db.query(SyncLog).order_by(\n            SyncLog.started_at.desc()\n        ).first()\n        \n        latest_backup = db.query(BackupHistory).order_by(\n            BackupHistory.started_at.desc()\n        ).first()\n        \n        return {\n            \"status\": \"success\",\n            \"jobs\": {\n                \"targeting\": {\n                    \"last_run\": latest_targeting.created_at.isoformat() if latest_targeting else None,\n                    \"status\": \"completed\" if latest_targeting else \"never_run\",\n                    \"result\": f\"Query: {latest_targeting.keyword}\" if latest_targeting else None\n                },\n                \"scraping\": {\n                    \"last_run\": latest_scraping.started_at.isoformat() if latest_scraping else None,\n                    \"status\": latest_scraping.status if latest_scraping else \"never_run\",\n                    \"result\": f\"{latest_scraping.success_count} scraped\" if latest_scraping else None\n                },\n                \"deduplication\": {\n                    \"last_run\": latest_dedup.created_at.isoformat() if latest_dedup else None,\n                    \"status\": latest_dedup.status if latest_dedup else \"never_run\",\n                    \"result\": f\"{len(latest_dedup.duplicate_ids) if latest_dedup and latest_dedup.duplicate_ids else 0} duplicates detected, {latest_merge.merged_ids if latest_merge else 0} merged\" if latest_dedup else None\n                },\n                \"gemini\": {\n                    \"last_run\": None,\n                    \"status\": \"not_tracked\",\n                    \"result\": \"Check processed_restaurants count\"\n                },\n                \"places\": {\n                    \"last_run\": None,\n                    \"status\": \"not_tracked\",\n                    \"result\": \"Check google_rating fields\"\n                },\n                \"sync\": {\n                    \"last_run\": latest_sync.started_at.isoformat() if latest_sync else None,\n                    \"status\": latest_sync.status if latest_sync else \"never_run\",\n                    \"result\": f\"{latest_sync.success_count} synced\" if latest_sync else None\n                },\n                \"backup\": {\n                    \"last_run\": latest_backup.started_at.isoformat() if latest_backup else None,\n                    \"status\": latest_backup.status if latest_backup else \"never_run\",\n                    \"result\": f\"{latest_backup.total_records} records\" if latest_backup else None\n                }\n            }\n        }\n    except Exception as e:\n        logger.error(f\"ì‘ì—… ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/scheduler/pause\")\nasync def pause_scheduler() -> Dict[str, Any]:\n    \"\"\"\n    ìŠ¤ì¼€ì¤„ëŸ¬ ì¼ì‹œì •ì§€ (ì£¼ì˜: ì‹¤ì œ êµ¬í˜„ í•„ìš”)\n    \"\"\"\n    return {\n        \"status\": \"info\",\n        \"message\": \"ìŠ¤ì¼€ì¤„ëŸ¬ ì¼ì‹œì •ì§€ ê¸°ëŠ¥ì€ scheduler.pyì—ì„œ êµ¬í˜„ í•„ìš”\",\n        \"note\": \"í˜„ì¬ëŠ” scheduler í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¤‘ì§€í•´ì•¼ í•©ë‹ˆë‹¤\"\n    }\n\n\n@router.post(\"/scheduler/resume\")\nasync def resume_scheduler() -> Dict[str, Any]:\n    \"\"\"\n    ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ê°œ (ì£¼ì˜: ì‹¤ì œ êµ¬í˜„ í•„ìš”)\n    \"\"\"\n    return {\n        \"status\": \"info\",\n        \"message\": \"ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ê°œ ê¸°ëŠ¥ì€ scheduler.pyì—ì„œ êµ¬í˜„ í•„ìš”\",\n        \"note\": \"í˜„ì¬ëŠ” scheduler í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤\"\n    }\n\n\n@router.post(\"/scheduler/restart\")\nasync def restart_scheduler() -> Dict[str, Any]:\n    \"\"\"\n    ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ì‹œì‘ (ì£¼ì˜: ì‹¤ì œ êµ¬í˜„ í•„ìš”)\n    \"\"\"\n    return {\n        \"status\": \"info\",\n        \"message\": \"ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ì‹œì‘ ê¸°ëŠ¥ì€ scheduler.pyì—ì„œ êµ¬í˜„ í•„ìš”\",\n        \"note\": \"í˜„ì¬ëŠ” Replit workflowë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¬ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤\"\n    }\n","size_bytes":12712},"attached_assets/urgent_improvement_plan_1762584071417.md":{"content":"# Data Hub ì–´ë“œë¯¼: ì¹˜ëª…ì  ë¬¸ì œ ì¦‰ì‹œ ê°œì„  ê³„íš\n\n**ìƒí™©**: ì‹¤ì œ ìŠ¤í¬ë¦°ìƒ· ê²€ì¦ í›„ ì‹¬ê°í•œ ë¬¸ì œ ë°œê²¬  \n**ë¬¸ì œ**: ë„¤ë¹„ê²Œì´ì…˜ ì—†ìŒ + ì‘ì—… ëª¨ë‹ˆí„°ë§ ë¶ˆê°€  \n**ì‹¬ê°ë„**: ğŸ”´ **ê¸´ê¸‰ (ì¦‰ì‹œ ê°œì„  í•„ìš”)**  \n**ì˜ˆìƒ ì†Œìš”**: 6-8ì‹œê°„\n\n---\n\n## ğŸš¨ ë°œê²¬ëœ ì¹˜ëª…ì  ë¬¸ì œ\n\n### 1. ë„¤ë¹„ê²Œì´ì…˜ì´ ì™„ì „íˆ ì—†ë‹¤\n\n**í˜„ì¬**:\n```\nëª¨ë“  ê¸°ëŠ¥ì´ í•œ í™”ë©´ì— ë­‰ì³ìˆìŒ\n- ë©”ë‰´ ì—†ìŒ\n- ë„¤ë¹„ê²Œì´ì…˜ ì—†ìŒ\n- ê¸°ëŠ¥ë³„ ë…ë¦½ í˜ì´ì§€ ì—†ìŒ\n```\n\n**í•„ìš”**:\n```\nëª…í™•í•œ ì¢Œì¸¡ ë˜ëŠ” ìƒë‹¨ ë©”ë‰´:\n- ëŒ€ì‹œë³´ë“œ\n- ë°ì´í„° ê´€ë¦¬\n- ì‘ì—… ëª¨ë‹ˆí„°ë§ â† ì—†ìŒ!\n- ë¶„ì„ & ë¦¬í¬íŠ¸\n- ì‹œìŠ¤í…œ ì„¤ì •\n```\n\n---\n\n### 2. ì‘ì—… ì‹¤í–‰ í›„ ëª¨ë‹ˆí„°ë§ ë¶ˆê°€\n\n**í˜„ì¬**:\n```\n[ë²„íŠ¼ í´ë¦­] â†’ ì‘ì—… ì‹¤í–‰ â†’ ???\nâ†’ ì§„í–‰ìƒí™© ëª¨ë¦„\nâ†’ ë¡œê·¸ ë³¼ ìˆ˜ ì—†ìŒ\nâ†’ ì–¸ì œ ì™„ë£Œë˜ë‚˜? ëª¨ë¦„\nâ†’ ì—ëŸ¬ ë‚¬ë‚˜? ëª¨ë¦„\n```\n\n**í•„ìš”**:\n```\nì‘ì—… ì‹¤í–‰ ì‹œ:\n- ì§„í–‰ìƒí™© í˜ì´ì§€ë¡œ ìë™ ì´ë™\n- ì‹¤ì‹œê°„ ë¡œê·¸ í‘œì‹œ\n- ì§„í–‰ë¥  ë°” í‘œì‹œ\n- ì™„ë£Œ í›„ ê²°ê³¼ í‘œì‹œ\n```\n\n---\n\n### 3. ì‘ì—… ê²°ê³¼ í™•ì¸ ë¶ˆê°€\n\n**í˜„ì¬**:\n```\n\"Smart Targeting ì‹¤í–‰\" ë²„íŠ¼ì„ í´ë¦­í–ˆë‹¤\nâ†’ ê·¸ ë‹¤ìŒ?\nâ†’ 33ê°œ ì¿¼ë¦¬ê°€ ìƒì„±ë˜ì—ˆë‚˜?\nâ†’ ì–´ë””ì— ìˆë‚˜?\nâ†’ í™•ì¸í•  ìˆ˜ ì—†ìŒ!\n```\n\n---\n\n## ğŸ“‹ Replitì—ê²Œ ë³´ë‚¼ ê¸´ê¸‰ í”„ë¡¬í”„íŠ¸\n\n```\ní˜„ì¬ ëŒ€ì‹œë³´ë“œì˜ ì¹˜ëª…ì  ë¬¸ì œ ë°œê²¬:\n\në¬¸ì œ:\n\"ì‘ì—… ì‹¤í–‰ ë²„íŠ¼ì€ ìˆì§€ë§Œ, \n ì‹¤í–‰ í›„ ì§„í–‰ìƒí™©/ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n\ní˜„ì¬ ìƒíƒœ:\nâœ… ê¸°ëŠ¥ ë²„íŠ¼: 7ê°œ ìˆìŒ\nâŒ ë„¤ë¹„ê²Œì´ì…˜: ì—†ìŒ\nâŒ ì‘ì—… ëª¨ë‹ˆí„°ë§: ì—†ìŒ\nâŒ ê²°ê³¼ í™•ì¸: ë¶ˆê°€ëŠ¥\n\nê²°ê³¼:\nâ†’ \"ê´€ë¦¬ ë„êµ¬\"ê°€ ì•„ë‹ˆë¼ \"ë²„íŠ¼ ëª¨ìŒ\"\n\nì¦‰ì‹œ ì¶”ê°€ í•„ìš”:\n\n1ï¸âƒ£ ì¢Œì¸¡ ë©”ë‰´ ë„¤ë¹„ê²Œì´ì…˜ ì¶”ê°€\n   í˜„ì¬ êµ¬ì¡°:\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚  í•œì‹ë‹¹ Data Hub    â”‚\n   â”‚  (ë©”ë‰´ ì—†ìŒ)        â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n   \n   ê°œì„  í›„:\n   â”Œâ”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚  â”‚  í•œì‹ë‹¹ Data Hub  â”‚\n   â”‚â‘ â”‚  ğŸ“Š ëŒ€ì‹œë³´ë“œ      â”‚\n   â”‚â‘¡â”‚  ğŸ” ë°ì´í„° ê´€ë¦¬  â”‚\n   â”‚â‘¢â”‚  âš™ï¸ ì‘ì—… ëª¨ë‹ˆí„°ë§â”‚ â† NEW!\n   â”‚â‘£â”‚  ğŸ“Š ë¶„ì„        â”‚\n   â”‚â‘¤â”‚  âš™ï¸ ì„¤ì •        â”‚\n   â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n2ï¸âƒ£ ì‘ì—… ëª¨ë‹ˆí„°ë§ í˜ì´ì§€ ì¶”ê°€\n   ê²½ë¡œ: /dashboard/jobs/monitoring\n   \n   í‘œì‹œ ë‚´ìš©:\n   - ì‘ì—…ëª…\n   - í˜„ì¬ ìƒíƒœ (ëŒ€ê¸°/ì‹¤í–‰ì¤‘/ì™„ë£Œ/ì‹¤íŒ¨)\n   - ì§„í–‰ë¥  (%)\n   - ì‹œì‘ ì‹œê°„\n   - ì˜ˆìƒ ì™„ë£Œ ì‹œê°„\n   - ì‹¤ì‹œê°„ ë¡œê·¸\n   - ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜\n   \n   ì˜ˆ:\n   ğŸ¯ Smart Targeting\n   ìƒíƒœ: ì‹¤í–‰ ì¤‘ (80%)\n   ì§„í–‰: 24 / 30ê°œ\n   ì‹œì‘: 15:30:05\n   ì˜ˆìƒ: 15:32:55\n   ë¡œê·¸:\n   [15:30:05] ì‘ì—… ì‹œì‘\n   [15:30:12] ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ (33ê°œ)\n   [15:30:25] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘\n   ...\n\n3ï¸âƒ£ ì‘ì—… ê²°ê³¼ í˜ì´ì§€ ì¶”ê°€\n   ê²½ë¡œ: /dashboard/jobs/{job_name}/result\n   \n   í‘œì‹œ ë‚´ìš©:\n   - ì‘ì—…ëª…\n   - ì‹¤í–‰ ì‹œê°„\n   - ìƒíƒœ (ì„±ê³µ/ì‹¤íŒ¨)\n   - ì²˜ë¦¬ í†µê³„\n   - ìƒì„±ëœ ë°ì´í„°\n   - [ë°ì´í„° í™•ì¸]ìœ¼ë¡œ ì´ë™ ê°€ëŠ¥\n   \n   ì˜ˆ:\n   ğŸ¯ Smart Targeting\n   ìƒíƒœ: ì™„ë£Œ âœ…\n   ì‹œê°„: 15:30:05 ~ 15:32:55\n   \n   ê²°ê³¼:\n   - ìƒì„± ì¿¼ë¦¬: 33ê°œ âœ…\n   - íŠ¸ë Œë“œ ë¶„ì„: ì™„ë£Œ âœ…\n   - ìš°ì„ ìˆœìœ„ ê³„ì‚°: ì™„ë£Œ âœ…\n   \n   [ğŸ“Š ê²°ê³¼ ë°ì´í„° í™•ì¸]\n\n4ï¸âƒ£ ê° ì‘ì—… ìƒì„¸ í˜ì´ì§€\n   ê²½ë¡œ: /dashboard/jobs/{job_name}/detail\n   \n   í‘œì‹œ ë‚´ìš©:\n   - ìµœê·¼ 10íšŒ ì‹¤í–‰ ì´ë ¥\n   - ê° ì‹¤í–‰ì˜ ë¡œê·¸\n   - í†µê³„\n   - ìŠ¤ì¼€ì¤„ ì •ë³´\n\n**ë‹¹ì‹ ì˜ íŒë‹¨:**\n1. ì´ êµ¬ì¡°ê°€ í•„ìš”í•œê°€?\n2. ë” ë‚˜ì€ ë°©ì•ˆì´ ìˆëŠ”ê°€?\n3. ìš°ì„ ìˆœìœ„ëŠ” ì–´ë–»ê²Œ ë˜ëŠ”ê°€?\n4. ì˜ˆìƒ ì†Œìš”ì‹œê°„ì€?\n\nì¦‰ì‹œ ì§„í–‰ ë¶€íƒí•©ë‹ˆë‹¤.\n```\n\n---\n\n## ğŸ¯ ì¦‰ì‹œ ì‹¤í–‰ ê³„íš\n\n### ì˜¤ëŠ˜ (11ì›” 8ì¼)\n\n```\n[ ] 56ë²ˆ PDF ê²€í†  (í˜„ì¬ ìƒí™© íŒŒì•…)\n[ ] ìœ„ í”„ë¡¬í”„íŠ¸ë¥¼ Replitì— ì „ë‹¬\n[ ] Replitì˜ í”¼ë“œë°± ëŒ€ê¸°\n```\n\n### ë‚´ì¼ (11ì›” 9ì¼)\n\n```\n[ ] ì¢Œì¸¡ ë©”ë‰´ ë„¤ë¹„ê²Œì´ì…˜ ê°œë°œ (2ì‹œê°„)\n[ ] ì‘ì—… ëª¨ë‹ˆí„°ë§ í˜ì´ì§€ ê°œë°œ (3ì‹œê°„)\n[ ] ì‘ì—… ê²°ê³¼ í˜ì´ì§€ ê°œë°œ (2ì‹œê°„)\n[ ] ê° ì‘ì—… ìƒì„¸ í˜ì´ì§€ ê°œë°œ (1ì‹œê°„)\n```\n\n### 11ì›” 10ì¼\n\n```\n[ ] í†µí•© í…ŒìŠ¤íŠ¸\n[ ] ìµœì¢… ê²€ì¦\n[ ] ë°°í¬ ì¤€ë¹„\n```\n\n---\n\n## ğŸ“Š ê°œì„  íš¨ê³¼\n\n### Before (í˜„ì¬)\n\n```\n[ëŒ€ì‹œë³´ë“œ]\nâ”œâ”€ ì‹œìŠ¤í…œ í—¬ìŠ¤ (ë³´ê¸°ë§Œ)\nâ”œâ”€ ì–´ì œ í†µê³„ (ë³´ê¸°ë§Œ)\nâ”œâ”€ ë°±ì—… ìƒíƒœ (ë³´ê¸°ë§Œ)\nâ””â”€ ì‘ì—… ì‹¤í–‰ ë²„íŠ¼ (7ê°œ)\n\në¬¸ì œ:\n- ì‘ì—… ì‹¤í–‰ í›„ ì§„í–‰ìƒí™© ë¶ˆëª…\n- ê²°ê³¼ í™•ì¸ ë¶ˆê°€\n- ê´€ë¦¬ ê¸°ëŠ¥ ì—†ìŒ\n\ní‰ê°€: 30ì  (ë²„íŠ¼ë§Œ ë§ìŒ)\n```\n\n### After (ê°œì„  í›„)\n\n```\n[ë©”ë‰´]\nâ”œâ”€ ëŒ€ì‹œë³´ë“œ (í˜„ì¬ í™”ë©´)\nâ”œâ”€ ë°ì´í„° ê´€ë¦¬\nâ”œâ”€ ì‘ì—… ëª¨ë‹ˆí„°ë§ â† NEW!\nâ”‚  â”œâ”€ ì‹¤ì‹œê°„ ì§„í–‰ìƒí™©\nâ”‚  â”œâ”€ ë¡œê·¸\nâ”‚  â””â”€ ê²°ê³¼\nâ”œâ”€ ë¶„ì„ & ë¦¬í¬íŠ¸\nâ””â”€ ì‹œìŠ¤í…œ ì„¤ì •\n\nì¥ì :\n- ê° ê¸°ëŠ¥ë³„ ë…ë¦½ ê´€ë¦¬\n- ì‘ì—… ì‹¤í–‰ í›„ ì§„í–‰ìƒí™© ì‹¤ì‹œê°„ í™•ì¸\n- ê²°ê³¼ ë°ì´í„° í™•ì¸ ê°€ëŠ¥\n- ì™„ì „í•œ ê´€ë¦¬ ì‹œìŠ¤í…œ\n\ní‰ê°€: 85ì  (ì§„ì •í•œ ì–´ë“œë¯¼)\n```\n\n---\n\n## ğŸ”¥ í•µì‹¬ ë©”ì‹œì§€\n\n```\në‹¹ì‹ ì˜ ì§€ì :\n\"ê° ë©”ë‰´ë³„ë¡œ ê´€ë¦¬ë¥¼ í•˜ëŠ” ë³„ë„ì˜ ê³µê°„ì´ ì—†ë‹¤\"\n\nì´ê²ƒì´ ì •í™•íˆ ë¬¸ì œì…ë‹ˆë‹¤!\n\ní˜„ì¬: ëª¨ë“  ê²ƒì´ í•œ í™”ë©´ì— ìˆê³  ë²„íŠ¼ë§Œ ë§ìŒ\ní•„ìš”: ê° ê¸°ëŠ¥ë³„ ë…ë¦½ í˜ì´ì§€ + ë„¤ë¹„ê²Œì´ì…˜\n\nReplitì´ ì ˆë°˜ë§Œ í–ˆìŠµë‹ˆë‹¤.\nì´ì œ ë‚˜ë¨¸ì§€ ì ˆë°˜ì„ í•´ì•¼ í•©ë‹ˆë‹¤.\n```\n\n---\n\n**ì§€ê¸ˆ ë°”ë¡œ Replitì—ê²Œ ìœ„ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ì„¸ìš”!** ğŸš€\n\n","size_bytes":5545},"data-hub/src/api/collection_routes.py":{"content":"\"\"\"\nìˆ˜ì§‘ ì„¤ì • ê´€ë¦¬ API - Stage A MVP\nCollection Configuration Management\n\"\"\"\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm import Session\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime, timezone\nfrom pydantic import BaseModel\nimport logging\n\nfrom src.database.connection import get_db\nfrom src.database.models import CollectionConfig\n\nrouter = APIRouter(prefix=\"/api/data-management/collection-configs\", tags=[\"collection-management\"])\nlogger = logging.getLogger(__name__)\n\n\nclass RegionSchema(BaseModel):\n    \"\"\"ì§€ì—­ ìŠ¤í‚¤ë§ˆ\"\"\"\n    sido: str\n    gugun: Optional[str] = None\n    dong: Optional[str] = None\n\n\nclass CollectionConfigCreate(BaseModel):\n    \"\"\"ìˆ˜ì§‘ ì„¤ì • ìƒì„± ìŠ¤í‚¤ë§ˆ\"\"\"\n    name: str\n    regions: List[Dict[str, str]]  # [{\"sido\": \"ì„œìš¸\", \"gugun\": \"ê°•ë‚¨êµ¬\"}]\n    keywords: List[str]  # [\"ê°ˆë¹„\", \"í•œìš°\"]\n    source: str = \"both\"  # google/naver/both\n    status: str = \"active\"  # active/paused\n    is_active: bool = True\n\n\nclass CollectionConfigUpdate(BaseModel):\n    \"\"\"ìˆ˜ì§‘ ì„¤ì • ìˆ˜ì • ìŠ¤í‚¤ë§ˆ\"\"\"\n    name: Optional[str] = None\n    regions: Optional[List[Dict[str, str]]] = None\n    keywords: Optional[List[str]] = None\n    source: Optional[str] = None\n    status: Optional[str] = None\n    is_active: Optional[bool] = None\n\n\nclass CollectionConfigResponse(BaseModel):\n    \"\"\"ìˆ˜ì§‘ ì„¤ì • ì‘ë‹µ ìŠ¤í‚¤ë§ˆ\"\"\"\n    id: int\n    name: str\n    regions: List[Dict[str, str]]\n    keywords: List[str]\n    source: str\n    status: str\n    monthly_cost: float\n    is_active: bool\n    created_at: datetime\n    updated_at: Optional[datetime] = None\n\n    class Config:\n        from_attributes = True\n\n\ndef calculate_monthly_cost(config: CollectionConfigCreate) -> float:\n    \"\"\"\n    ì›”ê°„ ì˜ˆìƒ ë¹„ìš© ê³„ì‚°\n    - í‚¤ì›Œë“œ ìˆ˜ Ã— ì§€ì—­ ìˆ˜ Ã— ì†ŒìŠ¤ë³„ ë‹¨ê°€\n    - Naver: $10/í‚¤ì›Œë“œ/ì§€ì—­\n    - Google: $15/í‚¤ì›Œë“œ/ì§€ì—­\n    \"\"\"\n    keyword_count = len(config.keywords)\n    region_count = len(config.regions)\n    \n    naver_cost = 10.0\n    google_cost = 15.0\n    \n    if config.source == 'naver':\n        return keyword_count * region_count * naver_cost\n    elif config.source == 'google':\n        return keyword_count * region_count * google_cost\n    elif config.source == 'both':\n        return keyword_count * region_count * (naver_cost + google_cost)\n    \n    return 0.0\n\n\n@router.post(\"\", response_model=CollectionConfigResponse)\nasync def create_collection_config(\n    config: CollectionConfigCreate,\n    db: Session = Depends(get_db)\n) -> Dict[str, Any]:\n    \"\"\"\n    ìƒˆ ìˆ˜ì§‘ ì„¤ì • ì¶”ê°€\n    \"\"\"\n    try:\n        monthly_cost = calculate_monthly_cost(config)\n        \n        new_config = CollectionConfig(\n            name=config.name,\n            regions=config.regions,\n            keywords=config.keywords,\n            source=config.source,\n            status=config.status,\n            monthly_cost=monthly_cost,\n            is_active=config.is_active\n        )\n        \n        db.add(new_config)\n        db.commit()\n        db.refresh(new_config)\n        \n        logger.info(f\"Created collection config: {new_config.id} - {new_config.name}\")\n        \n        return new_config\n    \n    except Exception as e:\n        logger.error(f\"Failed to create collection config: {e}\")\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"\", response_model=List[CollectionConfigResponse])\nasync def get_collection_configs(\n    status: Optional[str] = None,\n    is_active: Optional[bool] = None,\n    limit: int = 100,\n    db: Session = Depends(get_db)\n) -> List[CollectionConfig]:\n    \"\"\"\n    ìˆ˜ì§‘ ì„¤ì • ëª©ë¡ ì¡°íšŒ\n    \"\"\"\n    try:\n        query = db.query(CollectionConfig)\n        \n        if status:\n            query = query.filter(CollectionConfig.status == status)\n        \n        if is_active is not None:\n            query = query.filter(CollectionConfig.is_active == is_active)\n        \n        configs = query.order_by(\n            CollectionConfig.created_at.desc()\n        ).limit(limit).all()\n        \n        return configs\n    \n    except Exception as e:\n        logger.error(f\"Failed to get collection configs: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/{config_id}\", response_model=CollectionConfigResponse)\nasync def get_collection_config(\n    config_id: int,\n    db: Session = Depends(get_db)\n) -> CollectionConfig:\n    \"\"\"\n    íŠ¹ì • ìˆ˜ì§‘ ì„¤ì • ì¡°íšŒ\n    \"\"\"\n    try:\n        config = db.query(CollectionConfig).filter(\n            CollectionConfig.id == config_id\n        ).first()\n        \n        if not config:\n            raise HTTPException(status_code=404, detail=\"ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        return config\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to get collection config {config_id}: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.put(\"/{config_id}\", response_model=CollectionConfigResponse)\nasync def update_collection_config(\n    config_id: int,\n    config_update: CollectionConfigUpdate,\n    db: Session = Depends(get_db)\n) -> CollectionConfig:\n    \"\"\"\n    ìˆ˜ì§‘ ì„¤ì • ìˆ˜ì •\n    \"\"\"\n    try:\n        config = db.query(CollectionConfig).filter(\n            CollectionConfig.id == config_id\n        ).first()\n        \n        if not config:\n            raise HTTPException(status_code=404, detail=\"ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        update_data = config_update.dict(exclude_unset=True)\n        \n        for field, value in update_data.items():\n            setattr(config, field, value)\n        \n        if config_update.keywords or config_update.regions or config_update.source:\n            temp_create = CollectionConfigCreate(\n                name=config.name,\n                regions=config.regions,\n                keywords=config.keywords,\n                source=config.source\n            )\n            config.monthly_cost = calculate_monthly_cost(temp_create)\n        \n        config.updated_at = datetime.now(timezone.utc)\n        \n        db.commit()\n        db.refresh(config)\n        \n        logger.info(f\"Updated collection config: {config_id}\")\n        \n        return config\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to update collection config {config_id}: {e}\")\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ìˆ˜ì • ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.delete(\"/{config_id}\")\nasync def delete_collection_config(\n    config_id: int,\n    db: Session = Depends(get_db)\n) -> Dict[str, Any]:\n    \"\"\"\n    ìˆ˜ì§‘ ì„¤ì • ì‚­ì œ\n    \"\"\"\n    try:\n        config = db.query(CollectionConfig).filter(\n            CollectionConfig.id == config_id\n        ).first()\n        \n        if not config:\n            raise HTTPException(status_code=404, detail=\"ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        db.delete(config)\n        db.commit()\n        \n        logger.info(f\"Deleted collection config: {config_id}\")\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"ì„¤ì • {config_id}ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤\"\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to delete collection config {config_id}: {e}\")\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì‚­ì œ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/{config_id}/cost-summary\")\nasync def get_cost_summary(\n    config_id: int,\n    db: Session = Depends(get_db)\n) -> Dict[str, Any]:\n    \"\"\"\n    íŠ¹ì • ì„¤ì •ì˜ ë¹„ìš© ìš”ì•½\n    \"\"\"\n    try:\n        config = db.query(CollectionConfig).filter(\n            CollectionConfig.id == config_id\n        ).first()\n        \n        if not config:\n            raise HTTPException(status_code=404, detail=\"ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        keyword_count = len(config.keywords) if config.keywords else 0\n        region_count = len(config.regions) if config.regions else 0\n        \n        return {\n            \"config_id\": config_id,\n            \"config_name\": config.name,\n            \"monthly_cost\": config.monthly_cost,\n            \"keyword_count\": keyword_count,\n            \"region_count\": region_count,\n            \"source\": config.source,\n            \"cost_per_keyword\": config.monthly_cost / keyword_count if keyword_count > 0 else 0,\n            \"is_active\": config.is_active\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to get cost summary for {config_id}: {e}\")\n        raise HTTPException(status_code=500, detail=f\"ë¹„ìš© ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n","size_bytes":8767},"data-hub/src/api/duplicate_routes.py":{"content":"\"\"\"\nì¤‘ë³µ ê²€ì‚¬ ë° í’ˆì§ˆ ê´€ë¦¬ API - Stage B\nExact Match + Fuzzy Match ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ì¤‘ë³µ íƒì§€\n\"\"\"\nfrom fastapi import APIRouter, HTTPException, Depends\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import func, desc\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nfrom pydantic import BaseModel\nimport uuid\nimport Levenshtein\n\nfrom src.database.connection import get_db\nfrom src.database.models import (\n    ProcessedRestaurant, \n    DuplicateGroup, \n    QualityScore\n)\n\n\nrouter = APIRouter(prefix=\"/api/data-management/duplicates\", tags=[\"duplicate-management\"])\n\n\nclass DuplicateCheckRequest(BaseModel):\n    \"\"\"ì¤‘ë³µ ê²€ì‚¬ ìš”ì²­ (Exact Match)\"\"\"\n    restaurant_ids: Optional[List[str]] = None  # Noneì´ë©´ ì „ì²´ ê²€ì‚¬\n\n\nclass FuzzyCheckRequest(BaseModel):\n    \"\"\"Fuzzy Match ì¤‘ë³µ ê²€ì‚¬ ìš”ì²­\"\"\"\n    restaurant_ids: Optional[List[str]] = None  # Noneì´ë©´ ì „ì²´ ê²€ì‚¬\n    threshold: float = 85.0  # ìœ ì‚¬ë„ ì„ê³„ê°’ (0-100), ê¸°ë³¸ 85%\n\n\nclass DuplicateGroupResponse(BaseModel):\n    \"\"\"ì¤‘ë³µ ê·¸ë£¹ ì‘ë‹µ\"\"\"\n    id: int\n    restaurant_ids: List[str]\n    match_type: str\n    similarity_score: float\n    status: str\n    created_at: str\n    resolved_at: Optional[str] = None\n    resolved_by: Optional[str] = None\n\n\nclass MergeRequest(BaseModel):\n    \"\"\"ì¤‘ë³µ ë³‘í•© ìš”ì²­\"\"\"\n    master_id: str  # ìœ ì§€í•  ë ˆìŠ¤í† ë‘ ID\n    reason: Optional[str] = \"User merged duplicates\"\n\n\nclass QualityScoreResponse(BaseModel):\n    \"\"\"í’ˆì§ˆ ì ìˆ˜ ì‘ë‹µ\"\"\"\n    restaurant_id: str\n    completeness_score: float\n    phone_valid: bool\n    address_complete: bool\n    coordinates_valid: bool\n    total_score: float\n\n\ndef calculate_exact_match_score(r1: Dict[str, Any], r2: Dict[str, Any]) -> float:\n    \"\"\"\n    Exact Match ì•Œê³ ë¦¬ì¦˜: ì´ë¦„, ì£¼ì†Œ, ì „í™”ë²ˆí˜¸ ì™„ì „ ì¼ì¹˜ ê²€ì‚¬\n    \n    Returns:\n        100.0: ì™„ì „ ì¼ì¹˜\n        0.0: ë¶ˆì¼ì¹˜\n    \"\"\"\n    # ì´ë¦„ ë¹„êµ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì œê±°)\n    name1 = str(r1.get('name', '')).strip().lower().replace(' ', '')\n    name2 = str(r2.get('name', '')).strip().lower().replace(' ', '')\n    \n    if not name1 or not name2:\n        return 0.0\n    \n    if name1 != name2:\n        return 0.0\n    \n    # ì£¼ì†Œ ë¹„êµ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì œê±°)\n    addr1 = str(r1.get('address', '')).strip().lower().replace(' ', '')\n    addr2 = str(r2.get('address', '')).strip().lower().replace(' ', '')\n    \n    # ì „í™”ë²ˆí˜¸ ë¹„êµ (ìˆ«ìë§Œ ì¶”ì¶œ)\n    phone1 = ''.join(filter(str.isdigit, str(r1.get('phone', ''))))\n    phone2 = ''.join(filter(str.isdigit, str(r2.get('phone', ''))))\n    \n    # ì£¼ì†Œ ë˜ëŠ” ì „í™”ë²ˆí˜¸ ì¤‘ í•˜ë‚˜ë¼ë„ ì¼ì¹˜í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨\n    address_match = addr1 == addr2 and len(addr1) > 0\n    phone_match = phone1 == phone2 and len(phone1) >= 10\n    \n    if address_match or phone_match:\n        return 100.0\n    \n    return 0.0\n\n\ndef normalize_korean_text(text: str) -> str:\n    \"\"\"\n    í•œê¸€ í…ìŠ¤íŠ¸ ì •ê·œí™”\n    - ê³µë°± ì œê±°\n    - ì†Œë¬¸ì ë³€í™˜\n    - íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì¼ë¶€)\n    \"\"\"\n    if not text:\n        return \"\"\n    \n    # ê³µë°± ì œê±°\n    text = text.strip().replace(' ', '')\n    \n    # ì†Œë¬¸ì ë³€í™˜\n    text = text.lower()\n    \n    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ê´„í˜¸, ì  ë“±)\n    text = text.replace('(', '').replace(')', '').replace('.', '').replace(',', '')\n    \n    return text\n\n\ndef calculate_fuzzy_match_score(r1: Dict[str, Any], r2: Dict[str, Any]) -> float:\n    \"\"\"\n    Fuzzy Match ì•Œê³ ë¦¬ì¦˜: Levenshtein distance ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬\n    \n    Returns:\n        0.0-100.0: ìœ ì‚¬ë„ ì ìˆ˜ (%)\n    \"\"\"\n    # ì´ë¦„ ë¹„êµ\n    name1 = normalize_korean_text(str(r1.get('name', '')))\n    name2 = normalize_korean_text(str(r2.get('name', '')))\n    \n    if not name1 or not name2:\n        return 0.0\n    \n    # Levenshtein ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°\n    # ratio() í•¨ìˆ˜: 0.0 (ì™„ì „ ë‹¤ë¦„) ~ 1.0 (ì™„ì „ ê°™ìŒ)\n    name_similarity = Levenshtein.ratio(name1, name2) * 100\n    \n    # ì£¼ì†Œ ë¹„êµ\n    addr1 = normalize_korean_text(str(r1.get('address', '')))\n    addr2 = normalize_korean_text(str(r2.get('address', '')))\n    \n    addr_similarity = 0.0\n    if addr1 and addr2:\n        addr_similarity = Levenshtein.ratio(addr1, addr2) * 100\n    \n    # ì „í™”ë²ˆí˜¸ ë¹„êµ (ìˆ«ìë§Œ ì¶”ì¶œ)\n    phone1 = ''.join(filter(str.isdigit, str(r1.get('phone', ''))))\n    phone2 = ''.join(filter(str.isdigit, str(r2.get('phone', ''))))\n    \n    phone_similarity = 0.0\n    if phone1 and phone2 and len(phone1) >= 10 and len(phone2) >= 10:\n        phone_similarity = Levenshtein.ratio(phone1, phone2) * 100\n    \n    # ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°\n    # ê°€ì¤‘ì¹˜: ì´ë¦„ 50%, ì£¼ì†Œ 30%, ì „í™”ë²ˆí˜¸ 20%\n    total_similarity = (\n        name_similarity * 0.5 +\n        addr_similarity * 0.3 +\n        phone_similarity * 0.2\n    )\n    \n    return round(total_similarity, 2)\n\n\ndef calculate_quality_score(restaurant: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n    \n    Returns:\n        {\n            'completeness_score': float,  # 0-100\n            'phone_valid': bool,\n            'address_complete': bool,\n            'coordinates_valid': bool,\n            'total_score': float  # 0-100\n        }\n    \"\"\"\n    score = {\n        'completeness_score': 0.0,\n        'phone_valid': False,\n        'address_complete': False,\n        'coordinates_valid': False,\n        'total_score': 0.0\n    }\n    \n    # í•„ìˆ˜ í•„ë“œ ì²´í¬\n    required_fields = ['name', 'address', 'phone']\n    filled_required = sum(1 for field in required_fields if restaurant.get(field))\n    score['completeness_score'] = (filled_required / len(required_fields)) * 100\n    \n    # ì „í™”ë²ˆí˜¸ ìœ íš¨ì„± (10ìë¦¬ ì´ìƒ ìˆ«ì)\n    phone = ''.join(filter(str.isdigit, str(restaurant.get('phone', ''))))\n    score['phone_valid'] = len(phone) >= 10\n    \n    # ì£¼ì†Œ ì™„ì„±ë„ (10ì ì´ìƒ)\n    address = str(restaurant.get('address', '')).strip()\n    score['address_complete'] = len(address) >= 10\n    \n    # ì¢Œí‘œ ìœ íš¨ì„±\n    lat = restaurant.get('latitude')\n    lng = restaurant.get('longitude')\n    if lat and lng:\n        try:\n            lat_f = float(lat)\n            lng_f = float(lng)\n            # í•œêµ­ ì¢Œí‘œ ë²”ìœ„: ìœ„ë„ 33-39, ê²½ë„ 124-132\n            score['coordinates_valid'] = (33 <= lat_f <= 39 and 124 <= lng_f <= 132)\n        except:\n            score['coordinates_valid'] = False\n    \n    # ì¢…í•© ì ìˆ˜ ê³„ì‚°\n    total = 0\n    total += score['completeness_score'] * 0.4  # 40%\n    total += 20 if score['phone_valid'] else 0  # 20%\n    total += 20 if score['address_complete'] else 0  # 20%\n    total += 20 if score['coordinates_valid'] else 0  # 20%\n    \n    score['total_score'] = min(100, total)\n    \n    return score\n\n\n@router.post(\"/check\")\nasync def check_duplicates(\n    request: DuplicateCheckRequest = DuplicateCheckRequest(),\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    ì¤‘ë³µ ê²€ì‚¬ ì‹¤í–‰ (Exact Match)\n    \n    - restaurant_idsê°€ Noneì´ë©´ ì „ì²´ ë ˆìŠ¤í† ë‘ ê²€ì‚¬\n    - Exact Match: ì´ë¦„ + (ì£¼ì†Œ ë˜ëŠ” ì „í™”ë²ˆí˜¸) ì™„ì „ ì¼ì¹˜\n    \"\"\"\n    try:\n        # ê²€ì‚¬ ëŒ€ìƒ ë ˆìŠ¤í† ë‘ ì¡°íšŒ\n        query = db.query(ProcessedRestaurant)\n        if request.restaurant_ids:\n            query = query.filter(ProcessedRestaurant.id.in_(request.restaurant_ids))\n        \n        restaurants = query.all()\n        \n        if len(restaurants) < 2:\n            return {\n                \"message\": \"ê²€ì‚¬ ëŒ€ìƒì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 2ê°œ í•„ìš”)\",\n                \"total_checked\": len(restaurants),\n                \"duplicates_found\": 0,\n                \"groups_created\": 0\n            }\n        \n        # ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°\n        duplicate_groups = []\n        checked_ids = set()\n        \n        for i, r1 in enumerate(restaurants):\n            if r1.id in checked_ids:\n                continue\n            \n            r1_data = {\n                'name': r1.name,\n                'address': r1.address,\n                'phone': r1.phone\n            }\n            \n            group_members = [r1.id]\n            \n            for r2 in restaurants[i+1:]:\n                if r2.id in checked_ids:\n                    continue\n                \n                r2_data = {\n                    'name': r2.name,\n                    'address': r2.address,\n                    'phone': r2.phone\n                }\n                \n                similarity = calculate_exact_match_score(r1_data, r2_data)\n                \n                if similarity == 100.0:\n                    group_members.append(r2.id)\n                    checked_ids.add(r2.id)\n            \n            if len(group_members) >= 2:\n                duplicate_groups.append({\n                    'restaurant_ids': group_members,\n                    'similarity_score': 100.0\n                })\n                checked_ids.add(r1.id)\n        \n        # DBì— ì¤‘ë³µ ê·¸ë£¹ ì €ì¥\n        groups_created = 0\n        for group_data in duplicate_groups:\n            # ê¸°ì¡´ ê·¸ë£¹ í™•ì¸ (ê°™ì€ ë ˆìŠ¤í† ë‘ ì¡°í•©)\n            existing = db.query(DuplicateGroup).filter(\n                DuplicateGroup.restaurant_ids == group_data['restaurant_ids'],\n                DuplicateGroup.status == 'pending'\n            ).first()\n            \n            if not existing:\n                new_group = DuplicateGroup(\n                    restaurant_ids=group_data['restaurant_ids'],\n                    match_type='exact',\n                    similarity_score=group_data['similarity_score'],\n                    status='pending'\n                )\n                db.add(new_group)\n                groups_created += 1\n        \n        db.commit()\n        \n        return {\n            \"message\": \"ì¤‘ë³µ ê²€ì‚¬ ì™„ë£Œ\",\n            \"total_checked\": len(restaurants),\n            \"duplicates_found\": sum(len(g['restaurant_ids']) for g in duplicate_groups),\n            \"groups_created\": groups_created,\n            \"duplicate_groups\": duplicate_groups\n        }\n    \n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/check-fuzzy\")\nasync def check_fuzzy_duplicates(\n    request: FuzzyCheckRequest = FuzzyCheckRequest(),\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    Fuzzy Match ì¤‘ë³µ ê²€ì‚¬ ì‹¤í–‰\n    \n    - Levenshtein distance ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°\n    - threshold (ê¸°ë³¸ 85%) ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨\n    - ê°€ì¤‘ì¹˜: ì´ë¦„ 50%, ì£¼ì†Œ 30%, ì „í™”ë²ˆí˜¸ 20%\n    \"\"\"\n    try:\n        # ê²€ì‚¬ ëŒ€ìƒ ë ˆìŠ¤í† ë‘ ì¡°íšŒ\n        query = db.query(ProcessedRestaurant)\n        if request.restaurant_ids:\n            query = query.filter(ProcessedRestaurant.id.in_(request.restaurant_ids))\n        \n        restaurants = query.all()\n        \n        if len(restaurants) < 2:\n            return {\n                \"message\": \"ê²€ì‚¬ ëŒ€ìƒì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 2ê°œ í•„ìš”)\",\n                \"total_checked\": len(restaurants),\n                \"duplicates_found\": 0,\n                \"groups_created\": 0,\n                \"threshold\": request.threshold\n            }\n        \n        # ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°\n        duplicate_groups = []\n        checked_ids = set()\n        \n        for i, r1 in enumerate(restaurants):\n            if r1.id in checked_ids:\n                continue\n            \n            r1_data = {\n                'name': r1.name,\n                'address': r1.address,\n                'phone': r1.phone\n            }\n            \n            group_members = [r1.id]\n            max_similarity = 0.0\n            \n            for r2 in restaurants[i+1:]:\n                if r2.id in checked_ids:\n                    continue\n                \n                r2_data = {\n                    'name': r2.name,\n                    'address': r2.address,\n                    'phone': r2.phone\n                }\n                \n                similarity = calculate_fuzzy_match_score(r1_data, r2_data)\n                \n                if similarity >= request.threshold:\n                    group_members.append(r2.id)\n                    checked_ids.add(r2.id)\n                    max_similarity = max(max_similarity, similarity)\n            \n            if len(group_members) >= 2:\n                duplicate_groups.append({\n                    'restaurant_ids': group_members,\n                    'similarity_score': max_similarity\n                })\n                checked_ids.add(r1.id)\n        \n        # DBì— ì¤‘ë³µ ê·¸ë£¹ ì €ì¥\n        groups_created = 0\n        for group_data in duplicate_groups:\n            # ê¸°ì¡´ ê·¸ë£¹ í™•ì¸ (ê°™ì€ ë ˆìŠ¤í† ë‘ ì¡°í•©)\n            existing = db.query(DuplicateGroup).filter(\n                DuplicateGroup.restaurant_ids == group_data['restaurant_ids'],\n                DuplicateGroup.status == 'pending'\n            ).first()\n            \n            if not existing:\n                new_group = DuplicateGroup(\n                    restaurant_ids=group_data['restaurant_ids'],\n                    match_type='fuzzy',\n                    similarity_score=group_data['similarity_score'],\n                    status='pending'\n                )\n                db.add(new_group)\n                groups_created += 1\n        \n        db.commit()\n        \n        return {\n            \"message\": \"Fuzzy Match ì¤‘ë³µ ê²€ì‚¬ ì™„ë£Œ\",\n            \"total_checked\": len(restaurants),\n            \"duplicates_found\": sum(len(g['restaurant_ids']) for g in duplicate_groups),\n            \"groups_created\": groups_created,\n            \"threshold\": request.threshold,\n            \"duplicate_groups\": duplicate_groups\n        }\n    \n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"Fuzzy Match ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"\")\nasync def get_duplicate_groups(\n    status: Optional[str] = None,\n    match_type: Optional[str] = None,\n    limit: int = 50,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ì¤‘ë³µ ê·¸ë£¹ ëª©ë¡ ì¡°íšŒ\"\"\"\n    try:\n        query = db.query(DuplicateGroup)\n        \n        if status:\n            query = query.filter(DuplicateGroup.status == status)\n        \n        if match_type:\n            query = query.filter(DuplicateGroup.match_type == match_type)\n        \n        groups = query.order_by(desc(DuplicateGroup.created_at)).limit(limit).all()\n        \n        return [\n            {\n                \"id\": g.id,\n                \"restaurant_ids\": g.restaurant_ids,\n                \"match_type\": g.match_type,\n                \"similarity_score\": g.similarity_score,\n                \"status\": g.status,\n                \"created_at\": g.created_at.isoformat() if g.created_at else None,\n                \"resolved_at\": g.resolved_at.isoformat() if g.resolved_at else None,\n                \"resolved_by\": g.resolved_by\n            }\n            for g in groups\n        ]\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/{group_id}\")\nasync def get_duplicate_group(group_id: int, db: Session = Depends(get_db)):\n    \"\"\"ì¤‘ë³µ ê·¸ë£¹ ìƒì„¸ ì¡°íšŒ\"\"\"\n    try:\n        group = db.query(DuplicateGroup).filter(DuplicateGroup.id == group_id).first()\n        \n        if not group:\n            raise HTTPException(status_code=404, detail=\"ì¤‘ë³µ ê·¸ë£¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # ë ˆìŠ¤í† ë‘ ì •ë³´ ì¡°íšŒ\n        restaurants = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.id.in_(group.restaurant_ids)\n        ).all()\n        \n        return {\n            \"id\": group.id,\n            \"restaurant_ids\": group.restaurant_ids,\n            \"match_type\": group.match_type,\n            \"similarity_score\": group.similarity_score,\n            \"status\": group.status,\n            \"created_at\": group.created_at.isoformat() if group.created_at else None,\n            \"resolved_at\": group.resolved_at.isoformat() if group.resolved_at else None,\n            \"resolved_by\": group.resolved_by,\n            \"restaurants\": [\n                {\n                    \"id\": r.id,\n                    \"name\": r.name,\n                    \"address\": r.address,\n                    \"phone\": r.phone,\n                    \"source\": r.source\n                }\n                for r in restaurants\n            ]\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/{group_id}/merge\")\nasync def merge_duplicates(\n    group_id: int,\n    request: MergeRequest,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ì¤‘ë³µ ë³‘í•© (master ë ˆìŠ¤í† ë‘ë§Œ ìœ ì§€)\"\"\"\n    try:\n        group = db.query(DuplicateGroup).filter(DuplicateGroup.id == group_id).first()\n        \n        if not group:\n            raise HTTPException(status_code=404, detail=\"ì¤‘ë³µ ê·¸ë£¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        if group.status != 'pending':\n            raise HTTPException(status_code=400, detail=\"ì´ë¯¸ ì²˜ë¦¬ëœ ê·¸ë£¹ì…ë‹ˆë‹¤\")\n        \n        if request.master_id not in group.restaurant_ids:\n            raise HTTPException(status_code=400, detail=\"master_idê°€ ê·¸ë£¹ì— ì—†ìŠµë‹ˆë‹¤\")\n        \n        # ì¤‘ë³µ ê·¸ë£¹ ìƒíƒœ ì—…ë°ì´íŠ¸\n        group.status = 'merged'\n        group.resolved_at = datetime.utcnow()\n        group.resolved_by = request.master_id\n        \n        # TODO: ì‹¤ì œ ë³‘í•© ë¡œì§ (Stage Bì—ì„œ êµ¬í˜„)\n        # - ì¤‘ë³µ ë ˆìŠ¤í† ë‘ ë°ì´í„° ì‚­ì œ ë˜ëŠ” ë¹„í™œì„±í™”\n        # - masterì— ë°ì´í„° ë³‘í•©\n        \n        db.commit()\n        \n        return {\n            \"message\": \"ì¤‘ë³µì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"group_id\": group.id,\n            \"master_id\": request.master_id,\n            \"merged_count\": len(group.restaurant_ids) - 1\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ë³‘í•© ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/{group_id}/separate\")\nasync def separate_duplicates(\n    group_id: int,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ì¤‘ë³µì´ ì•„ë‹˜ (ë³„ê°œ ë ˆìŠ¤í† ë‘ìœ¼ë¡œ í‘œì‹œ)\"\"\"\n    try:\n        group = db.query(DuplicateGroup).filter(DuplicateGroup.id == group_id).first()\n        \n        if not group:\n            raise HTTPException(status_code=404, detail=\"ì¤‘ë³µ ê·¸ë£¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        if group.status != 'pending':\n            raise HTTPException(status_code=400, detail=\"ì´ë¯¸ ì²˜ë¦¬ëœ ê·¸ë£¹ì…ë‹ˆë‹¤\")\n        \n        # ì¤‘ë³µ ê·¸ë£¹ ìƒíƒœ ì—…ë°ì´íŠ¸\n        group.status = 'separated'\n        group.resolved_at = datetime.utcnow()\n        group.resolved_by = 'user'\n        \n        db.commit()\n        \n        return {\n            \"message\": \"ë³„ê°œ ë ˆìŠ¤í† ë‘ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"group_id\": group.id\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/{group_id}/ignore\")\nasync def ignore_duplicate(\n    group_id: int,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ì¤‘ë³µ ê·¸ë£¹ ë¬´ì‹œ\"\"\"\n    try:\n        group = db.query(DuplicateGroup).filter(DuplicateGroup.id == group_id).first()\n        \n        if not group:\n            raise HTTPException(status_code=404, detail=\"ì¤‘ë³µ ê·¸ë£¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        group.status = 'ignored'\n        group.resolved_at = datetime.utcnow()\n        \n        db.commit()\n        \n        return {\n            \"message\": \"ì¤‘ë³µ ê·¸ë£¹ì´ ë¬´ì‹œë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"group_id\": group.id\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n\n\nclass QualityCalculateRequest(BaseModel):\n    \"\"\"í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ìš”ì²­\"\"\"\n    restaurant_ids: Optional[List[str]] = None\n\n\n@router.post(\"/quality/calculate\")\nasync def calculate_quality_scores(\n    request: QualityCalculateRequest = QualityCalculateRequest(),\n    db: Session = Depends(get_db)\n):\n    \"\"\"í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ë° ì €ì¥\"\"\"\n    try:\n        # ëŒ€ìƒ ë ˆìŠ¤í† ë‘ ì¡°íšŒ\n        query = db.query(ProcessedRestaurant)\n        if request.restaurant_ids:\n            query = query.filter(ProcessedRestaurant.id.in_(request.restaurant_ids))\n        \n        restaurants = query.all()\n        \n        if not restaurants:\n            return {\n                \"message\": \"ê³„ì‚° ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤\",\n                \"total_calculated\": 0\n            }\n        \n        calculated = 0\n        for restaurant in restaurants:\n            r_data = {\n                'name': restaurant.name,\n                'address': restaurant.address,\n                'phone': restaurant.phone,\n                'latitude': restaurant.latitude,\n                'longitude': restaurant.longitude\n            }\n            \n            score_data = calculate_quality_score(r_data)\n            \n            # ê¸°ì¡´ ì ìˆ˜ í™•ì¸\n            existing = db.query(QualityScore).filter(\n                QualityScore.restaurant_id == restaurant.id\n            ).first()\n            \n            if existing:\n                # ì—…ë°ì´íŠ¸\n                existing.completeness_score = score_data['completeness_score']\n                existing.phone_valid = score_data['phone_valid']\n                existing.address_complete = score_data['address_complete']\n                existing.coordinates_valid = score_data['coordinates_valid']\n                existing.total_score = score_data['total_score']\n            else:\n                # ì‹ ê·œ ìƒì„±\n                new_score = QualityScore(\n                    restaurant_id=restaurant.id,\n                    completeness_score=score_data['completeness_score'],\n                    phone_valid=score_data['phone_valid'],\n                    address_complete=score_data['address_complete'],\n                    coordinates_valid=score_data['coordinates_valid'],\n                    total_score=score_data['total_score']\n                )\n                db.add(new_score)\n            \n            calculated += 1\n        \n        db.commit()\n        \n        return {\n            \"message\": \"í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ\",\n            \"total_calculated\": calculated\n        }\n    \n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ê³„ì‚° ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/quality/scores\")\nasync def get_quality_scores(\n    min_score: Optional[float] = None,\n    limit: int = 50,\n    db: Session = Depends(get_db)\n):\n    \"\"\"í’ˆì§ˆ ì ìˆ˜ ëª©ë¡ ì¡°íšŒ\"\"\"\n    try:\n        query = db.query(QualityScore)\n        \n        if min_score is not None:\n            query = query.filter(QualityScore.total_score >= min_score)\n        \n        scores = query.order_by(desc(QualityScore.total_score)).limit(limit).all()\n        \n        return [\n            {\n                \"id\": s.id,\n                \"restaurant_id\": s.restaurant_id,\n                \"completeness_score\": s.completeness_score,\n                \"phone_valid\": s.phone_valid,\n                \"address_complete\": s.address_complete,\n                \"coordinates_valid\": s.coordinates_valid,\n                \"total_score\": s.total_score,\n                \"created_at\": s.created_at.isoformat() if s.created_at else None\n            }\n            for s in scores\n        ]\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n","size_bytes":23563},"attached_assets/replit_implementation_prompt_1762585701813.md":{"content":"# Data Hub ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ: Replit êµ¬í˜„ ê¸´ê¸‰ í”„ë¡¬í”„íŠ¸\n\n**ëª©í‘œ**: 4ë‹¨ê³„ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•  \n**ì˜ˆìƒ ì†Œìš”**: 20-24ì‹œê°„  \n**ìš°ì„ ìˆœìœ„**: ğŸ”´ **ê¸´ê¸‰ (ë¹„ì¦ˆë‹ˆìŠ¤ í•µì‹¬)**\n\n---\n\n## ğŸ“‹ Replitì—ê²Œ ë³´ë‚¼ ë§ˆìŠ¤í„° í”„ë¡¬í”„íŠ¸\n\n```\ní•œì‹ë‹¹ Data Hubì˜ í•µì‹¬ ê¸°ëŠ¥: \"ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ\" êµ¬ì¶•\n\n**ë‹¹ì‹ ì´ ë†“ì¹œ ê²ƒ:**\ní˜„ì¬ ëŒ€ì‹œë³´ë“œëŠ” \"ì‘ì—… ì‹¤í–‰ ë²„íŠ¼\"ë§Œ ìˆì§€ë§Œ,\nê´€ë¦¬ìê°€ ì‹¤ì œë¡œ í•„ìš”í•œ ê²ƒì€\n\"ì „ì²´ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ì˜ ì™„ì „í•œ í†µì œ\"ì…ë‹ˆë‹¤.\n\n**êµ¬ì²´ì  ë¬¸ì œ:**\n1. ë§¤ì¼ ì–´ë–¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í• ì§€ ì„¤ì •í•  ìˆ˜ ì—†ìŒ\n2. ë¹„ìš©ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë“œëŠ”ì§€ ë³¼ ìˆ˜ ì—†ìŒ\n3. ì¤‘ë³µ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ì—†ìŒ\n4. ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì œëŒ€ë¡œ ì•±ì— ë™ê¸°í™”ë˜ëŠ”ì§€ ëª¨ë¦„\n\n**í•„ìš”í•œ í•´ê²°ì±…:**\n4ë‹¨ê³„ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•\n\n---\n\n## Phase 1: ìˆ˜ì§‘ í‚¤ì›Œë“œ ê´€ë¦¬ (8ì‹œê°„)\n\n### 1-1. í˜ì´ì§€: /dashboard/data-management/collection-keywords\n\n**í•„ìˆ˜ ê¸°ëŠ¥:**\n\n1. ìˆ˜ì§‘ ì„¤ì • ëª©ë¡ í˜ì´ì§€\n   - ì„¤ì •ëª…, ì§€ì—­, í‚¤ì›Œë“œ, ìœ í˜•, ìƒíƒœ í‘œì‹œ\n   - [í¸ì§‘] [ë³µì œ] [ì¼ì‹œì •ì§€] [ì‚­ì œ] [ìƒì„¸] ë²„íŠ¼\n   - ìƒíƒœ í•„í„° (í™œì„±/ëŒ€ê¸°/ìŠ¹ì¸ëŒ€ê¸°/ê±°ì ˆ)\n   - ì›”ê°„ ì˜ˆìƒ ë¹„ìš© ìë™ ê³„ì‚° ë° í‘œì‹œ\n   - í˜ì´ì§€ë„¤ì´ì…˜\n\n2. ìƒˆ ì„¤ì • ì¶”ê°€ ëª¨ë‹¬\n   ì…ë ¥ í•­ëª©:\n   - ì„¤ì •ëª… (í…ìŠ¤íŠ¸)\n   - ìˆ˜ì§‘ ì§€ì—­ (ë‹¤ì¤‘ ì„ íƒ: ì‹œ/ë„ â†’ êµ¬/êµ° â†’ ë™)\n   - ê²€ìƒ‰ í‚¤ì›Œë“œ (ë‹¤ì¤‘ ì¶”ê°€ ê°€ëŠ¥)\n   - í¬ë¡¤ë§ ì†ŒìŠ¤ (Google/Naver/ë‘˜ ë‹¤)\n   - ì›”ê°„ ì˜ˆìƒ ë¹„ìš© ìë™ ê³„ì‚°\n   - ìŠ¤ì¼€ì¤„ (ë§¤ì¼/ì£¼ê°„/ì›”ê°„ + ì‹œê°„)\n   - ìƒíƒœ (ì¦‰ì‹œ í™œì„± / ìŠ¹ì¸ ëŒ€ê¸°)\n   \n   ì €ì¥ ì‹œ:\n   - DBì— ì €ì¥\n   - ë¹„ìš© ì¶”ì  ì‹œì‘\n   - ìŠ¹ì¸ ëŒ€ê¸°ì¸ ê²½ìš° ê´€ë¦¬ì ì•Œë¦¼\n\n3. ì„¤ì • ìƒì„¸/í¸ì§‘ í˜ì´ì§€\n   - ëª¨ë“  ì •ë³´ í‘œì‹œ\n   - ì‹¤í–‰ ì´ë ¥ (ìµœê·¼ 10íšŒ)\n   - ê° ì‹¤í–‰ì˜ ë¹„ìš© ê¸°ë¡\n   - [ìˆ˜ì •] [í™œì„±í™”/ë¹„í™œì„±í™”] [ì‚­ì œ]\n\n### 1-2. API ì—”ë“œí¬ì¸íŠ¸:\n\n```\nPOST /api/data-management/collection-configs\nGET /api/data-management/collection-configs\nGET /api/data-management/collection-configs/{id}\nPUT /api/data-management/collection-configs/{id}\nDELETE /api/data-management/collection-configs/{id}\nGET /api/data-management/collection-configs/{id}/history\n```\n\n**DB ìŠ¤í‚¤ë§ˆ:**\n```\ncollection_configs:\n- id\n- name (ì„¤ì •ëª…)\n- regions (JSON: ë‹¤ì¤‘ ì§€ì—­)\n- keywords (JSON: ë‹¤ì¤‘ í‚¤ì›Œë“œ)\n- source (google/naver/both)\n- schedule (ë§¤ì¼/ì£¼ê°„/ì›”ê°„)\n- schedule_time (HH:MM)\n- status (active/paused/pending/rejected)\n- monthly_cost (ê³„ì‚°ë¨)\n- created_at\n- updated_at\n- approved_by (null or admin_id)\n- approval_date\n```\n\n---\n\n## Phase 2: í¬ë¡¤ë§ ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© í†µì œ (8ì‹œê°„)\n\n### 2-1. í˜ì´ì§€: /dashboard/data-management/crawling-jobs\n\n**í•„ìˆ˜ ê¸°ëŠ¥:**\n\n1. ë¹„ìš© í˜„í™© ëŒ€ì‹œë³´ë“œ\n   - ì´ë‹¬ ì‚¬ìš©ì•¡\n   - ì›” ì˜ˆì‚°\n   - ì‚¬ìš©ë¥  (%)\n   - ë‚¨ì€ì•¡ìˆ˜\n   - ì˜ˆìƒ ì›”ë§ ì‚¬ìš©ì•¡\n\n2. ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ëª©ë¡\n   - ì‘ì—…ëª…\n   - ìƒíƒœ (ì‹¤í–‰ì¤‘/ëŒ€ê¸°/ì™„ë£Œ/ì‹¤íŒ¨)\n   - ì§„í–‰ë¥  (%)\n   - ì‹¤ì‹œê°„ ë¹„ìš© ê³„ì‚° ë° í‘œì‹œ\n   - ì‹œì‘ ì‹œê°„, ì˜ˆìƒ ì™„ë£Œ ì‹œê°„\n   - [ì¤‘ì§€] [ë¡œê·¸] [ìƒì„¸] ë²„íŠ¼\n\n3. ë¹„ìš© ê²½ê³  ì‹œìŠ¤í…œ\n   - ì˜ˆìƒ ë¹„ìš© ì´ˆê³¼ ì‹œ ê²½ê³  í‘œì‹œ\n   - ì›ì¸ ë¶„ì„\n   - [ì„¤ì • ìˆ˜ì •] [ì¼ì‹œì •ì§€] ì˜µì…˜\n\n### 2-2. í˜ì´ì§€: /dashboard/data-management/crawling-jobs/{job_id}\n\n**í‘œì‹œ ë‚´ìš©:**\n- ê¸°ë³¸ ì •ë³´ (ì„¤ì •ëª…, ì†ŒìŠ¤, ì§€ì—­, í‚¤ì›Œë“œ)\n- ì‹¤í–‰ ì •ë³´ (ì‹œì‘ì‹œê°„, ìƒíƒœ, ì†Œìš”ì‹œê°„)\n- ë¹„ìš© ì •ë³´ (ì˜ˆìƒ vs ì‹¤ì œ, ì´ˆê³¼ ì´ìœ )\n- ìˆ˜ì§‘ ê²°ê³¼ (ì´/ì‹ ê·œ/ì¤‘ë³µê²½ê³ /í’ˆì§ˆì ìˆ˜)\n- ìƒì„¸ ë¡œê·¸ (íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜)\n- [ğŸ“Š ê²°ê³¼ ë°ì´í„°] [ğŸ”„ ì¬ì‹¤í–‰] ë²„íŠ¼\n\n### 2-3. API ì—”ë“œí¬ì¸íŠ¸:\n\n```\nGET /api/data-management/crawling-jobs\nGET /api/data-management/crawling-jobs/{job_id}\nPOST /api/data-management/crawling-jobs/{job_id}/stop\nGET /api/data-management/crawling-jobs/{job_id}/logs\nPOST /api/data-management/cost-control/summary\n```\n\n---\n\n## Phase 3: ì¤‘ë³µ ê²€ì‚¬ ë° í’ˆì§ˆ ê²€ì¦ (6ì‹œê°„)\n\n### 3-1. í˜ì´ì§€: /dashboard/data-management/quality-check\n\n**í•„ìˆ˜ ê¸°ëŠ¥:**\n\n1. ìµœê·¼ ìˆ˜ì§‘ ë°ì´í„° ê²€ì¦\n   - ìˆ˜ì§‘ ë‚ ì§œ ë° ê±´ìˆ˜\n   - ê²€ì¦ í˜„í™©:\n     â”œâ”€ ì‹ ê·œ (ì •ìƒ)\n     â”œâ”€ ì¤‘ë³µ ê²½ê³  (ìˆ˜ë™ ê²€í†  í•„ìš”)\n     â””â”€ í’ˆì§ˆ ì–‘í˜¸/ê²½ê³ \n\n2. ìë™ ì¤‘ë³µ ê²€ì‚¬ (3ë‹¨ê³„ êµ¬í˜„)\n   \n   Step 1: Exact Match (100%)\n   - name == name AND address == address AND phone == phone\n   - ìë™ ë³‘í•© ì œì•ˆ\n   \n   Step 2: Fuzzy Match (85%+)\n   - ì´ë¦„ ìœ ì‚¬ë„ + ì£¼ì†Œ ìœ ì‚¬ë„ + ì „í™”ë²ˆí˜¸\n   - ê´€ë¦¬ì ê²€í†  í•„ìš”\n   \n   Step 3: Geo Match\n   - ê°™ì€ ë™/ì/ë©´ ë‚´ ì¤‘ë³µ ì´ë¦„\n   - ë³„ê°œ ì—…ì†Œì¼ ê°€ëŠ¥ì„± í‘œì‹œ\n\n3. ê° ì¤‘ë³µ ê²½ê³ ë³„ë¡œ:\n   - ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµ\n   - ì¼ì¹˜ë„ % í‘œì‹œ\n   - ë‹¤ë¥¸ í•„ë“œ ê°•ì¡°\n   - [ë³‘í•©] [ë³„ê°œ] [ë¬´ì‹œ] ì„ íƒì§€\n\n4. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n   - í•„ìˆ˜ì •ë³´ ì™„ì„±ë„\n   - ì „í™”ë²ˆí˜¸ ìœ íš¨ì„±\n   - ì£¼ì†Œ ì™„ì„±ë„\n   - ì¢Œí‘œ ì •í™•ë„\n   - ì¢…í•© ì ìˆ˜\n\n5. ìŠ¹ì¸ ë˜ëŠ” ê±°ì ˆ\n   - [âœ… ëª¨ë‘ ìŠ¹ì¸] â†’ DBì— ì €ì¥\n   - [ğŸ”„ ì¼ë¶€ ìˆ˜ì •] â†’ í¸ì§‘ í˜ì´ì§€ë¡œ\n   - [âŒ ê±°ì ˆ] â†’ ì‚¬ìœ  ê¸°ë¡ í›„ ë°˜ë ¤\n\n### 3-2. í˜ì´ì§€: /dashboard/data-management/data-editor\n\n**í•„ìˆ˜ ê¸°ëŠ¥:**\n\n1. ìˆ˜ì§‘ëœ ë°ì´í„° ëª©ë¡\n   - í•„í„° (ìƒíƒœ, í’ˆì§ˆì ìˆ˜)\n   - ê²€ìƒ‰ (ì´ë¦„, ì£¼ì†Œ)\n   - ì •ë ¬ (ìµœì‹ ìˆœ, í‰ì ìˆœ)\n\n2. ê° ë°ì´í„°ë³„ [ìˆ˜ì •] í´ë¦­ ì‹œ ëª¨ë‹¬\n   í¸ì§‘ ê°€ëŠ¥ í•„ë“œ:\n   - ì´ë¦„\n   - ì£¼ì†Œ\n   - ì „í™”ë²ˆí˜¸\n   - í‰ì \n   - ë¦¬ë·°ìˆ˜\n   - ì„¤ëª…\n   - íƒœê·¸\n   \n   ì €ì¥ ì˜µì…˜:\n   - [ì €ì¥] â†’ ì €ì¥ë§Œ\n   - [ì €ì¥ í›„ ë‹¤ìŒ] â†’ ì €ì¥ + ë‹¤ìŒ í•­ëª©\n\n### 3-3. API ì—”ë“œí¬ì¸íŠ¸:\n\n```\nGET /api/data-management/recent-collections\nPOST /api/data-management/quality-check/{collection_id}\nGET /api/data-management/duplicate-check/{collection_id}\nPOST /api/data-management/duplicate-check/{duplicate_id}/resolve\nPUT /api/data-management/collected-data/{data_id}\nPOST /api/data-management/collections/{collection_id}/approve\n```\n\n---\n\n## Phase 4: ë™ê¸°í™” ê´€ë¦¬ (6ì‹œê°„)\n\n### 4-1. í˜ì´ì§€: /dashboard/data-management/sync-management\n\n**í•„ìˆ˜ ê¸°ëŠ¥:**\n\n1. ë™ê¸°í™” í˜„í™©\n   - ëŒ€ê¸° ì¤‘: Xê°œ\n   - ì§„í–‰ ì¤‘: Xê°œ\n   - ì™„ë£Œ: Xê°œ\n\n2. ê° ë°°ì¹˜ë³„ ìƒíƒœ\n   - ë°°ì¹˜ ë²ˆí˜¸\n   - ê±´ìˆ˜\n   - ì§„í–‰ë¥  (%)\n   - ì†Œìš”ì‹œê°„\n   - ìƒíƒœ (ì§„í–‰/ì™„ë£Œ/ì‹¤íŒ¨)\n   - ì™„ë£Œì‹œê°„\n\n3. ë°°ì¹˜ ê´€ë¦¬\n   - [ì‹œì‘] â†’ ë™ê¸°í™” ì‹œì‘\n   - [ì¤‘ì§€] â†’ ì§„í–‰ ì¤‘ë‹¨\n   - [ìƒì„¸] â†’ ìƒì„¸ ì •ë³´\n   - [ë¡œê·¸] â†’ ì‹¤ì‹œê°„ ë¡œê·¸\n   - [ì¬ë™ê¸°í™”] â†’ ì‹¤íŒ¨ê±´ ì¬ì‹œë„\n\n4. ë™ê¸°í™” í†µê³„\n   - ì´ ë™ê¸°í™” ê±´ìˆ˜ (ì´ë‹¬)\n   - ì„±ê³µë¥ \n   - ì‹¤íŒ¨ ê±´ìˆ˜ ë° ì›ì¸\n   - í‰ê·  ì†Œìš”ì‹œê°„\n\n### 4-2. API ì—”ë“œí¬ì¸íŠ¸:\n\n```\nGET /api/data-management/sync-status\nPOST /api/data-management/sync/batch\nGET /api/data-management/sync/batch/{batch_id}\nPOST /api/data-management/sync/batch/{batch_id}/start\nPOST /api/data-management/sync/batch/{batch_id}/stop\nGET /api/data-management/sync/batch/{batch_id}/logs\nPOST /api/data-management/sync/retry-failed\n```\n\n---\n\n## Phase 5: ë¹„ìš© í†µì œ ì‹œìŠ¤í…œ (2ì‹œê°„)\n\n### 5-1. í˜ì´ì§€: /dashboard/data-management/cost-control\n\n**í•„ìˆ˜ ê¸°ëŠ¥:**\n\n1. ì›”ê°„ ë¹„ìš© ìš”ì•½\n   - ì´ ì˜ˆì‚°\n   - ì‚¬ìš©ì•¡\n   - ì‚¬ìš©ë¥ \n   - ë‚¨ì€ì•¡ìˆ˜\n\n2. ë¹„ìš© ì¶”ì´ ê·¸ë˜í”„\n   - ìµœê·¼ 6ê°œì›” ì¶”ì´\n   - ì´ˆê³¼ ì—¬ë¶€ í‘œì‹œ\n\n3. ë¹„ìš© ë¶„ì„\n   - ì†ŒìŠ¤ë³„ ë¹„ìš© (Naver vs Google)\n   - ì„¤ì •ë³„ ë¹„ìš© ìƒìœ„ 10ê°œ\n   - ë¹„ìš© ì´ˆê³¼ ì„¤ì • ê°•ì¡°\n\n4. ë¹„ìš© ì´ˆê³¼ ëŒ€ì‘\n   - ìë™ ì˜ˆì¸¡ (í˜„ì¬ ì¶”ì´ëŒ€ë¡œ ì§„í–‰ ì‹œ)\n   - ì œì–´ ì˜µì…˜:\n     â”œâ”€ ì„¤ì • ì¼ì‹œì •ì§€\n     â”œâ”€ ìˆ˜ì§‘ ë¹ˆë„ ì¡°ì •\n     â”œâ”€ ë°°ì¹˜ í¬ê¸° ê°ì†Œ\n     â””â”€ ì˜ˆì‚° ìˆ˜ì •\n\n### 5-2. API ì—”ë“œí¬ì¸íŠ¸:\n\n```\nGET /api/data-management/cost/monthly-summary\nGET /api/data-management/cost/by-source\nGET /api/data-management/cost/by-config\nGET /api/data-management/cost/prediction\n```\n\n---\n\n## ğŸ¯ êµ¬í˜„ ìš°ì„ ìˆœìœ„\n\n### 1ìˆœìœ„: Phase 1 & 2 (ìˆ˜ì§‘ ì„¤ì • + ëª¨ë‹ˆí„°ë§)\n- ì‚¬ìš©ìê°€ ê°€ì¥ ìì£¼ ë³¼ í˜ì´ì§€\n- ë¹„ìš© í†µì œì˜ í•µì‹¬\n\n### 2ìˆœìœ„: Phase 3 (ì¤‘ë³µ ê²€ì‚¬ + í’ˆì§ˆ)\n- ë°ì´í„° í’ˆì§ˆì˜ í•µì‹¬\n- ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ë†’ìŒ\n\n### 3ìˆœìœ„: Phase 4 (ë™ê¸°í™”)\n- ìë™í™” ê°€ëŠ¥\n- ì¶”í›„ ê°œì„  ì—¬ì§€ ìˆìŒ\n\n### 4ìˆœìœ„: Phase 5 (ë¹„ìš© í†µì œ)\n- Phase 1-3ì´ ì™„ì„±ëœ í›„ í†µí•©\n\n---\n\n## ğŸ”§ ê¸°ìˆ  ìš”êµ¬ì‚¬í•­\n\n### Frontend (Vue.js)\n- ë³µì¡í•œ í¼ ì²˜ë¦¬\n- ì‹¤ì‹œê°„ ë°ì´í„° ë°”ì¸ë”©\n- í…Œì´ë¸” + í•„í„° + ì •ë ¬\n- ëª¨ë‹¬ ì‹œìŠ¤í…œ\n- ê·¸ë˜í”„ (Chart.js)\n- ì§„í–‰ë¥  ë°”\n\n### Backend (FastAPI)\n- CRUD ì—”ë“œí¬ì¸íŠ¸ (ë‹¤ì¤‘)\n- ì¤‘ë³µ ê²€ì‚¬ ë¡œì§ êµ¬í˜„\n- ë¹„ìš© ê³„ì‚° ë¡œì§\n- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ API\n- ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§\n\n### Database\n- ì„¤ì • ê´€ë¦¬ í…Œì´ë¸”\n- ì‘ì—… ì´ë ¥ í…Œì´ë¸”\n- ë¹„ìš© ì¶”ì  í…Œì´ë¸”\n- ìˆ˜ì§‘ ë°ì´í„° í…Œì´ë¸”\n\n---\n\n## â° ì˜ˆìƒ ì¼ì •\n\n```\nDay 1 (8ì‹œê°„): Phase 1 & 2 êµ¬í˜„\nDay 2 (8ì‹œê°„): Phase 3 êµ¬í˜„ + í†µí•©\nDay 3 (6ì‹œê°„): Phase 4 êµ¬í˜„ + í…ŒìŠ¤íŠ¸\nDay 4 (2ì‹œê°„): Phase 5 êµ¬í˜„ + ìµœì¢… í…ŒìŠ¤íŠ¸\n\nì´ 24ì‹œê°„ (ì•½ 3ì¼)\n```\n\n---\n\n## âœ… ì™„ì„± ê¸°ì¤€\n\n```\n[ ] ëª¨ë“  í˜ì´ì§€ ì •ìƒ ë¡œë“œ\n[ ] ëª¨ë“  CRUD ê¸°ëŠ¥ ì‘ë™\n[ ] ì¤‘ë³µ ê²€ì‚¬ 3ë‹¨ê³„ ëª¨ë‘ ì‘ë™\n[ ] ë¹„ìš© ê³„ì‚° ì •í™•\n[ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì •ìƒ\n[ ] ë™ê¸°í™” ë°°ì¹˜ ì²˜ë¦¬ ì •ìƒ\n[ ] ë¹„ìš© ì˜ˆì¸¡ ì •í™•\n[ ] ëª¨ë“  API í…ŒìŠ¤íŠ¸ í†µê³¼\n[ ] ì—ëŸ¬ ì²˜ë¦¬ ì™„ë²½\n[ ] ì„±ëŠ¥ ìµœì í™” (ì‘ë‹µ < 500ms)\n```\n\n---\n\n## ğŸ¯ ë‹¹ì‹ ì˜ íŒë‹¨\n\nì´ê²ƒì´:\n1. ë§ëŠ” ì„¤ê³„ì¸ê°€?\n2. ë” ë‚˜ì€ ë°©ì•ˆì´ ìˆëŠ”ê°€?\n3. ì‹¤í˜„ ê°€ëŠ¥í•œê°€?\n4. ì˜ˆìƒ ì‹œê°„ì´ ë§ëŠ”ê°€?\n\nì¦‰ì‹œ í”¼ë“œë°± ë¶€íƒí•©ë‹ˆë‹¤!\n```\n\n---\n\n## ğŸ“Š ìš”ì•½\n\n### ì´ ì‹œìŠ¤í…œì´ ì™„ì„±ë˜ë©´\n\n```\nâœ… ê´€ë¦¬ìê°€ ë§¤ì¼ì˜ ìˆ˜ì§‘ì„ í†µì œ ê°€ëŠ¥\nâœ… ë¹„ìš©ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ íˆ¬ëª…í•˜ê²Œ ê´€ë¦¬ë¨\nâœ… ì¤‘ë³µ ë°ì´í„° ìë™ ê²€ì‚¬ + ìˆ˜ë™ ìŠ¹ì¸\nâœ… ë°ì´í„° í’ˆì§ˆ ì™„ë²½ ê´€ë¦¬\nâœ… ë©”ì¸ ì•± ë™ê¸°í™” ì „ì²´ ì¶”ì \n\nâ†’ \"ì§„ì •í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ê´€ë¦¬ ì‹œìŠ¤í…œ\"\n```\n\n---\n\n**ë‹¹ì‹ ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì´ ëª¨ë‘ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!**\n\nì§€ê¸ˆ ë°”ë¡œ Replitì—ê²Œ ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ì„¸ìš”! ğŸš€\n\n","size_bytes":10106},"data-hub/src/api/deployment_routes.py":{"content":"\"\"\"\nStage C-5: ì„ íƒì  ë°°í¬ & ë™ê¸°í™” ì‹œìŠ¤í…œ\n- ìŠ¹ì¸ëœ ë ˆìŠ¤í† ë‘ë§Œ í•œì‹ë‹¹ í”Œë«í¼ì— ë°°í¬\n\"\"\"\n\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Any, Optional, List\nimport json\nfrom datetime import datetime\n\nrouter = APIRouter(prefix=\"/api/deployment\", tags=[\"Deployment\"])\n\n\nclass DeploymentRequest(BaseModel):\n    restaurant_ids: List[str]\n    deployment_type: str = \"immediate\"\n    notes: Optional[str] = None\n\n\ndef get_db():\n    from src.database.connection import get_db as _get_db\n    db_gen = _get_db()\n    db = next(db_gen)\n    try:\n        yield db\n    finally:\n        try:\n            next(db_gen)\n        except StopIteration:\n            pass\n\n\n@router.get(\"/candidates\")\nasync def get_deployment_candidates(\n    min_score: Optional[float] = None,\n    status: Optional[str] = \"approved\",\n    limit: int = 100,\n    db: Any = Depends(get_db)\n):\n    \"\"\"\n    C-5-1: ë°°í¬ ê°€ëŠ¥í•œ ë ˆìŠ¤í† ë‘ ëª©ë¡ ì¡°íšŒ\n    \"\"\"\n    from sqlalchemy import text\n    \n    try:\n        conditions = [\"edit_status = :status\"]\n        params = {\"status\": status, \"limit\": limit}\n        \n        if min_score:\n            conditions.append(\"popularity_score >= :min_score\")\n            params[\"min_score\"] = min_score\n        \n        where_clause = \" AND \".join(conditions)\n        \n        query = text(f\"\"\"\n        SELECT \n            id, name, category, address, rating, review_count,\n            popularity_score, quality_score, source, created_at\n        FROM collection_results\n        WHERE {where_clause}\n        ORDER BY popularity_score DESC\n        LIMIT :limit\n        \"\"\")\n        \n        result = db.execute(query, params)\n        rows = result.fetchall()\n        \n        data = []\n        for row in rows:\n            data.append({\n                \"id\": row[0],\n                \"name\": row[1],\n                \"category\": row[2],\n                \"address\": row[3],\n                \"rating\": float(row[4]) if row[4] else None,\n                \"review_count\": row[5],\n                \"popularity_score\": float(row[6]) if row[6] else None,\n                \"quality_score\": float(row[7]) if row[7] else None,\n                \"source\": row[8],\n                \"created_at\": row[9].isoformat() if row[9] else None\n            })\n        \n        return {\n            \"success\": True,\n            \"data\": data,\n            \"total\": len(data)\n        }\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n\n\n@router.post(\"/execute\")\nasync def execute_deployment(request: DeploymentRequest, db: Any = Depends(get_db)):\n    \"\"\"\n    C-5-2: ì„ íƒí•œ ë ˆìŠ¤í† ë‘ì„ í•œì‹ë‹¹ í”Œë«í¼ì— ë°°í¬\n    \"\"\"\n    from sqlalchemy import text\n    \n    try:\n        deployed_count = 0\n        failed_count = 0\n        \n        for restaurant_id in request.restaurant_ids:\n            try:\n                update_query = text(\"\"\"\n                UPDATE collection_results\n                SET edit_status = 'deployed',\n                    updated_at = :updated_at\n                WHERE id = :id AND edit_status = 'approved'\n                RETURNING id\n                \"\"\")\n                \n                result = db.execute(update_query, {\n                    \"id\": restaurant_id,\n                    \"updated_at\": datetime.utcnow()\n                })\n                \n                row = result.fetchone()\n                if row:\n                    deployed_count += 1\n                else:\n                    failed_count += 1\n            \n            except Exception as e:\n                failed_count += 1\n                continue\n        \n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"data\": {\n                \"total_selected\": len(request.restaurant_ids),\n                \"deployed\": deployed_count,\n                \"failed\": failed_count,\n                \"deployment_type\": request.deployment_type,\n                \"timestamp\": datetime.utcnow().isoformat()\n            },\n            \"message\": f\"{deployed_count}ê°œ ë ˆìŠ¤í† ë‘ì´ ì„±ê³µì ìœ¼ë¡œ ë°°í¬ë˜ì—ˆìŠµë‹ˆë‹¤\"\n        }\n    \n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ë°°í¬ ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n\n\n@router.get(\"/history\")\nasync def get_deployment_history(\n    days: int = 7,\n    limit: int = 50,\n    db: Any = Depends(get_db)\n):\n    \"\"\"\n    C-5-3: ë°°í¬ ì´ë ¥ ì¡°íšŒ\n    \"\"\"\n    from sqlalchemy import text\n    \n    try:\n        query = text(\"\"\"\n        SELECT \n            id, name, category, popularity_score, \n            edit_status, updated_at\n        FROM collection_results\n        WHERE edit_status = 'deployed'\n            AND updated_at >= NOW() - INTERVAL ':days days'\n        ORDER BY updated_at DESC\n        LIMIT :limit\n        \"\"\")\n        \n        result = db.execute(query, {\"days\": days, \"limit\": limit})\n        rows = result.fetchall()\n        \n        data = []\n        for row in rows:\n            data.append({\n                \"id\": row[0],\n                \"name\": row[1],\n                \"category\": row[2],\n                \"popularity_score\": float(row[3]) if row[3] else None,\n                \"status\": row[4],\n                \"deployed_at\": row[5].isoformat() if row[5] else None\n            })\n        \n        return {\n            \"success\": True,\n            \"data\": data,\n            \"total\": len(data)\n        }\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n\n\n@router.get(\"/stats\")\nasync def get_deployment_stats(db: Any = Depends(get_db)):\n    \"\"\"\n    ë°°í¬ í†µê³„\n    \"\"\"\n    from sqlalchemy import text\n    \n    try:\n        stats_query = text(\"\"\"\n        SELECT \n            COUNT(*) FILTER (WHERE edit_status = 'approved') as ready_to_deploy,\n            COUNT(*) FILTER (WHERE edit_status = 'deployed') as deployed,\n            COUNT(*) FILTER (WHERE edit_status = 'pending') as pending,\n            AVG(popularity_score) FILTER (WHERE edit_status = 'deployed') as avg_deployed_score\n        FROM collection_results\n        \"\"\")\n        \n        result = db.execute(stats_query)\n        row = result.fetchone()\n        \n        return {\n            \"success\": True,\n            \"data\": {\n                \"ready_to_deploy\": row[0] or 0,\n                \"deployed\": row[1] or 0,\n                \"pending\": row[2] or 0,\n                \"avg_deployed_score\": float(row[3]) if row[3] else 0.0\n            }\n        }\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n","size_bytes":6769},"data-hub/migration_simplify_schema.py":{"content":"\"\"\"\nDB ìŠ¤í‚¤ë§ˆ ê°„ì†Œí™” ë§ˆì´ê·¸ë ˆì´ì…˜\n- ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°\n- links í•„ë“œ í†µí•© (google_maps_url, naver_map_url ë“±)\n- ê¸°ì¡´ ë°ì´í„° ë³€í™˜\n\"\"\"\n\nimport asyncpg\nimport asyncio\nimport json\nimport os\nfrom datetime import datetime\n\nasync def migrate():\n    # DB ì—°ê²° (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)\n    database_url = os.getenv('DATABASE_URL')\n    if not database_url:\n        print(\"âŒ DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤\")\n        return\n    \n    conn = await asyncpg.connect(database_url)\n    \n    try:\n        print(\"=\" * 60)\n        print(\"ğŸš€ DB ìŠ¤í‚¤ë§ˆ ê°„ì†Œí™” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘\")\n        print(\"=\" * 60)\n        print(f\"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        print(\"=\" * 60)\n        \n        # Step 1: ê¸°ì¡´ ë°ì´í„° ë°±ì—…\n        print(\"\\nğŸ“¦ Step 1: ê¸°ì¡´ ë°ì´í„° ë°±ì—…...\")\n        existing_data = await conn.fetch(\"\"\"\n            SELECT * FROM collection_results\n        \"\"\")\n        print(f\"   âœ“ {len(existing_data)}ê°œ ë°ì´í„° ë°±ì—… ì™„ë£Œ\")\n        \n        # Step 2: ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€ (links)\n        print(\"\\nğŸ”§ Step 2: ì‹ ê·œ ì»¬ëŸ¼ ì¶”ê°€...\")\n        await conn.execute(\"\"\"\n            ALTER TABLE collection_results\n            ADD COLUMN IF NOT EXISTS links JSONB DEFAULT '{}'::jsonb\n        \"\"\")\n        print(\"   âœ“ links ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ\")\n        \n        # collection_request_id ì»¬ëŸ¼ ì¶”ê°€ (request_idë¥¼ ëŒ€ì²´)\n        await conn.execute(\"\"\"\n            ALTER TABLE collection_results\n            ADD COLUMN IF NOT EXISTS collection_request_id VARCHAR\n        \"\"\")\n        print(\"   âœ“ collection_request_id ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ\")\n        \n        # description ì»¬ëŸ¼ ì¶”ê°€ (ìˆëŠ”ì§€ í™•ì¸ í›„)\n        await conn.execute(\"\"\"\n            ALTER TABLE collection_results\n            ADD COLUMN IF NOT EXISTS description TEXT\n        \"\"\")\n        print(\"   âœ“ description ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ\")\n        \n        # Step 3: ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‹ ê·œ êµ¬ì¡°ë¡œ ë³€í™˜\n        print(\"\\nğŸ”„ Step 3: ë°ì´í„° ë³€í™˜ ì¤‘...\")\n        converted_count = 0\n        for row in existing_data:\n            links = {}\n            \n            # Google ë§í¬\n            if row.get('google_maps_url'):\n                links['google'] = row['google_maps_url']\n            \n            # Naver ë§í¬\n            if row.get('naver_map_url'):\n                links['naver'] = row['naver_map_url']\n            \n            # ê¸°íƒ€ source_urlì´ ìˆìœ¼ë©´ otherë¡œ ì¶”ê°€\n            if row.get('source_url'):\n                links['other'] = row['source_url']\n            \n            # request_idë¥¼ collection_request_idë¡œ ë³µì‚¬\n            collection_request_id = row.get('request_id')\n            \n            # links ë° collection_request_id ì—…ë°ì´íŠ¸\n            await conn.execute(\"\"\"\n                UPDATE collection_results\n                SET links = $1,\n                    collection_request_id = $2\n                WHERE id = $3\n            \"\"\", json.dumps(links), collection_request_id, row['id'])\n            \n            converted_count += 1\n        \n        print(f\"   âœ“ {converted_count}ê°œ ë°ì´í„° ë³€í™˜ ì™„ë£Œ\")\n        \n        # Step 4: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì‚­ì œ\n        print(\"\\nğŸ—‘ï¸  Step 4: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì‚­ì œ...\")\n        columns_to_drop = [\n            # ì§€ì‹œì„œì— ëª…ì‹œëœ ì‚­ì œ ëŒ€ìƒ\n            'price_range',\n            'latitude',\n            'longitude',\n            'popularity_score',\n            'thumbnail_url',\n            'images',\n            'youtube_mention_count',\n            'blog_mention_count',\n            'facilities',\n            'google_place_id',\n            'naver_place_id',\n            'google_maps_url',\n            'naver_map_url',\n            'source_url',\n            'quality_score',\n            # ì¶”ê°€ ì ìˆ˜ ê´€ë ¨ í•„ë“œë“¤\n            'popularity_tier',\n            'rating_score',\n            'review_score',\n            'youtube_score',\n            'blog_score',\n            # ê¸°íƒ€ ë¶ˆí•„ìš” í•„ë“œ\n            'request_id',  # collection_request_idë¡œ ëŒ€ì²´\n            'source_data',\n            'edit_status',\n            'is_validated',\n            'is_duplicate',\n            'duplicate_group_id',\n            'created_by',\n            'edited_by',\n            'edited_at'\n        ]\n        \n        dropped_count = 0\n        for col in columns_to_drop:\n            try:\n                await conn.execute(f\"\"\"\n                    ALTER TABLE collection_results\n                    DROP COLUMN IF EXISTS {col}\n                \"\"\")\n                print(f\"   âœ“ {col} ì‚­ì œ ì™„ë£Œ\")\n                dropped_count += 1\n            except Exception as e:\n                print(f\"   âš ï¸  {col} ì‚­ì œ ì‹¤íŒ¨: {e}\")\n        \n        print(f\"\\n   ì´ {dropped_count}ê°œ ì»¬ëŸ¼ ì‚­ì œ ì™„ë£Œ\")\n        \n        # Step 5: ìµœì¢… ìŠ¤í‚¤ë§ˆ í™•ì¸\n        print(\"\\nğŸ“Š Step 5: ìµœì¢… ìŠ¤í‚¤ë§ˆ í™•ì¸...\")\n        final_schema = await conn.fetch(\"\"\"\n            SELECT column_name, data_type\n            FROM information_schema.columns\n            WHERE table_name = 'collection_results'\n            ORDER BY ordinal_position\n        \"\"\")\n        \n        print(\"\\n   ìµœì¢… ì»¬ëŸ¼ ëª©ë¡:\")\n        for i, col in enumerate(final_schema, 1):\n            print(f\"   {i:2d}. {col['column_name']:30s} - {col['data_type']}\")\n        \n        print(f\"\\n   ì´ {len(final_schema)}ê°œ ì»¬ëŸ¼\")\n        \n        # Step 6: ë°ì´í„° ê²€ì¦\n        print(\"\\nâœ… Step 6: ë°ì´í„° ê²€ì¦...\")\n        final_count = await conn.fetchval(\"SELECT COUNT(*) FROM collection_results\")\n        print(f\"   âœ“ ìµœì¢… ë°ì´í„° ê°œìˆ˜: {final_count}ê°œ\")\n        \n        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n        sample = await conn.fetchrow(\"\"\"\n            SELECT id, name, category, links, collection_request_id\n            FROM collection_results\n            LIMIT 1\n        \"\"\")\n        if sample:\n            print(f\"\\n   ìƒ˜í”Œ ë°ì´í„°:\")\n            print(f\"   - ID: {sample['id']}\")\n            print(f\"   - ì´ë¦„: {sample['name']}\")\n            print(f\"   - ì¹´í…Œê³ ë¦¬: {sample['category']}\")\n            print(f\"   - ë§í¬: {sample['links']}\")\n            print(f\"   - ìˆ˜ì§‘ ìš”ì²­ ID: {sample['collection_request_id']}\")\n        \n        print(\"\\n\" + \"=\" * 60)\n        print(\"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!\")\n        print(\"=\" * 60)\n        print(f\"ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        print(\"=\" * 60)\n        \n    except Exception as e:\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}\")\n        print(\"=\" * 60)\n        import traceback\n        traceback.print_exc()\n        raise\n    \n    finally:\n        await conn.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(migrate())\n","size_bytes":6763},"data-hub/src/api/batch_sync_routes.py":{"content":"\"\"\"\në°°ì¹˜ ë™ê¸°í™” ê´€ë¦¬ API - Stage B (B-4)\nì„ íƒí•œ ë ˆìŠ¤í† ë‘ ë°°ì¹˜ ë™ê¸°í™” ë° ì´ë ¥ ê´€ë¦¬\n\"\"\"\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import desc, func\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime, timedelta\nfrom pydantic import BaseModel\n\nfrom src.database.connection import get_db\nfrom src.database.models import SyncLog, ProcessedRestaurant\nfrom src.workflows.sync import SyncWorkflow\n\n\nrouter = APIRouter(prefix=\"/api/data-management/sync\", tags=[\"batch-sync\"])\n\n\nclass BatchSyncRequest(BaseModel):\n    \"\"\"ë°°ì¹˜ ë™ê¸°í™” ìš”ì²­\"\"\"\n    restaurant_ids: List[str]\n    batch_name: Optional[str] = None\n\n\nclass SyncHistoryFilter(BaseModel):\n    \"\"\"ë™ê¸°í™” ì´ë ¥ í•„í„°\"\"\"\n    days: int = 7\n    status: Optional[str] = None\n    limit: int = 50\n\n\n@router.post(\"/batch\")\nasync def sync_batch(\n    request: BatchSyncRequest,\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    ì„ íƒí•œ ë ˆìŠ¤í† ë‘ ë°°ì¹˜ ë™ê¸°í™”\n    \n    - restaurant_ids: ë™ê¸°í™”í•  ë ˆìŠ¤í† ë‘ ID ëª©ë¡\n    - batch_name: ë°°ì¹˜ ì´ë¦„ (ì„ íƒ)\n    \"\"\"\n    try:\n        if not request.restaurant_ids:\n            raise HTTPException(status_code=400, detail=\"ë ˆìŠ¤í† ë‘ IDê°€ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # ë ˆìŠ¤í† ë‘ ì¡°íšŒ\n        restaurants = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.id.in_(request.restaurant_ids)\n        ).all()\n        \n        if not restaurants:\n            return {\n                \"status\": \"error\",\n                \"message\": \"ì„ íƒí•œ ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\",\n                \"synced_count\": 0\n            }\n        \n        # ë™ê¸°í™” ê°€ëŠ¥í•œ ë ˆìŠ¤í† ë‘ë§Œ í•„í„° (pending ìƒíƒœ)\n        pending_restaurants = [r for r in restaurants if r.sync_status == 'pending']\n        \n        if not pending_restaurants:\n            return {\n                \"status\": \"warning\",\n                \"message\": \"ë™ê¸°í™”í•  ë ˆìŠ¤í† ë‘ì´ ì—†ìŠµë‹ˆë‹¤ (ì´ë¯¸ ë™ê¸°í™”ë¨)\",\n                \"total_selected\": len(restaurants),\n                \"already_synced\": len(restaurants)\n            }\n        \n        # ë™ê¸°í™” ì‹¤í–‰\n        workflow = SyncWorkflow()\n        \n        # ë°°ì¹˜ ì´ë¦„ì´ ìˆìœ¼ë©´ ë¡œê·¸ì— ê¸°ë¡ (í–¥í›„ í™•ì¥ ê°€ëŠ¥)\n        batch_info = {\n            \"batch_name\": request.batch_name or f\"Manual Batch {datetime.now().strftime('%Y%m%d_%H%M')}\",\n            \"restaurant_count\": len(pending_restaurants)\n        }\n        \n        # í•œì‹ë‹¹ í”Œë«í¼ìœ¼ë¡œ ë™ê¸°í™”\n        await workflow.sync_to_hansikdang()\n        \n        # ìµœì‹  ë™ê¸°í™” ë¡œê·¸ ì¡°íšŒ\n        latest_log = db.query(SyncLog).order_by(\n            SyncLog.started_at.desc()\n        ).first()\n        \n        # ë™ê¸°í™”ëœ ë ˆìŠ¤í† ë‘ ìˆ˜ ì¹´ìš´íŠ¸\n        synced_count = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.id.in_(request.restaurant_ids),\n            ProcessedRestaurant.sync_status == 'synced'\n        ).count()\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"{synced_count}ê°œ ë ˆìŠ¤í† ë‘ ë™ê¸°í™” ì™„ë£Œ\",\n            \"batch_info\": batch_info,\n            \"total_selected\": len(request.restaurant_ids),\n            \"pending_count\": len(pending_restaurants),\n            \"synced_count\": synced_count,\n            \"log\": {\n                \"id\": latest_log.id,\n                \"status\": latest_log.status,\n                \"total_sent\": latest_log.total_sent,\n                \"success_count\": latest_log.success_count,\n                \"error_count\": latest_log.error_count,\n                \"started_at\": latest_log.started_at.isoformat() if latest_log.started_at else None,\n                \"completed_at\": latest_log.completed_at.isoformat() if latest_log.completed_at else None\n            } if latest_log else None\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ë°°ì¹˜ ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/history\")\nasync def get_sync_history(\n    days: int = 7,\n    status: Optional[str] = None,\n    limit: int = 50,\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    ë™ê¸°í™” ì´ë ¥ ì¡°íšŒ\n    \n    - days: ì¡°íšŒ ê¸°ê°„ (ê¸°ë³¸ 7ì¼)\n    - status: ìƒíƒœ í•„í„° (running, completed, failed)\n    - limit: ìµœëŒ€ ê°œìˆ˜\n    \"\"\"\n    try:\n        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°\n        start_date = datetime.now() - timedelta(days=days)\n        \n        # ì¿¼ë¦¬ êµ¬ì„±\n        query = db.query(SyncLog).filter(\n            SyncLog.started_at >= start_date\n        )\n        \n        if status:\n            query = query.filter(SyncLog.status == status)\n        \n        # ìµœì‹ ìˆœ ì •ë ¬\n        logs = query.order_by(desc(SyncLog.started_at)).limit(limit).all()\n        \n        # í†µê³„ ê³„ì‚°\n        total_synced = sum(log.success_count or 0 for log in logs)\n        total_failed = sum(log.error_count or 0 for log in logs)\n        \n        return {\n            \"status\": \"success\",\n            \"period\": {\n                \"days\": days,\n                \"start_date\": start_date.isoformat(),\n                \"end_date\": datetime.now().isoformat()\n            },\n            \"summary\": {\n                \"total_runs\": len(logs),\n                \"total_synced\": total_synced,\n                \"total_failed\": total_failed,\n                \"success_rate\": round((total_synced / (total_synced + total_failed) * 100) if (total_synced + total_failed) > 0 else 0, 2)\n            },\n            \"history\": [\n                {\n                    \"id\": log.id,\n                    \"status\": log.status,\n                    \"total_sent\": log.total_sent,\n                    \"success_count\": log.success_count,\n                    \"error_count\": log.error_count,\n                    \"started_at\": log.started_at.isoformat() if log.started_at else None,\n                    \"completed_at\": log.completed_at.isoformat() if log.completed_at else None,\n                    \"duration\": (log.completed_at - log.started_at).total_seconds() if (log.completed_at and log.started_at) else None,\n                    \"error_details\": log.error_details\n                }\n                for log in logs\n            ]\n        }\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/restaurants\")\nasync def get_syncable_restaurants(\n    status: Optional[str] = 'pending',\n    min_quality: int = 0,\n    limit: int = 100,\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    ë™ê¸°í™” ê°€ëŠ¥í•œ ë ˆìŠ¤í† ë‘ ëª©ë¡ ì¡°íšŒ\n    \n    - status: ë™ê¸°í™” ìƒíƒœ (pending, synced, failed)\n    - min_quality: ìµœì†Œ í’ˆì§ˆ ì ìˆ˜\n    - limit: ìµœëŒ€ ê°œìˆ˜\n    \"\"\"\n    try:\n        query = db.query(ProcessedRestaurant)\n        \n        if status:\n            query = query.filter(ProcessedRestaurant.sync_status == status)\n        \n        if min_quality > 0:\n            query = query.filter(ProcessedRestaurant.quality_score >= min_quality)\n        \n        restaurants = query.order_by(\n            desc(ProcessedRestaurant.quality_score)\n        ).limit(limit).all()\n        \n        # í†µê³„\n        total_pending = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.sync_status == 'pending'\n        ).count()\n        \n        total_synced = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.sync_status == 'synced'\n        ).count()\n        \n        return {\n            \"status\": \"success\",\n            \"stats\": {\n                \"total_pending\": total_pending,\n                \"total_synced\": total_synced,\n                \"showing\": len(restaurants),\n                \"filter\": {\n                    \"status\": status,\n                    \"min_quality\": min_quality\n                }\n            },\n            \"restaurants\": [\n                {\n                    \"id\": r.id,\n                    \"name\": r.name,\n                    \"district\": r.district,\n                    \"address\": r.address,\n                    \"phone\": r.phone,\n                    \"quality_score\": r.quality_score,\n                    \"sync_status\": r.sync_status,\n                    \"synced_at\": r.synced_at.isoformat() if r.synced_at else None,\n                    \"created_at\": r.created_at.isoformat() if r.created_at else None\n                }\n                for r in restaurants\n            ]\n        }\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/stats/summary\")\nasync def get_sync_summary(db: Session = Depends(get_db)):\n    \"\"\"\n    ë™ê¸°í™” í†µê³„ ìš”ì•½\n    \n    - ì „ì²´ ë ˆìŠ¤í† ë‘ ìˆ˜\n    - ë™ê¸°í™” ìƒíƒœë³„ ê°œìˆ˜\n    - ìµœê·¼ ë™ê¸°í™” ì´ë ¥\n    \"\"\"\n    try:\n        # ì „ì²´ í†µê³„\n        total = db.query(ProcessedRestaurant).count()\n        pending = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.sync_status == 'pending'\n        ).count()\n        synced = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.sync_status == 'synced'\n        ).count()\n        failed = db.query(ProcessedRestaurant).filter(\n            ProcessedRestaurant.sync_status == 'failed'\n        ).count()\n        \n        # ìµœê·¼ 24ì‹œê°„ í†µê³„\n        last_24h = datetime.now() - timedelta(hours=24)\n        recent_logs = db.query(SyncLog).filter(\n            SyncLog.started_at >= last_24h\n        ).all()\n        \n        synced_24h = sum(log.success_count or 0 for log in recent_logs)\n        failed_24h = sum(log.error_count or 0 for log in recent_logs)\n        \n        # ìµœì‹  ë™ê¸°í™”\n        latest_log = db.query(SyncLog).order_by(\n            desc(SyncLog.started_at)\n        ).first()\n        \n        return {\n            \"status\": \"success\",\n            \"overview\": {\n                \"total_restaurants\": total,\n                \"pending\": pending,\n                \"synced\": synced,\n                \"failed\": failed,\n                \"sync_rate\": round((synced / total * 100) if total > 0 else 0, 2)\n            },\n            \"last_24h\": {\n                \"synced\": synced_24h,\n                \"failed\": failed_24h,\n                \"runs\": len(recent_logs)\n            },\n            \"latest_sync\": {\n                \"id\": latest_log.id,\n                \"status\": latest_log.status,\n                \"total_sent\": latest_log.total_sent,\n                \"success_count\": latest_log.success_count,\n                \"error_count\": latest_log.error_count,\n                \"started_at\": latest_log.started_at.isoformat() if latest_log.started_at else None,\n                \"completed_at\": latest_log.completed_at.isoformat() if latest_log.completed_at else None\n            } if latest_log else None\n        }\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n","size_bytes":10897},"attached_assets/ui-ux-emergency-improvement_1762677867632.md":{"content":"# ğŸ”´ Data Hub UI/UX ê¸´ê¸‰ ê°œì„  ì§€ì‹œì„œ (v1.0)\n\n**ì‘ì„±ì¼**: 2025ë…„ 11ì›” 9ì¼  \n**í‰ê°€**: í˜„ì¬ UIëŠ” ìš´ì˜ ë¶ˆê°€ëŠ¥ ìˆ˜ì¤€  \n**ëª©í‘œ**: ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ Data Hubë¡œ ì „í™˜  \n**ì˜ˆìƒ ì‹œê°„**: 12-15ì‹œê°„ (2-3ì¼)\n\n---\n\n## ğŸ“Š í˜„ì¬ í™”ë©´ ë¶„ì„ ê²°ê³¼\n\n### í™”ë©´ 1: ë©”ì¸ ëŒ€ì‹œë³´ë“œ (`/dashboard`)\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ (4ê°œ í•­ëª©)\n- ì–´ì œ ìˆ˜ì§‘ í†µê³„ (4ê°œ)\n- ë°±ì—… ìƒíƒœ í‘œì‹œ\n- ì‘ì—… ì‹¤í–‰ ì„¼í„° (7ê°œ ë²„íŠ¼)\n- 7ì¼ ìˆ˜ì§‘ ì¶”ì´ ì°¨íŠ¸\n\nâŒ ë¬¸ì œì :\n1. ì •ë³´ê°€ ë„ˆë¬´ ë§ìŒ (í•œ í™”ë©´ì— ëª¨ë“  ê²ƒ)\n2. ìš°ì„ ìˆœìœ„ê°€ ë¶ˆëª…í™•\n3. ì‹¤ì œ ì•¡ì…˜ì´ ì–´ë µì›€\n4. ì‹œê°í™” ë‹¨ì¡°ë¡œì›€\n```\n\n#### ê°œì„  ë°©í–¥\n```\n1. ëŒ€ì‹œë³´ë“œëŠ” \"í•œëˆˆì— ë³´ëŠ” ìš”ì•½\" ì—­í• ì— ì§‘ì¤‘\n2. ìƒì„¸ ê¸°ëŠ¥ì€ ê° ë©”ë‰´ë¡œ ì´ë™\n3. ìµœê·¼ í™œë™/ì•Œë¦¼ ì¶”ê°€\n4. ë¹ ë¥¸ ì•¡ì…˜ ë²„íŠ¼ ìµœì†Œí™” (3ê°œë§Œ)\n```\n\n---\n\n### í™”ë©´ 2: ë°ì´í„° ê´€ë¦¬ > ìˆ˜ì§‘ ì„¤ì • (`/dashboard/data-management`)\n\n#### ğŸš¨ ì‹¬ê°í•œ ë¬¸ì œ ë°œê²¬\n\n```\ní˜„ì¬ ìƒí™©:\nâŒ \"ë°ì´í„° ê´€ë¦¬\" í•˜ìœ„ ë©”ë‰´ 3ê°œê°€ ëª¨ë‘ ë™ì¼í•œ í™”ë©´!\n   - ìˆ˜ì§‘ ì„¤ì •\n   - ìˆ˜ì§‘ ê²°ê³¼\n   - ì¤‘ë³µ ê²€ì‚¬\n   - í’ˆì§ˆ ê´€ë¦¬\n   \nâŒ ë©”ë‰´ êµ¬ì¡°ê°€ ë…¼ë¦¬ì ì´ì§€ ì•ŠìŒ\nâŒ ê¸°ëŠ¥ì´ ì„ì—¬ìˆìŒ (ìˆ˜ì§‘/ê²°ê³¼/ê²€ì‚¬/í’ˆì§ˆ)\nâŒ ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ëª¨ë¦„\n\ní‰ê°€: \"ì™„ì „íˆ ì¬ì„¤ê³„ í•„ìš”\" ğŸ”´\n```\n\n#### ì˜¬ë°”ë¥¸ ë©”ë‰´ êµ¬ì¡°\n\n```\nğŸ“Š ëŒ€ì‹œë³´ë“œ\n   â””â”€ ìš”ì•½ + ìµœê·¼ í™œë™\n\nğŸ” ë°ì´í„° ìˆ˜ì§‘ (NEW - ë¶„ë¦¬ í•„ìš”!)\n   â”œâ”€ ìˆ˜ì§‘ ìš”ì²­ (ê¸°ì¡´ collection-request)\n   â”‚   â”œâ”€ ìƒˆ ìš”ì²­ ìƒì„±\n   â”‚   â”œâ”€ ì§„í–‰ ì¤‘ì¸ ì‘ì—…\n   â”‚   â””â”€ ì™„ë£Œëœ ì‘ì—…\n   â”‚\n   â””â”€ ìˆ˜ì§‘ ê²°ê³¼ (ê¸°ì¡´ collection-results)\n       â”œâ”€ ì „ì²´ ê²°ê³¼ ëª©ë¡\n       â”œâ”€ ì¤‘ë³µ ê²€ì‚¬ (íƒ­/ê¸°ëŠ¥ìœ¼ë¡œ í†µí•©)\n       â”œâ”€ í’ˆì§ˆ ê´€ë¦¬ (íƒ­/ê¸°ëŠ¥ìœ¼ë¡œ í†µí•©)\n       â””â”€ í•„í„°/ì •ë ¬/ë‚´ë³´ë‚´ê¸°\n\nâœï¸ ë°ì´í„° ì…ë ¥ (NEW - ì™„ì „íˆ ìƒˆë¡œ ë§Œë“¤ì–´ì•¼!)\n   â”œâ”€ URL ì…ë ¥ (Google/Naver/Kakao)\n   â”œâ”€ CSV ì—…ë¡œë“œ\n   â””â”€ ì§ì ‘ ì…ë ¥\n\nğŸ”§ ë°ì´í„° í¸ì§‘\n   â”œâ”€ í†µí•© í¸ì§‘\n   â””â”€ ë³‘í•© ê´€ë¦¬\n\nğŸš€ ë°°í¬ ê´€ë¦¬\n   â””â”€ ë°°í¬ ëŒ€ìƒ ì„ íƒ/ì‹¤í–‰\n```\n\n---\n\n### í™”ë©´ 3: ë ˆìŠ¤í† ë‘ ìƒì„¸ ì •ë³´ (íŒì—…)\n\n#### ğŸš¨ ì¹˜ëª…ì ì¸ ëˆ„ë½ ì‚¬í•­\n\n```\ní˜„ì¬:\nâœ… ê¸°ë³¸ ì •ë³´: ì´ë¦„, ì¹´í…Œê³ ë¦¬, ì£¼ì†Œ, ì „í™”\nâœ… í‰ì /ë¦¬ë·°: Google, í‰ì , ë¦¬ë·° ìˆ˜\nâœ… í•œê¸€/ì˜ë¬¸ ì„¤ëª…\n\nâŒ ì‹¬ê°í•œ ëˆ„ë½:\n1. ë©”ë‰´ ì •ë³´ ì™„ì „ ëˆ„ë½ (ê°€ì¥ ì¤‘ìš”!)\n2. Google/Naver Place ID ì—†ìŒ\n3. ì›ë³¸ ë§í¬ ì—†ìŒ (ë°”ë¡œê°€ê¸° ë¶ˆê°€)\n4. ì˜ì—… ì‹œê°„ ì—†ìŒ\n5. ê°€ê²©ëŒ€ ì •ë³´ ë¶€ì¡±\n6. ì‚¬ì§„ ê°¤ëŸ¬ë¦¬ ì—†ìŒ\n7. í¸ì˜ì‹œì„¤ ì •ë³´ ì—†ìŒ\n\ní‰ê°€: \"ë ˆìŠ¤í† ë‘ ì •ë³´ì˜ í•µì‹¬ì´ ë¹ ì§\" ğŸ”´\n```\n\n---\n\n## ğŸ¯ ê¸´ê¸‰ ê°œì„  ìš°ì„ ìˆœìœ„\n\n### Priority 1: ì¹˜ëª…ì  (ì¦‰ì‹œ ìˆ˜ì •, 4-5ì‹œê°„)\n\n#### 1-1. ë©”ë‰´ êµ¬ì¡° ì¬ì„¤ê³„ (2ì‹œê°„)\n\n```markdown\n**ì‘ì—…**: ì¢Œì¸¡ ì‚¬ì´ë“œë°” ë©”ë‰´ ì¬êµ¬ì¡°í™”\n\nBefore:\nğŸ“Š ëŒ€ì‹œë³´ë“œ\nğŸ” ë°ì´í„° ê´€ë¦¬ (ë™ì¼ í™”ë©´ 4ê°œ âŒ)\n   â”œâ”€ ìˆ˜ì§‘ ì„¤ì •\n   â”œâ”€ ìˆ˜ì§‘ ê²°ê³¼\n   â”œâ”€ ì¤‘ë³µ ê²€ì‚¬\n   â””â”€ í’ˆì§ˆ ê´€ë¦¬\n\nAfter:\nğŸ“Š ëŒ€ì‹œë³´ë“œ\nğŸ” ë°ì´í„° ìˆ˜ì§‘\n   â”œâ”€ ìˆ˜ì§‘ ìš”ì²­\n   â””â”€ ìˆ˜ì§‘ ê²°ê³¼\nâœï¸ ë°ì´í„° ì…ë ¥ (NEW)\n   â”œâ”€ URL ì…ë ¥\n   â”œâ”€ CSV ì—…ë¡œë“œ\n   â””â”€ ì§ì ‘ ì…ë ¥\nğŸ”§ ë°ì´í„° í¸ì§‘\n   â””â”€ í†µí•© í¸ì§‘\nğŸš€ ë°°í¬ ê´€ë¦¬\n\nêµ¬í˜„:\n1. sidebar HTML ìˆ˜ì •\n2. ê° ë©”ë‰´ì˜ ë¼ìš°íŒ… ìˆ˜ì •\n3. ë©”ë‰´ í™œì„±í™” ìƒíƒœ êµ¬í˜„\n```\n\n#### 1-2. ë ˆìŠ¤í† ë‘ ìƒì„¸ ì •ë³´ í•µì‹¬ í•„ë“œ ì¶”ê°€ (3ì‹œê°„)\n\n```markdown\n**ì‘ì—…**: íŒì—…ì— ëˆ„ë½ëœ í•„ìˆ˜ ì •ë³´ ì¶”ê°€\n\nì¶”ê°€í•  í•„ë“œ:\n\n1. ë©”ë‰´ ì •ë³´ (ê°€ì¥ ì¤‘ìš”!)\n   â”œâ”€ menu_items (JSONB)\n   â”‚   â””â”€ {name: \"ê¹€ì¹˜ì°Œê°œ\", price: \"8000\", description: \"...\"}\n   â””â”€ UI: ë©”ë‰´ ëª©ë¡ ì„¹ì…˜ ì¶”ê°€\n\n2. ì›ë³¸ ë§í¬ & Place ID\n   â”œâ”€ google_place_id\n   â”œâ”€ naver_place_id\n   â”œâ”€ google_maps_url\n   â”œâ”€ naver_map_url\n   â””â”€ UI: \"Googleì—ì„œ ë³´ê¸°\", \"ë„¤ì´ë²„ì—ì„œ ë³´ê¸°\" ë²„íŠ¼\n\n3. ì˜ì—… ì‹œê°„\n   â”œâ”€ business_hours (JSONB)\n   â””â”€ UI: ìš”ì¼ë³„ ì˜ì—…ì‹œê°„ í‘œì‹œ\n\n4. í¸ì˜ì‹œì„¤\n   â”œâ”€ facilities (JSONB)\n   â”‚   â””â”€ [\"ì£¼ì°¨\", \"ì™€ì´íŒŒì´\", \"ì˜ˆì•½\", \"ë°°ë‹¬\"]\n   â””â”€ UI: ì•„ì´ì½˜ìœ¼ë¡œ í‘œì‹œ\n\nêµ¬í˜„:\n1. DB ìŠ¤í‚¤ë§ˆ í™•ì¸/ìˆ˜ì •\n2. API ì‘ë‹µì— í•„ë“œ ì¶”ê°€\n3. íŒì—… HTML ìˆ˜ì •\n4. ë°ì´í„° í‘œì‹œ ë¡œì§ ì¶”ê°€\n```\n\n---\n\n### Priority 2: ì¤‘ìš” (ì´í›„ ìˆ˜ì •, 5-6ì‹œê°„)\n\n#### 2-1. ìˆ˜ì§‘ ê²°ê³¼ í™”ë©´ í†µí•© ê°œì„  (3ì‹œê°„)\n\n```markdown\n**ì‘ì—…**: collection-resultsì— ì¤‘ë³µ ê²€ì‚¬ + í’ˆì§ˆ ê´€ë¦¬ í†µí•©\n\nBefore:\n- ìˆ˜ì§‘ ê²°ê³¼: ë‹¨ìˆœ í…Œì´ë¸”\n- ì¤‘ë³µ ê²€ì‚¬: ë³„ë„ ë©”ë‰´ (ë™ì¼ í™”ë©´)\n- í’ˆì§ˆ ê´€ë¦¬: ë³„ë„ ë©”ë‰´ (ë™ì¼ í™”ë©´)\n\nAfter:\nìˆ˜ì§‘ ê²°ê³¼ í˜ì´ì§€ì— íƒ­ ì¶”ê°€:\n[ì „ì²´ ê²°ê³¼] [ì¤‘ë³µ ê²€ì‚¬] [í’ˆì§ˆ ê´€ë¦¬]\n\n1. [ì „ì²´ ê²°ê³¼] íƒ­\n   â”œâ”€ ê¸°ì¡´ í…Œì´ë¸”\n   â”œâ”€ ê³ ê¸‰ í•„í„° (ì†ŒìŠ¤/ìƒíƒœ/ì ìˆ˜)\n   â”œâ”€ ì¼ê´„ ì‘ì—… (ì„ íƒ í›„ ìƒíƒœ ë³€ê²½)\n   â””â”€ ë‚´ë³´ë‚´ê¸° (CSV/Excel)\n\n2. [ì¤‘ë³µ ê²€ì‚¬] íƒ­\n   â”œâ”€ ìë™ ì¤‘ë³µ ê°ì§€ ì‹¤í–‰\n   â”œâ”€ ì¤‘ë³µ ê·¸ë£¹ë³„ í‘œì‹œ\n   â”œâ”€ ë³‘í•©/ì œì™¸ ì•¡ì…˜\n   â””â”€ Fuzzy Match ì„¤ì •\n\n3. [í’ˆì§ˆ ê´€ë¦¬] íƒ­\n   â”œâ”€ í’ˆì§ˆ ì ìˆ˜ë³„ í•„í„°\n   â”œâ”€ ëˆ„ë½ í•„ë“œ í™•ì¸\n   â”œâ”€ ì¼ê´„ í’ˆì§ˆ ê°œì„ \n   â””â”€ ìŠ¹ì¸/ê±°ì ˆ ì›Œí¬í”Œë¡œìš°\n\nêµ¬í˜„:\n1. collection-results.htmlì— íƒ­ ì¶”ê°€\n2. ê° íƒ­ë³„ API ì—”ë“œí¬ì¸íŠ¸ í™œìš©\n3. JavaScript íƒ­ ì „í™˜ ë¡œì§\n4. ê¸°ëŠ¥ë³„ UI ì»´í¬ë„ŒíŠ¸ ì¶”ê°€\n```\n\n#### 2-2. ë°ì´í„° ì…ë ¥ ë©”ë‰´ ìƒì„± (3ì‹œê°„)\n\n```markdown\n**ì‘ì—…**: ìƒˆë¡œìš´ \"ë°ì´í„° ì…ë ¥\" ë©”ë‰´ + í˜ì´ì§€ ìƒì„±\n\nêµ¬ì¡°:\n/dashboard/data-input (NEW í˜ì´ì§€)\n\níƒ­ êµ¬ì¡°:\n[URL ì…ë ¥] [CSV ì—…ë¡œë“œ] [ì§ì ‘ ì…ë ¥]\n\n1. [URL ì…ë ¥] íƒ­\n   â”œâ”€ URL ì…ë ¥ í•„ë“œ\n   â”œâ”€ í”Œë«í¼ ìë™ ê°ì§€ (Google/Naver/Kakao)\n   â”œâ”€ íŒŒì‹± ë¯¸ë¦¬ë³´ê¸°\n   â”œâ”€ [íŒŒì‹± ì‹¤í–‰] ë²„íŠ¼\n   â””â”€ ê²°ê³¼ í™•ì¸ ë° ì €ì¥\n\n2. [CSV ì—…ë¡œë“œ] íƒ­\n   â”œâ”€ [í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ] ë²„íŠ¼\n   â”œâ”€ íŒŒì¼ ì—…ë¡œë“œ ë“œë˜ê·¸&ë“œë¡­\n   â”œâ”€ ì»¬ëŸ¼ ë§¤í•‘ (CSV â†’ DB)\n   â”œâ”€ ë¯¸ë¦¬ë³´ê¸° (10ê°œ)\n   â””â”€ [ì¼ê´„ ì €ì¥] ë²„íŠ¼\n\n3. [ì§ì ‘ ì…ë ¥] íƒ­\n   â”œâ”€ í•„ìˆ˜ í•„ë“œ (ì´ë¦„, ì¹´í…Œê³ ë¦¬, ì£¼ì†Œ)\n   â”œâ”€ ì„ íƒ í•„ë“œ (ì „í™”, í‰ì , ë©”ë‰´ ë“±)\n   â”œâ”€ ì‹¤ì‹œê°„ ê²€ì¦\n   â””â”€ [ì €ì¥] ë²„íŠ¼\n\nêµ¬í˜„:\n1. ìƒˆ í˜ì´ì§€ data-input.html ìƒì„±\n2. ê¸°ì¡´ manual-input API í™œìš©\n3. ì‚¬ì´ë“œë°”ì— ë©”ë‰´ ì¶”ê°€\n4. ë¼ìš°íŒ… ì„¤ì •\n```\n\n---\n\n### Priority 3: ê°œì„  (ë‚˜ì¤‘ì—, 3-4ì‹œê°„)\n\n#### 3-1. ë©”ì¸ ëŒ€ì‹œë³´ë“œ ìµœì í™” (2ì‹œê°„)\n\n```markdown\n**ì‘ì—…**: ëŒ€ì‹œë³´ë“œë¥¼ ë” ì§ê´€ì ìœ¼ë¡œ ê°œì„ \n\nê°œì„  ì‚¬í•­:\n\n1. ìµœê·¼ í™œë™ ì„¹ì…˜ ì¶”ê°€\n   â”œâ”€ ìµœê·¼ ìˆ˜ì§‘ ìš”ì²­ (3ê°œ)\n   â”œâ”€ ìµœê·¼ ìŠ¹ì¸/ê±°ì ˆ (3ê°œ)\n   â””â”€ ìµœê·¼ ë°°í¬ (3ê°œ)\n\n2. ì•Œë¦¼/ê²½ê³  ì„¹ì…˜\n   â”œâ”€ ì¤‘ë³µ ë°ì´í„° ë°œê²¬ (Nê°œ)\n   â”œâ”€ í’ˆì§ˆ ë‚®ì€ ë°ì´í„° (Nê°œ)\n   â””â”€ ìŠ¹ì¸ ëŒ€ê¸° ì¤‘ (Nê°œ)\n\n3. ë¹ ë¥¸ ì•¡ì…˜ (3ê°œë§Œ ë‚¨ê¸°ê¸°)\n   â”œâ”€ [ìƒˆ ìˆ˜ì§‘ ìš”ì²­]\n   â”œâ”€ [ë°ì´í„° ì…ë ¥]\n   â””â”€ [ë°°í¬ ê´€ë¦¬]\n\n4. ì°¨íŠ¸ ê°œì„ \n   â”œâ”€ 7ì¼ ìˆ˜ì§‘ ì¶”ì´ (ê¸°ì¡´ ìœ ì§€)\n   â””â”€ ë°ì´í„° ì†ŒìŠ¤ë³„ ë¶„í¬ (NEW)\n\nêµ¬í˜„:\n1. dashboard.html ìˆ˜ì •\n2. ìµœê·¼ í™œë™ API ìƒì„±\n3. ì•Œë¦¼ ì§‘ê³„ API ìƒì„±\n4. ì°¨íŠ¸ ì¶”ê°€\n```\n\n#### 3-2. í•„í„°/ì •ë ¬ ê³ ê¸‰ ê¸°ëŠ¥ (2ì‹œê°„)\n\n```markdown\n**ì‘ì—…**: ëª¨ë“  ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì— ê³ ê¸‰ í•„í„° ì¶”ê°€\n\nê³µí†µ í•„í„° ì»´í¬ë„ŒíŠ¸:\n1. ê²€ìƒ‰ (ì´ë¦„/ì£¼ì†Œ/ì¹´í…Œê³ ë¦¬)\n2. ì†ŒìŠ¤ (Google/Naver/Manual)\n3. ìƒíƒœ (Pending/Approved/Rejected)\n4. ì ìˆ˜ ë²”ìœ„ (ìŠ¬ë¼ì´ë”)\n5. ë‚ ì§œ ë²”ìœ„ (ë‹¬ë ¥)\n\nì ìš© í˜ì´ì§€:\n- ìˆ˜ì§‘ ê²°ê³¼\n- ìˆ˜ì§‘ ìš”ì²­\n- í†µí•© í¸ì§‘\n- ë°°í¬ ê´€ë¦¬\n\nêµ¬í˜„:\n1. ê³µí†µ í•„í„° ì»´í¬ë„ŒíŠ¸ ìƒì„±\n2. ê° í˜ì´ì§€ì— ì ìš©\n3. API ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° í™œìš©\n4. URL ë™ê¸°í™” (í•„í„° ìƒíƒœ ìœ ì§€)\n```\n\n---\n\n## ğŸ“‹ ìµœì¢… ì‹¤í–‰ ê³„íš\n\n### Phase 1: ì¹˜ëª…ì  ìˆ˜ì • (ì¦‰ì‹œ, 4-5ì‹œê°„)\n\n```\nDay 1 (ì˜¤ëŠ˜-ë‚´ì¼):\nâ–¡ 1-1. ë©”ë‰´ êµ¬ì¡° ì¬ì„¤ê³„ (2ì‹œê°„)\nâ–¡ 1-2. ë ˆìŠ¤í† ë‘ ìƒì„¸ í•„ë“œ ì¶”ê°€ (3ì‹œê°„)\n\nëª©í‘œ: ë©”ë‰´ê°€ ë…¼ë¦¬ì ì´ê³ , ë ˆìŠ¤í† ë‘ ì •ë³´ê°€ ì™„ì „í•¨\n```\n\n### Phase 2: ì¤‘ìš” ìˆ˜ì • (ì´í›„, 5-6ì‹œê°„)\n\n```\nDay 2-3:\nâ–¡ 2-1. ìˆ˜ì§‘ ê²°ê³¼ í†µí•© ê°œì„  (3ì‹œê°„)\nâ–¡ 2-2. ë°ì´í„° ì…ë ¥ ë©”ë‰´ ìƒì„± (3ì‹œê°„)\n\nëª©í‘œ: ê¸°ëŠ¥ì´ ëª…í™•íˆ ë¶„ë¦¬ë˜ê³ , ìˆ˜ë™ ì…ë ¥ ê°€ëŠ¥\n```\n\n### Phase 3: ê°œì„  (ì—¬ìœ  ìˆì„ ë•Œ, 3-4ì‹œê°„)\n\n```\nDay 4:\nâ–¡ 3-1. ëŒ€ì‹œë³´ë“œ ìµœì í™” (2ì‹œê°„)\nâ–¡ 3-2. í•„í„°/ì •ë ¬ ê³ ê¸‰ ê¸°ëŠ¥ (2ì‹œê°„)\n\nëª©í‘œ: ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ\n```\n\n---\n\n## ğŸ’° ë¹„ìš© ê³ ë ¤ì‚¬í•­\n\n### ì˜ˆìƒ ë¹„ìš©\n\n```\nPhase 1 (ì¹˜ëª…ì ): 4-5ì‹œê°„ Ã— $0.05/min â‰ˆ $12-15\nPhase 2 (ì¤‘ìš”): 5-6ì‹œê°„ Ã— $0.05/min â‰ˆ $15-18\nPhase 3 (ê°œì„ ): 3-4ì‹œê°„ Ã— $0.05/min â‰ˆ $9-12\n\nì´ ì˜ˆìƒ: $36-45\n\në¹„ìš© ìµœì í™” ë°©ì•ˆ:\n1. Phase 1ë§Œ ìš°ì„  ì§„í–‰ (í•„ìˆ˜)\n2. Phase 2 ì„ íƒì  ì§„í–‰\n3. Phase 3ëŠ” ìš´ì˜ ì•ˆì •í™” í›„\n```\n\n### ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼\n\n```\nPhase 1 ($12-15):\nâ†’ ìš´ì˜ ê°€ëŠ¥í•œ ìµœì†Œ ìˆ˜ì¤€\nâ†’ ROI: ë§¤ìš° ë†’ìŒ âœ…\n\nPhase 2 ($15-18):\nâ†’ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€\nâ†’ ROI: ë†’ìŒ âœ…\n\nPhase 3 ($9-12):\nâ†’ ì‚¬ìš©ì ë§Œì¡±ë„ í–¥ìƒ\nâ†’ ROI: ì¤‘ê°„ âœ…\n\nê¶Œì¥: Phase 1+2 ì§„í–‰ ($27-33)\n```\n\n---\n\n## ğŸ¯ Replit ì§€ì‹œì‚¬í•­ (Phase 1)\n\n```markdown\nê¸´ê¸‰ UI/UX ê°œì„  ì‹œì‘!\n\ní˜„ì¬ ë¬¸ì œ:\n1. ë©”ë‰´ êµ¬ì¡° í˜¼ë€ (ë™ì¼ í™”ë©´ 4ê°œ)\n2. ë ˆìŠ¤í† ë‘ ì •ë³´ ëˆ„ë½ (ë©”ë‰´, Place ID, ë§í¬)\n3. ë°ì´í„° ì…ë ¥ ë©”ë‰´ ì—†ìŒ\n\nPhase 1: ì¹˜ëª…ì  ìˆ˜ì • (4-5ì‹œê°„)\n\nâ–¡ Task 1-1: ë©”ë‰´ êµ¬ì¡° ì¬ì„¤ê³„ (2ì‹œê°„)\n\n1. ì¢Œì¸¡ ì‚¬ì´ë“œë°” ìˆ˜ì •:\n   ```\n   ğŸ“Š ëŒ€ì‹œë³´ë“œ\n   ğŸ” ë°ì´í„° ìˆ˜ì§‘\n      â”œâ”€ ìˆ˜ì§‘ ìš”ì²­\n      â””â”€ ìˆ˜ì§‘ ê²°ê³¼\n   âœï¸ ë°ì´í„° ì…ë ¥ (NEW)\n      â”œâ”€ URL ì…ë ¥\n      â”œâ”€ CSV ì—…ë¡œë“œ\n      â””â”€ ì§ì ‘ ì…ë ¥\n   ğŸ”§ ë°ì´í„° í¸ì§‘\n   ğŸš€ ë°°í¬ ê´€ë¦¬\n   ```\n\n2. ë¼ìš°íŒ… ìˆ˜ì •:\n   - /dashboard/collection-request (ìˆ˜ì§‘ ìš”ì²­)\n   - /dashboard/collection-results (ìˆ˜ì§‘ ê²°ê³¼)\n   - /dashboard/data-input (ë°ì´í„° ì…ë ¥ - NEW)\n   - /dashboard/unified-editor (ë°ì´í„° í¸ì§‘)\n   - /dashboard/deployment (ë°°í¬ ê´€ë¦¬)\n\n3. ì¤‘ë³µ ê²€ì‚¬/í’ˆì§ˆ ê´€ë¦¬:\n   â†’ collection-results í˜ì´ì§€ ë‚´ íƒ­ìœ¼ë¡œ í†µí•©\n\nâ–¡ Task 1-2: ë ˆìŠ¤í† ë‘ ìƒì„¸ í•„ë“œ ì¶”ê°€ (3ì‹œê°„)\n\n1. DB ìŠ¤í‚¤ë§ˆ í™•ì¸:\n   ```sql\n   -- í•„ìš”í•œ ì»¬ëŸ¼ë“¤:\n   menu_items JSONB  -- ë©”ë‰´ ì •ë³´\n   google_place_id VARCHAR\n   naver_place_id VARCHAR\n   google_maps_url VARCHAR\n   naver_map_url VARCHAR\n   business_hours JSONB  -- ì˜ì—… ì‹œê°„\n   facilities JSONB  -- í¸ì˜ì‹œì„¤\n   ```\n\n2. API ì‘ë‹µ ìˆ˜ì •:\n   - GET /collection-results/{id}\n   â†’ ìœ„ í•„ë“œë“¤ í¬í•¨\n\n3. íŒì—… HTML ìˆ˜ì •:\n   ```html\n   <section class=\"menu-section\">\n     <h3>ë©”ë‰´</h3>\n     <div v-for=\"menu in menuItems\">\n       {{ menu.name }} - {{ menu.price }}ì›\n     </div>\n   </section>\n   \n   <section class=\"links-section\">\n     <a :href=\"googleMapsUrl\" target=\"_blank\">\n       Googleì—ì„œ ë³´ê¸°\n     </a>\n     <a :href=\"naverMapUrl\" target=\"_blank\">\n       ë„¤ì´ë²„ì—ì„œ ë³´ê¸°\n     </a>\n   </section>\n   \n   <section class=\"hours-section\">\n     <h3>ì˜ì—…ì‹œê°„</h3>\n     {{ businessHours }}\n   </section>\n   ```\n\nì‹œì‘í•˜ì„¸ìš”! ğŸš€\n```\n\n---\n\n## ğŸ“Š ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n### Phase 1 ì™„ë£Œ ê¸°ì¤€\n\n```\nâ–¡ ì¢Œì¸¡ ë©”ë‰´ê°€ 5ê°œ ì„¹ì…˜ìœ¼ë¡œ ì¬êµ¬ì„±ë¨\nâ–¡ ê° ë©”ë‰´ë³„ í˜ì´ì§€ê°€ ë¶„ë¦¬ë¨\nâ–¡ ë°ì´í„° ì…ë ¥ ë©”ë‰´ê°€ ìƒˆë¡œ ìƒê¹€\nâ–¡ ë ˆìŠ¤í† ë‘ ìƒì„¸ íŒì—…ì— ë©”ë‰´ ì •ë³´ í‘œì‹œ\nâ–¡ Google/Naver ë°”ë¡œê°€ê¸° ë§í¬ ì‘ë™\nâ–¡ ì˜ì—…ì‹œê°„ í‘œì‹œ\nâ–¡ ëª¨ë“  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n\n= Phase 1 ì™„ë£Œ! âœ…\n```\n\n### Phase 2 ì§„í–‰ ì—¬ë¶€\n\n```\nPhase 1 ì™„ë£Œ í›„ ê²€í† :\n1. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘\n2. ìš´ì˜ ê°€ëŠ¥ ì—¬ë¶€ í‰ê°€\n3. Phase 2 í•„ìš”ì„± ì¬ê²€í† \n\nâ†’ í•„ìš”í•˜ë©´ Phase 2 ì§„í–‰\nâ†’ ì¶©ë¶„í•˜ë©´ ìš´ì˜ í…ŒìŠ¤íŠ¸\n```\n\n---\n\n**ì´ê²ƒì´ ì „ë¬¸ê°€ì˜ ëƒ‰ì •í•œ UI/UX ê°œì„  ì§€ì‹œì…ë‹ˆë‹¤!** âš ï¸\n\n**Phase 1ë¶€í„° ì¦‰ì‹œ ì‹œì‘í•˜ì„¸ìš”!** ğŸš€\n","size_bytes":11894},"attached_assets/ui-ux-detailed-analysis-final_1762737704160.md":{"content":"# ğŸš¨ Data Hub UI/UX ìƒì„¸ ë¶„ì„ & ê¸´ê¸‰ ìˆ˜ì • ë³´ê³ ì„œ (ìµœì¢…)\n\n**ë¶„ì„ì¼**: 2025ë…„ 11ì›” 9ì¼ 23:49  \n**ì´ í™”ë©´**: 10ê°œ  \n**í‰ê°€**: ë°°í¬ ë¶ˆê°€ëŠ¥ ìˆ˜ì¤€, ê¸´ê¸‰ ìˆ˜ì • í•„ìš”  \n**ê¶Œì¥ì‚¬í•­**: ì¦‰ì‹œ ê°œë°œ ì¤‘ë‹¨ í›„ ìˆ˜ì •\n\n---\n\n## ğŸ“‹ 10ê°œ í™”ë©´ ìƒì„¸ ë¶„ì„\n\n### í™”ë©´ 1: `daesibodeu.jpg` - ë©”ì¸ ëŒ€ì‹œë³´ë“œ\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬ (Database, API Server, Scheduler, Drive Backup)\n- ì–´ì œ ìˆ˜ì§‘ í†µê³„ (ì‹ ê·œ ìˆ˜ì§‘, ìŠ¹ì¸ëœ ê²ƒ)\n- ë°±ì—… ìƒíƒœ\n- ì‘ì—… ì‹¤í–‰ ì„¼í„° (8ê°œ ë²„íŠ¼: Smart Targeting, Naver ìŠ¤í¬ë˜í•‘, ë°ì´í„° ë†ì¥, Gemini AI, Google Places, ì½˜í…ì¸  ë™ê¸°í™”, Drive ì—­ì—…, etc)\n- 7ì¼ ìˆ˜ì§‘ ì¶”ì´ ì°¨íŠ¸\n\nâŒ ì‹¬ê°í•œ ë¬¸ì œ:\n1. ì •ë³´ ê³¼ì‰ (Cognitive Overload)\n   - í•œ í™”ë©´ì— 6ê°œ ì„¹ì…˜ (ë„ˆë¬´ ë§ìŒ)\n   - ì‚¬ìš©ìê°€ ë­˜ ë¨¼ì € í• ì§€ ëª¨ë¦„\n   \n2. ì‘ì—… ë²„íŠ¼ì´ ë„ˆë¬´ ë§ê³  ëª…í™•í•˜ì§€ ì•ŠìŒ\n   - \"Smart Targeting\", \"Naver ìŠ¤í¬ë˜í•‘\" ë“±ì´ ë¬´ì—‡ì¸ê°€?\n   - í˜„ì¬ 5ê°œ ë°ì´í„°ë§Œ ìˆëŠ”ë° 8ê°œ ë²„íŠ¼?\n   - ë²„íŠ¼ë³„ ìƒíƒœ í‘œì‹œ ì—†ìŒ (ì§„í–‰ ì¤‘? ì™„ë£Œ? ì˜¤ë¥˜?)\n   \n3. ìš°ì„ ìˆœìœ„ ë¶ˆëª…í™•\n   - ëŒ€ì‹œë³´ë“œì˜ \"í•µì‹¬\" ì•¡ì…˜ì´ ë­”ê°€?\n   - \"ìˆ˜ì§‘ ìš”ì²­\" ë²„íŠ¼ì´ ë³´ì´ì§€ ì•ŠìŒ (ì¤‘ìš”í•œë°!)\n   \n4. ìµœê·¼ í™œë™ ë¡œê·¸ ì—†ìŒ\n   - ì–´ë–¤ ì‘ì—…ì´ ì–¸ì œ ì‹¤í–‰ëëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìŒ\n   \n5. ì•Œë¦¼/ê²½ê³  ì„¹ì…˜ ì—†ìŒ\n   - ì¤‘ë³µ ë°ì´í„° ë°œê²¬ â†’ ì•Œë¦¼ ì—†ìŒ\n   - í’ˆì§ˆ ë‚®ì€ ë°ì´í„° â†’ ê²½ê³  ì—†ìŒ\n   - ë°°í¬ ì‹¤íŒ¨ â†’ ì•Œë¦¼ ì—†ìŒ\n\nâŒ ë ˆì´ì•„ì›ƒ ë¬¸ì œ:\n- í†µê³„ 3ê°œê°€ ë„ˆë¬´ ì‘ìŒ (ìˆ«ìë§Œ ë¡ ìˆìŒ)\n- ì°¨íŠ¸ê°€ ë„ˆë¬´ í¬ê²Œ ì°¨ì§€\n- ë°±ì—… ìƒíƒœ ì„¹ì…˜ì´ í•„ìš”í•œê°€? (ê´€ë¦¬ììš©ì´ ì•„ë‹ˆë©´ ë¶ˆí•„ìš”)\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. ëŒ€ì‹œë³´ë“œ êµ¬ì¡° ì¬ì„¤ê³„ (í”¼ë¼ë¯¸ë“œ êµ¬ì¡°)\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚  [ì£¼ìš” ì•¡ì…˜] 3ê°œë§Œ ê°•ì¡°         â”‚ (ìˆ˜ì§‘ìš”ì²­, ë°ì´í„°ì…ë ¥, ë°°í¬)\n   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n   â”‚  [í•µì‹¬ í†µê³„] ìš”ì•½                â”‚ (ì „ì²´, í‰ê·  í‰ì , Top Rated)\n   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n   â”‚  [ìµœê·¼ í™œë™] ë¡œê·¸ 10ê°œ            â”‚ (ìƒˆë¡œ ì¶”ê°€)\n   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n   â”‚  [ê²½ê³ /ì•Œë¦¼] ì¤‘ìš”í•œ ê²ƒë§Œ          â”‚ (ìƒˆë¡œ ì¶”ê°€)\n   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n   â”‚  [7ì¼ ì¶”ì´] ì°¨íŠ¸                 â”‚ (ê¸°ì¡´ ìœ ì§€)\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n2. ì‘ì—… ë²„íŠ¼ ì •ë¦¬\n   - í˜„ì¬: 8ê°œ ë²„íŠ¼ (ë„ˆë¬´ ë§ìŒ)\n   - ë³€ê²½: 3ê°œ ì£¼ìš” ë²„íŠ¼ë§Œ í‘œì‹œ\n     * [ìƒˆ ìˆ˜ì§‘ ìš”ì²­] (íŒŒë€ìƒ‰, ê°•ì¡°)\n     * [ë°ì´í„° ì…ë ¥] (ì´ˆë¡ìƒ‰)\n     * [ë°°í¬ ê´€ë¦¬] (ì£¼í™©ìƒ‰)\n   - ë‚˜ë¨¸ì§€ëŠ” \"ë”ë³´ê¸°\" ë˜ëŠ” ë³„ë„ ë©”ë‰´ë¡œ\n   \n3. í†µê³„ ê°•í™”\n   - í˜„ì¬: ì‘ì€ ì¹´ë“œ (ì–´ëŠ ê²ƒì´ ì¤‘ìš”í•œì§€ ëª¨ë¦„)\n   - ë³€ê²½:\n     * ìƒë‹¨: 3ê°œ ì£¼ìš” í†µê³„ (í¼)\n     * í•˜ë‹¨: ì¶”ê°€ í†µê³„ (ì‘ìŒ)\n     \n4. ìµœê·¼ í™œë™ ì¶”ê°€ (ë§¤ìš° ì¤‘ìš”!)\n   ```\n   [ìµœê·¼ í™œë™]\n   - 2025-11-09 11:45 | ì˜¬ë˜êµ­ìˆ˜ | ë°ì´í„° ì…ë ¥ | ì„±ê³µ âœ…\n   - 2025-11-09 11:30 | 5ê°œ | ë°°í¬ ì‹¤í–‰ | ì§„í–‰ ì¤‘ â³\n   - 2025-11-09 11:00 | 2ê°œ ì¤‘ë³µ | ìë™ ê°ì§€ | ê²½ê³  âš ï¸\n   ```\n   \n5. ê²½ê³ /ì•Œë¦¼ ì„¹ì…˜ ì¶”ê°€\n   ```\n   [âš ï¸ ì¤‘ìš” ì•Œë¦¼]\n   - ì¤‘ë³µ ë°ì´í„°: 2ê°œ ë°œê²¬ (ì²˜ë¦¬ í•„ìš”)\n   - í’ˆì§ˆ ë‚®ìŒ: 3ê°œ ë°œê²¬ (í’ˆì§ˆ ê´€ë¦¬ í•„ìš”)\n   - ìŠ¹ì¸ ëŒ€ê¸°: 0ê°œ\n   ```\n```\n\n---\n\n### í™”ë©´ 2: `deiteoibryeogmein.jpg` - ë°ì´í„° ì…ë ¥ ë©”ì¸ (URL/CSV/ì§ì ‘ì…ë ¥)\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- 3ê°€ì§€ ì…ë ¥ ë°©ë²•ì´ íƒ­ìœ¼ë¡œ ë¶„ë¦¬ (ì¢‹ì€ êµ¬ì¡°)\n- URL ì…ë ¥, CSV ì—…ë¡œë“œ, ì§ì ‘ ì…ë ¥ ëª…í™•\n- ì…ë ¥ í•„ë“œë“¤ì´ í‘œì‹œë¨\n\nâŒ ì‹¬ê°í•œ ë¬¸ì œ:\n1. í•„ë“œê°€ ë„ˆë¬´ ë§ê³  ë³µì¡\n   - í•„ìˆ˜ í•„ë“œ: ì´ë¦„, ì¹´í…Œê³ ë¦¬, ì£¼ì†Œ, í‰ì , í‰ê°€ ì‚¬ìœ \n   - ì„ íƒ í•„ë“œ: ìœ„ë„/ê²½ë„, ì›ë³¸ ì„¤ëª…, ë³„ì  ìƒì„¸ì •ë³´\n   - í‰ê°€ ì‚¬ìœ  í…ìŠ¤íŠ¸ ì…ë ¥ (400ì ì œí•œ)\n   \n2. ë©”ë‰´, ì˜ì—…ì‹œê°„, í¸ì˜ì‹œì„¤ ì…ë ¥ ë¶ˆê°€ëŠ¥!\n   - DBì—ëŠ” ìˆëŠ” í•„ë“œì¸ë° ì…ë ¥ í¼ì— ì—†ìŒ\n   - \"ë©”ë‰´\" ê°™ì€ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ê°€ ì…ë ¥ ì•ˆ ë¨!\n   \n3. ë©”ë‰´ ì¶”ê°€ ë°©ì‹ì´ ë¶ˆëª…í™•\n   - ë™ì ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ë²„íŠ¼ ìˆë‚˜? ì—†ë‚˜?\n   - \"ë©”ë‰´\" í•„ë“œê°€ í•œ ì¤„ë¡œë§Œ ì…ë ¥ ê°€ëŠ¥í•´ ë³´ì„\n   \n4. ì˜ì—…ì‹œê°„ í˜•ì‹\n   - \"10:00-22:00\" ê°™ì€ ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ ì…ë ¥\n   - ìš”ì¼ë³„ ì˜ì—…ì‹œê°„ ì…ë ¥ ë¶ˆê°€ëŠ¥\n   \n5. ì €ì¥ ì „ ê²€ì¦ ì—†ìŒ\n   - í•„ìˆ˜ í•„ë“œ í™•ì¸ ì•ˆ í•¨\n   - ë°ì´í„° í˜•ì‹ í™•ì¸ ì•ˆ í•¨ (ì˜ˆ: í‰ì ì€ 1-5 ë²”ìœ„?)\n   \n6. ë³€ê²½/ì·¨ì†Œ ë²„íŠ¼ì€ ìˆìœ¼ë‚˜, ë™ì‘ì´ ë¶ˆëª…í™•\n   - \"ë³€ê²½\" vs \"ì €ì¥\" ì°¨ì´ê°€ ë­”ê°€?\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. í¼ êµ¬ì¡° ì¬ì„¤ê³„ (ì„¹ì…˜ ë¶„ë¦¬)\n   \n   [ê¸°ë³¸ ì •ë³´] ì„¹ì…˜\n   â”œâ”€ ì´ë¦„ * (í•„ìˆ˜, í…ìŠ¤íŠ¸)\n   â”œâ”€ ì¹´í…Œê³ ë¦¬ * (í•„ìˆ˜, ë“œë¡­ë‹¤ìš´)\n   â”œâ”€ ì£¼ì†Œ * (í•„ìˆ˜, í…ìŠ¤íŠ¸)\n   â””â”€ ì „í™” (ì„ íƒ, ìˆ«ì)\n   \n   [ìƒì„¸ ì •ë³´] ì„¹ì…˜\n   â”œâ”€ ì„¤ëª… (ì„ íƒ, í…ìŠ¤íŠ¸ 500ì)\n   â”œâ”€ ê°€ê²©ëŒ€ (ì„ íƒ, ë“œë¡­ë‹¤ìš´: ì €/ì¤‘/ê³ )\n   â””â”€ ì¹´í…Œê³ ë¦¬ ìƒì„¸ (ì„ íƒ, ë©€í‹°ì…€ë ‰íŠ¸)\n   \n   [í‰ê°€ ì •ë³´] ì„¹ì…˜\n   â”œâ”€ í‰ì  * (í•„ìˆ˜, 1-5 ìŠ¬ë¼ì´ë”)\n   â”œâ”€ ë¦¬ë·° ìˆ˜ (ì„ íƒ, ìˆ«ì)\n   â”œâ”€ í‰ê°€ ì‚¬ìœ  (ì„ íƒ, í…ìŠ¤íŠ¸ 400ì)\n   â””â”€ í‰ê°€ ìƒíƒœ (ì„ íƒ, ë¼ë””ì˜¤: ë°°í¬ë¨/ëŒ€ê¸°ì¤‘/ê±°ì ˆ)\n   \n   [ë©”ë‰´ ì •ë³´] ì„¹ì…˜ (NEW!)\n   â”œâ”€ [ë©”ë‰´ ì¶”ê°€] ë²„íŠ¼\n   â”œâ”€ ë©”ë‰´ 1\n   â”‚  â”œâ”€ ë©”ë‰´ëª… (ì˜ˆ: ê¹€ì¹˜ì°Œê°œ)\n   â”‚  â”œâ”€ ê°€ê²© (ì˜ˆ: 8000)\n   â”‚  â””â”€ ì„¤ëª… (ì„ íƒ)\n   â”œâ”€ ë©”ë‰´ 2\n   â””â”€ ...\n   \n   [ì˜ì—…ì‹œê°„] ì„¹ì…˜ (NEW!)\n   â”œâ”€ ì›”ìš”ì¼: 10:00 ~ 22:00\n   â”œâ”€ í™”ìš”ì¼: 10:00 ~ 22:00\n   â”œâ”€ ...\n   â”œâ”€ ì¼ìš”ì¼: íœ´ë¬´\n   â””â”€ [ì‹œê°„ ìë™ì±„ìš°ê¸°] ë²„íŠ¼\n   \n   [í¸ì˜ì‹œì„¤] ì„¹ì…˜ (NEW!)\n   â”œâ”€ â˜ ì£¼ì°¨\n   â”œâ”€ â˜ ì™€ì´íŒŒì´\n   â”œâ”€ â˜ ì˜ˆì•½ê°€ëŠ¥\n   â”œâ”€ â˜ ë°°ë‹¬ê°€ëŠ¥\n   â””â”€ â˜ í…Œì´ë¸”ì„\n   \n   [ìœ„ì¹˜ ì •ë³´] ì„¹ì…˜ (NEW!)\n   â”œâ”€ ìœ„ë„/ê²½ë„ (ìˆ«ì)\n   â”œâ”€ [ì§€ë„ì—ì„œ ì„ íƒ] ë²„íŠ¼\n   â””â”€ [ì£¼ì†Œ ê²€ìƒ‰ ìë™ì…ë ¥] ë²„íŠ¼\n   \n   [ë§í¬ ì •ë³´] ì„¹ì…˜ (NEW!)\n   â”œâ”€ Google Place ID (ì„ íƒ)\n   â”œâ”€ Naver Place ID (ì„ íƒ)\n   â”œâ”€ ì›ë³¸ URL (ì„ íƒ)\n   â””â”€ [ë§í¬ ê²€ìƒ‰ ìë™ì…ë ¥] ë²„íŠ¼\n\n2. ë©”ë‰´ ì¶”ê°€ (ë™ì )\n   - [ë©”ë‰´ ì¶”ê°€] í´ë¦­ â†’ ìƒˆ ë©”ë‰´ í–‰ ì¶”ê°€\n   - ê° ë©”ë‰´ ì˜†ì— [X] ë²„íŠ¼ìœ¼ë¡œ ì‚­ì œ ê°€ëŠ¥\n   - ë©”ë‰´ ìµœëŒ€ 10ê°œ (í˜„ì¬: ê°œìˆ˜ ì œí•œ ì—†ìŒ?)\n   \n3. ì˜ì—…ì‹œê°„ ì…ë ¥ ê°œì„ \n   - ìš”ì¼ë³„ ì…ë ¥ (ì›”-ì¼)\n   - ì‹œê°„ ì„ íƒ (ì‹œê°„:ë¶„ ë“œë¡­ë‹¤ìš´)\n   - íœ´ë¬´ì¼ ì²´í¬ë°•ìŠ¤\n   - [ê°™ì€ ì‹œê°„ ì ìš©] ë²„íŠ¼ (ëª¨ë“  ìš”ì¼ì— ì ìš©)\n   \n4. ê²€ì¦ ì¶”ê°€\n   - í•„ìˆ˜ í•„ë“œ í‘œì‹œ (*)\n   - ì €ì¥ ì „ ê²€ì¦ (ì˜ˆ: í‰ì ì€ 1-5)\n   - ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œ\n   - ë¹„í¬/ì• í”„í„° ë¯¸ë¦¬ë³´ê¸°\n   \n5. ë²„íŠ¼ ëª…í™•í™”\n   - [ì €ì¥] (ì´ˆë¡ìƒ‰) - ì €ì¥\n   - [ì·¨ì†Œ] (íšŒìƒ‰) - íê¸°\n   - ìƒíƒœ í‘œì‹œ \"ì €ì¥ ì¤‘...\" â†’ \"ì €ì¥ ì™„ë£Œ!\" ë˜ëŠ” \"ì €ì¥ ì‹¤íŒ¨!\"\n```\n\n---\n\n### í™”ë©´ 3: `deiteoibryeogjungcsveobrodeuhwamyeon.jpg` - ë°ì´í„° ì…ë ¥ > CSV íƒ­\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- CSV ì—…ë¡œë“œ ì•ˆë‚´ ìˆìŒ\n- í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ ë§í¬ ìˆìŒ\n\nâŒ ì‹¬ê°í•œ ë¬¸ì œ:\n1. ì—…ë¡œë“œ í›„ ì•„ë¬´ê²ƒë„ ì•ˆ ë³´ì„\n   - íŒŒì¼ ì—…ë¡œë“œ ì˜ì—­ë§Œ ìˆìŒ\n   - ì—…ë¡œë“œ í›„ ê²°ê³¼ í‘œì‹œ ì•ˆ ë¨\n   \n2. ì»¬ëŸ¼ ë§¤í•‘ UI ì—†ìŒ\n   - CSV ì–´ëŠ ì»¬ëŸ¼ì´ ì–´ëŠ DB í•„ë“œì¸ì§€ ë§¤í•‘í•´ì•¼ í•˜ëŠ”ë°\n   - ë§¤í•‘ ì¸í„°í˜ì´ìŠ¤ê°€ ì—†ìŒ!\n   \n3. ê²€ì¦ ê²°ê³¼ ì—†ìŒ\n   - ì—…ë¡œë“œí•œ CSVê°€ ìœ íš¨í•œê°€?\n   - ì—ëŸ¬ê°€ ìˆëŠ” í–‰ì€?\n   - ì¤‘ë³µì´ ìˆëŠ”ê°€?\n   \n4. ë¯¸ë¦¬ë³´ê¸° ì—†ìŒ\n   - ì–´ë–¤ ë°ì´í„°ê°€ ì—…ë¡œë“œë  ê±´ì§€ í™•ì¸ ë¶ˆê°€ëŠ¥\n   \n5. ì¼ê´„ ì €ì¥ ë²„íŠ¼ ë¶ˆëª…í™•\n   - ì €ì¥ ì „ í™•ì¸ ë‹¨ê³„ê°€ ì—†ìŒ\n\nâŒ UX ë¬¸ì œ:\n- \"ë“œë˜ê·¸í•´ì„œ íŒŒì¼ì„ ë“œë¡­í•˜ê±°ë‚˜ í´ë¦­\" ë©”ì‹œì§€ë§Œ ìˆê³ \n- ì‹¤ì œ ì—…ë¡œë“œ í›„ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ ë¶ˆëª…í™•\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. CSV ì—…ë¡œë“œ ì›Œí¬í”Œë¡œìš°\n   \n   Step 1: íŒŒì¼ ì„ íƒ\n   â”œâ”€ ë“œë˜ê·¸ & ë“œë¡­ ë˜ëŠ” [íŒŒì¼ ì„ íƒ] ë²„íŠ¼\n   â””â”€ íŒŒì¼ ìœ í˜• í™•ì¸ (.csvë§Œ ê°€ëŠ¥)\n   \n   Step 2: ì»¬ëŸ¼ ë§¤í•‘ (NEW!)\n   â”œâ”€ CSV ì»¬ëŸ¼ ëª©ë¡ (ì˜ˆ: ì´ë¦„, ì£¼ì†Œ, í‰ì )\n   â”œâ”€ DB í•„ë“œ ëª©ë¡ (name, address, rating ë“±)\n   â”œâ”€ ë“œë˜ê·¸ & ë“œë¡­ìœ¼ë¡œ ë§¤í•‘\n   â”‚  ì˜ˆ: CSV \"ì´ë¦„\" â†’ DB \"name\"\n   â””â”€ ìë™ ë§¤í•‘ ì œì•ˆ (ë™ì¼ ì´ë¦„)\n   \n   Step 3: ë¯¸ë¦¬ë³´ê¸° (NEW!)\n   â”œâ”€ ì²« 5í–‰ í‘œì‹œ (í…Œì´ë¸” í˜•íƒœ)\n   â”œâ”€ ê²€ì¦ ê²°ê³¼\n   â”‚  âœ… ì˜¬ë°”ë¥¸ í–‰: Nê°œ\n   â”‚  âŒ ì—ëŸ¬ í–‰: 0ê°œ\n   â”‚  âš ï¸ ì¤‘ë³µ: 0ê°œ\n   â””â”€ ì—ëŸ¬ ìƒì„¸ ë³´ê¸°\n   \n   Step 4: ì €ì¥ í™•ì¸\n   â”œâ”€ \"Nê°œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\"\n   â”œâ”€ ì¶”ì • ì²˜ë¦¬ ì‹œê°„\n   â””â”€ [ì €ì¥] [ì·¨ì†Œ] ë²„íŠ¼\n   \n   Step 5: ê²°ê³¼ í™•ì¸\n   â”œâ”€ ì§„í–‰ë¥  í‘œì‹œ (í”„ë¡œê·¸ë ˆìŠ¤ë°”)\n   â”œâ”€ ì €ì¥ ì™„ë£Œ â†’ ì™„ë£Œ ë©”ì‹œì§€\n   â””â”€ ì˜¤ë¥˜ ë°œìƒ â†’ ì˜¤ë¥˜ ë¡œê·¸\n\n2. í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ ê°œì„ \n   - í…œí”Œë¦¿ì— ì˜ˆì‹œ ë°ì´í„° í¬í•¨\n   - í•„ìˆ˜ ì»¬ëŸ¼ í‘œì‹œ (*)\n   - ë°ì´í„° í˜•ì‹ ì„¤ëª… (ì˜ˆ: í‰ì ì€ 1-5)\n\n3. ì»¬ëŸ¼ ë§¤í•‘ ì €ì¥\n   - ë§¤í•‘ ì •ë³´ ì €ì¥ (ë‹¤ìŒ ì—…ë¡œë“œ ì‹œ ì¬ì‚¬ìš©)\n   - ìì£¼ ì‚¬ìš©í•˜ëŠ” ë§¤í•‘ \"í”„ë¦¬ì…‹\"ìœ¼ë¡œ ì €ì¥\n\n4. ì—ëŸ¬ ì²˜ë¦¬\n   - ìœ íš¨í•˜ì§€ ì•Šì€ í–‰ í‘œì‹œ\n   - ê° í–‰ì˜ ì—ëŸ¬ ì´ìœ  ëª…í™•íˆ\n   - \"ìˆ˜ì • í›„ ì¬ì—…ë¡œë“œ\" vs \"ê±´ë„ˆë›°ê³  ì €ì¥\" ì„ íƒ\n```\n\n---\n\n### í™”ë©´ 4: `sujibyoceong.jpg` - ìˆ˜ì§‘ ë³´ì •(?)\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâŒ ë¶ˆëª…í™•í•œ í™”ë©´\n- \"ìˆ˜ì§‘ ë³´ì •\" ë©”ë‰´ê°€ ë­”ê°€?\n- ìš©ë„ê°€ ëª…í™•í•˜ì§€ ì•ŠìŒ\n- ì½˜í…ì¸ ê°€ ë¹„ì–´ ìˆëŠ” ê²ƒ ê°™ìŒ\n\në¬¸ì œ: ì´ ë©”ë‰´ê°€ í•„ìš”í•œê°€? \n- ì—†ìœ¼ë©´ ì‚­ì œ\n- í•„ìš”í•˜ë©´ ëª…í™•í•œ ì„¤ëª…ê³¼ ê¸°ëŠ¥ ì¶”ê°€\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. ë©”ë‰´ ìš©ë„ ëª…í™•í™”\n   - \"ìˆ˜ì§‘ ë³´ì •\"ì´ ë­”ê°€? (ì´ë¦„ ë³€ê²½ í•„ìš”)\n   - ê°€ëŠ¥í•œ ì´ë¦„:\n     * \"ë°ì´í„° ì •ì œ\" (ì¤‘ë³µ ì œê±°, í’ˆì§ˆ ê°œì„ )\n     * \"ìˆ˜ì§‘ ê²°ê³¼ ë³´ì •\" (ì˜¤ë¥˜ ìˆ˜ì •)\n     * \"ê³ ê¸‰ í¸ì§‘\" (ë³µì¡í•œ í¸ì§‘)\n   \n2. ê¸°ëŠ¥ ì¶”ê°€\n   - ë§Œì•½ \"ë°ì´í„° ì •ì œ\"ë¼ë©´:\n     * ì¤‘ë³µ ê²€ì‚¬ ë° ë³‘í•©\n     * ëˆ„ë½ í•„ë“œ ì±„ìš°ê¸°\n     * ë°ì´í„° í˜•ì‹ í†µì¼\n   \n3. ë¶ˆí•„ìš”í•˜ë©´ ì‚­ì œ\n   - ë©”ë‰´ê°€ ë¹„ì–´ìˆê³  ì„¤ëª…ì´ ì—†ìœ¼ë©´ ì¦‰ì‹œ ì œê±°\n```\n\n---\n\n### í™”ë©´ 5: `sujibgyeolgwamein.jpg` - ìˆ˜ì§‘ ê²°ê³¼ ë©”ì¸\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- í…Œì´ë¸” í‘œì‹œ (5ê°œ ë ˆìŠ¤í† ë‘)\n- í•„í„°/ì •ë ¬ ë“œë¡­ë‹¤ìš´ ìˆìŒ\n- [CSV ë‚´ë³´ë‚´ê¸°] ë²„íŠ¼\n- [ì„¸ë¶€ ì¡°íšŒ] [ì—…ë¡œë“œ ì¡°íšŒ] ë²„íŠ¼\n\nâŒ ì‹¬ê°í•œ ë¬¸ì œ:\n1. ë©”ë‰´ ì •ë³´ê°€ í…Œì´ë¸”ì— ì—†ìŒ!\n   - í…Œì´ë¸” ì»¬ëŸ¼: ì´ë¦„, ì¹´í…Œê³ ë¦¬, ì£¼ì†Œ, í‰ì , ì¸ê¸°ë„, í‹°ì–´, ìƒíƒœ\n   - ëˆ„ë½ëœ ì»¬ëŸ¼:\n     * ë©”ë‰´ (ê°€ì¥ ì¤‘ìš”!)\n     * Platform (Google/Naver/Manual)\n     * ë§í¬ (ë°”ë¡œê°€ê¸°)\n     * í’ˆì§ˆ ì ìˆ˜\n   \n2. ë©”ë‰´ ì •ë³´ê°€ ì‘ì€ í…ìŠ¤íŠ¸ë¡œë§Œ í‘œì‹œ\n   - \"ê¹€ì¹˜ì°Œê°œ, ëœì¥ì°Œê°œ\" ê°™ì€ í…ìŠ¤íŠ¸\n   - ë©”ë‰´ì™€ ê°€ê²©ì´ ë¶„ë¦¬ ì•ˆ ë¨\n   - ë©”ë‰´ ê°œìˆ˜ê°€ ë§ìœ¼ë©´ í‘œì‹œ ë¶ˆê°€ëŠ¥\n   \n3. Platform ì •ë³´ ì—†ìŒ\n   - Google / Naver / Manual êµ¬ë¶„ ì•ˆ ë³´ì„\n   - í•„í„°ë§ì´ë‚˜ ì •ë ¬ ë¶ˆê°€ëŠ¥\n   \n4. ì¸ë¼ì¸ í¸ì§‘ ë¶ˆê°€ëŠ¥\n   - ê° í–‰ì„ ë°”ë¡œ ìˆ˜ì •í•  ìˆ˜ ì—†ìŒ\n   - \"ìì„¸íˆ ë³´ê¸°\" í´ë¦­ â†’ ìƒì„¸ í˜ì´ì§€ â†’ ìˆ˜ì •\n   - ë„ˆë¬´ ë²ˆê±°ë¡œì›€\n   \n5. í’ˆì§ˆ í‘œì‹œ\n   - ìƒ‰ìƒ ì½”ë”©ì€ ìˆìœ¼ë‚˜ (ìš°ìˆ˜/ë³´í†µ/ë‚®ìŒ)\n   - ìƒì„¸ í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ ì•ˆ ë¨ (ì˜ˆ: 70%)\n\nâŒ ë„¤ë¹„ê²Œì´ì…˜ ë¬¸ì œ:\n- ìƒì„¸ ë ˆìŠ¤í† ë‘ í˜ì´ì§€ë¡œ ì´ë™ ì‹œ\n- \"ë’¤ë¡œê°€ê¸°\"ë§Œ ìˆìŒ\n- ì‚¬ì´ë“œë°”ê°€ ì—†ì–´ì•¼ í•˜ëŠ”ê°€? (ì¼ê´€ì„± í™•ì¸ í•„ìš”)\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. í…Œì´ë¸” ì»¬ëŸ¼ ì¬êµ¬ì„±\n   \n   í˜„ì¬: [ì´ë¦„] [ì¹´í…Œê³ ë¦¬] [ì£¼ì†Œ] [í‰ì ] [ì¸ê¸°ë„] [í‹°ì–´] [ìƒíƒœ]\n   \n   ë³€ê²½:\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚ì´ë¦„      â”‚ì¹´í…Œê³ ë¦¬  â”‚ë©”ë‰´    â”‚í‰ì   â”‚Platformâ”‚ìƒíƒœ  â”‚í¸ì§‘    â”‚\n   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n   â”‚ì˜¬ë˜êµ­ìˆ˜  â”‚í•œì‹      â”‚êµ­ìˆ˜... â”‚4.5  â”‚Naverâ”‚ìš°ìˆ˜  â”‚[ìˆ˜ì •]  â”‚\n   â”‚ìˆ˜ë™ì…ë ¥  â”‚í•œì‹      â”‚-      â”‚4.8  â”‚Manualâ”‚ë³´í†µ  â”‚[ìˆ˜ì •]  â”‚\n   â”‚...       â”‚...      â”‚...    â”‚... â”‚...  â”‚...   â”‚...    â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n   \n   - ì»¬ëŸ¼ ì œê±°: ì£¼ì†Œ, ì¸ê¸°ë„, í‹°ì–´ (í•„ìš” ì—†ìŒ)\n   - ì»¬ëŸ¼ ì¶”ê°€: Platform, í¸ì§‘ ë²„íŠ¼\n   \n2. ë©”ë‰´ í‘œì‹œ ê°œì„ \n   - ë©”ë‰´ 1-3ê°œë§Œ í‘œì‹œ\n   - ë” ë§ìœ¼ë©´ \"[ë©”ë‰´ ë”ë³´ê¸°]\" ë§í¬\n   - í˜¸ë²„ ì‹œ ì „ì²´ ë©”ë‰´ íŒì—… í‘œì‹œ\n   \n   ì˜ˆ:\n   \"ê¹€ì¹˜ì°Œê°œ (8,000)\\nëœì¥ì°Œê°œ (8,500)\\n[+ 2ê°œ ë”]\"\n   \n3. Platform í‘œì‹œ\n   - ì•„ì´ì½˜ìœ¼ë¡œ í‘œì‹œ (Google: G, Naver: N, Manual: M)\n   - ë˜ëŠ” ë°°ì§€ë¡œ í‘œì‹œ (íŒŒë€ìƒ‰, ì´ˆë¡ìƒ‰, íšŒìƒ‰)\n   \n4. ì¸ë¼ì¸ í¸ì§‘ ì¶”ê°€\n   - ê° í–‰ ìš°ì¸¡ì— [ìˆ˜ì •] ë²„íŠ¼\n   - í´ë¦­ ì‹œ íŒì—… ì—´ê¸° (ì œìë¦¬ í¸ì§‘)\n   - íŒì—…ì—ì„œ ìˆ˜ì • â†’ ì €ì¥\n   \n5. í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ\n   - ìƒ‰ìƒ ì™¸ì— ìˆ«ì í‘œì‹œ\n   - ì˜ˆ: \"70% ì™„ì„±ë„\" ë°°ì§€\n   \n6. í–‰ í´ë¦­ ë™ì‘\n   - í´ë¦­ â†’ ìƒì„¸ í˜ì´ì§€ ì´ë™ (í˜„ì¬)\n   - ì‚¬ì´ë“œë°” ìœ ì§€ (ì¼ê´€ì„±)\n   - \"ë’¤ë¡œê°€ê¸°\" ë²„íŠ¼ ì¶”ê°€\n```\n\n---\n\n### í™”ë©´ 6: `sujibgyeolgwanaereseutorangkeulrigsihwamyeon.jpg` - ìˆ˜ì§‘ ê²°ê³¼ > ë ˆìŠ¤í† ë‘ í´ë¦­ ì‹œ í™”ë©´\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- ê¸°ë³¸ ì •ë³´ í‘œì‹œ (ì´ë¦„, ì¹´í…Œê³ ë¦¬, ì£¼ì†Œ, ì „í™”, ì„¤ëª…)\n- í‰ì  í‘œì‹œ\n- ë¦¬ë·° ìˆ˜ í‘œì‹œ\n- ë©”ë‰´ ì •ë³´ í‘œì‹œ (ì¼ë¶€)\n\nâŒ ì‹¬ê°í•œ ë¬¸ì œ:\n1. ì™¼ìª½ ì‚¬ì´ë“œë°” ì—†ìŒ! (ì¤‘ëŒ€ ì˜¤ë¥˜)\n   - ë‹¤ë¥¸ ë©”ë‰´ë¡œ ì´ë™ ë¶ˆê°€ëŠ¥\n   - ì‚¬ìš©ìê°€ ë§‰í ìˆ˜ ìˆìŒ\n   - \"ìˆ˜ì§‘ ê²°ê³¼ë¡œ ëŒì•„ê°€ê¸°\" ë²„íŠ¼ë„ ëª…í™•í•˜ì§€ ì•ŠìŒ\n   \n2. í¸ì§‘ ë²„íŠ¼ ì—†ìŒ!\n   - ì™œ ìƒì„¸ ì •ë³´ë¥¼ ë³´ì—¬ì£¼ëŠ”ê°€?\n   - ìˆ˜ì •í•  ìˆ˜ ì—†ìŒ!\n   \n3. Google/Naver ë§í¬ ì—†ìŒ\n   - DBì—ëŠ” ìˆëŠ”ë° í‘œì‹œ ì•ˆ í•¨\n   - \"Googleì—ì„œ ë³´ê¸°\" ë²„íŠ¼ ì—†ìŒ\n   \n4. ì˜ì—…ì‹œê°„ ë¯¸í‘œì‹œ\n   - DBì— ìˆëŠ”ë° í‘œì‹œ ì•ˆ í•¨\n   \n5. í¸ì˜ì‹œì„¤ ì •ë³´ ì—†ìŒ\n   - DBì— ìˆëŠ”ë° í‘œì‹œ ì•ˆ í•¨\n   \n6. í˜ì´ì§€ ëª©ì ì´ ë¶ˆëª…í™•\n   - ì½ê¸° ì „ìš©? í¸ì§‘ ê°€ëŠ¥? ë­”ê°€?\n   - ì‚¬ìš©ìëŠ” ì—¬ê¸°ì„œ ë­˜ í•´ì•¼ í•˜ë‚˜?\n\nâŒ ë ˆì´ì•„ì›ƒ ë¬¸ì œ:\n- ë©”ë‰´ ì •ë³´ê°€ ì‘ì€ ì˜ì—­ì— ë¹„ì¢ê²Œ í‘œì‹œ\n- ì˜ì—…ì‹œê°„, í¸ì˜ì‹œì„¤ ì„¹ì…˜ ì—†ìŒ\n- \"ê¸°ë³¸ ì •ë³´\" vs \"ìƒì„¸ ì •ë³´\" êµ¬ë¶„ ëª…í™•í•˜ì§€ ì•ŠìŒ\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. ì‚¬ì´ë“œë°” ë³µêµ¬! (ë§¤ìš° ì¤‘ìš”)\n   - ì¢Œì¸¡ì— ë©”ë‰´ë°” ì¶”ê°€\n   - ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ìœ ì§€\n   \n2. [ìˆ˜ì •] ë²„íŠ¼ ì¶”ê°€\n   - ìš°ì¸¡ ìƒë‹¨ì— íŒŒë€ìƒ‰ [ìˆ˜ì •] ë²„íŠ¼\n   - í´ë¦­ ì‹œ íŒì—… ë˜ëŠ” í¸ì§‘ ëª¨ë“œë¡œ\n   \n3. ë ˆì´ì•„ì›ƒ ì¬ì„¤ê³„\n   \n   [ê¸°ë³¸ ì •ë³´]\n   â”œâ”€ ì´ë¦„: ì˜¬ë˜êµ­ìˆ˜\n   â”œâ”€ ì¹´í…Œê³ ë¦¬: í•œì‹\n   â”œâ”€ ì£¼ì†Œ: ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123\n   â””â”€ ì „í™”: 02-1234-5678\n   \n   [í‰ê°€ ì •ë³´]\n   â”œâ”€ í‰ì : â­â­â­â­â˜† (4.5)\n   â”œâ”€ ë¦¬ë·° ìˆ˜: 40ê°œ\n   â””â”€ í‰ê°€ ì‚¬ìœ : \"ë§›ìˆê³  ë¶„ìœ„ê¸° ì¢‹ìŒ\"\n   \n   [ë©”ë‰´] (ì„¹ì…˜ ê°•í™”)\n   â”œâ”€ ê¹€ì¹˜ì°Œê°œ (8,000ì›)\n   â”‚  â””â”€ \"ì§„í•œ ë§›ì´ ì¼í’ˆ\"\n   â”œâ”€ ëœì¥ì°Œê°œ (8,500ì›)\n   â”‚  â””â”€ \"í‘¸ì§í•œ ì–‘\"\n   â””â”€ [ë©”ë‰´ ë”ë³´ê¸°] ë˜ëŠ” ëª¨ë‘ í‘œì‹œ\n   \n   [ì˜ì—…ì‹œê°„] (NEW!)\n   â”œâ”€ ì›”-ê¸ˆ: 10:00 ~ 22:00\n   â”œâ”€ í† : 11:00 ~ 23:00\n   â”œâ”€ ì¼: 11:00 ~ 22:00\n   â””â”€ íœ´ë¬´ì¼: ì—†ìŒ\n   \n   [í¸ì˜ì‹œì„¤] (NEW!)\n   â”œâ”€ ğŸ…¿ï¸ ì£¼ì°¨ ê°€ëŠ¥\n   â”œâ”€ ğŸ“¶ ì™€ì´íŒŒì´ ìˆìŒ\n   â”œâ”€ âœ… ì˜ˆì•½ ê°€ëŠ¥\n   â””â”€ ğŸš— ë°°ë‹¬ ê°€ëŠ¥\n   \n   [ë§í¬] (NEW!)\n   â”œâ”€ [Googleì—ì„œ ë³´ê¸°] â†’ Google Maps ì—´ê¸°\n   â””â”€ [ë„¤ì´ë²„ì—ì„œ ë³´ê¸°] â†’ Naver Map ì—´ê¸°\n   \n   [ìƒì„¸ ì„¤ëª…]\n   â””â”€ (ì›ë³¸ ì„¤ëª… í…ìŠ¤íŠ¸)\n\n4. ìš°ì¸¡ ìƒë‹¨ ë²„íŠ¼\n   â”œâ”€ [ìˆ˜ì •] (íŒŒë€ìƒ‰) - í¸ì§‘ íŒì—… ì—´ê¸°\n   â”œâ”€ [ì‚­ì œ] (ë¹¨ê°•ìƒ‰) - ì‚­ì œ í™•ì¸\n   â””â”€ [ë’¤ë¡œê°€ê¸°] (íšŒìƒ‰) - ëª©ë¡ìœ¼ë¡œ\n\n5. í˜ì´ì§€ ëª©ì  ëª…í™•í™”\n   - ì œëª©: \"ì˜¬ë˜êµ­ìˆ˜ - ìƒì„¸ì •ë³´\"\n   - ë˜ëŠ” \"ì˜¬ë˜êµ­ìˆ˜ - ê²€í†  ë° í¸ì§‘\"\n```\n\n---\n\n### í™”ë©´ 7: `saeyoceongpabeobhwamyeon.jpg` - ìƒˆ ìš”ì²­ íŒì—… í™”ë©´\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- íŒì—… êµ¬ì¡° ìˆìŒ\n- í•„ë“œë“¤ì´ í‘œì‹œë¨\n\nâŒ ì‹¬ê°í•œ ë¬¸ì œ:\n1. íŒì—…ì´ ë„ˆë¬´ ì‘ìŒ\n   - ë‚´ìš©ì´ ë‹¤ ë³´ì´ì§€ ì•ŠìŒ\n   - ìŠ¤í¬ë¡¤ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ\n   \n2. í•„ë“œ ê°œìˆ˜ ê³¼ë‹¤\n   - ì†ŒìŠ¤/ì´ìŠˆ, ì„¤ëª…, ì§€ì • ëŒ€ìƒ, ì¹´í…Œê³ ë¦¬, ëŒ€ìƒ URL, ìˆ˜ì§‘ ë²”ìœ„\n   - ëª¨ë‘ í•œ íŒì—…ì— ìˆìŒ\n   - í•„ìˆ˜/ì„ íƒ êµ¬ë¶„ ëª…í™•í•˜ì§€ ì•ŠìŒ\n   \n3. ë¹„ìš© í‘œì‹œ (í•˜ë‹¨ \"$25.00\")\n   - ìˆ˜ì§‘ ìš”ì²­ ë¹„ìš©ì´ ìˆë‹¤ëŠ” ëœ»ì¸ê°€?\n   - ì„¤ëª… ì—†ìŒ\n   \n4. ì €ì¥/ì·¨ì†Œ ë²„íŠ¼\n   - ëª…í™•í•˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸\n\nâŒ ë‚´ìš© ë¬¸ì œ:\n- \"ìƒˆ ìˆ˜ì§‘ ìš”ì²­\"ì¸ë° í•„ë“œê°€ ë­”ê°€?\n- ì–´ë–¤ ì •ë³´ë¥¼ ì…ë ¥í•´ì•¼ í•˜ëŠ”ê°€?\n- ê° í•„ë“œì˜ ì„¤ëª…/íŒíŠ¸ ì—†ìŒ\n\nìš”êµ¬ì‚¬í•­: \n- ì´ íŒì—…ì˜ ëª©ì ì´ ë­”ê°€?\n- \"ìƒˆ ìˆ˜ì§‘ ìš”ì²­\"ì€ ë­”ê°€? (API í˜¸ì¶œ? ì‚¬ìš©ì ìš”ì²­?)\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. íŒì—… í¬ê¸° í™•ëŒ€\n   - í™”ë©´ ë„ˆë¹„ 60-80% ì°¨ì§€\n   - ë†’ì´ ìë™ (ìŠ¤í¬ë¡¤ í•„ìš” ì‹œ)\n   \n2. í•„ë“œ êµ¬ì¡° ì •ë¦¬\n   \n   í•„ìˆ˜ í•„ë“œ (*)\n   â”œâ”€ ì†ŒìŠ¤/ì´ìŠˆ * (ë“œë¡­ë‹¤ìš´)\n   â”‚  â””â”€ ì˜µì…˜: \"Google í˜ì´ì§€\", \"Naver ê²€ìƒ‰\", \"ê¸°íƒ€\"\n   â””â”€ ì„¤ëª… * (í…ìŠ¤íŠ¸ 500ì)\n      â””â”€ íŒíŠ¸: \"ìˆ˜ì§‘ ìš”ì²­ì˜ ìƒì„¸ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”\"\n   \n   ì„ íƒ í•„ë“œ\n   â”œâ”€ ì§€ì • ëŒ€ìƒ (í…ìŠ¤íŠ¸)\n   â”‚  â””â”€ íŒíŠ¸: \"íŠ¹ì • ì¹´í…Œê³ ë¦¬ë‚˜ ì§€ì—­ (ì˜ˆ: ê°•ë‚¨êµ¬ í•œì‹ë‹¹)\"\n   â”œâ”€ ì¹´í…Œê³ ë¦¬ (ë©€í‹°ì…€ë ‰íŠ¸)\n   â”‚  â””â”€ ì˜µì…˜: \"í•œì‹\", \"ì¤‘ì‹\", \"ì¼ì‹\", ...\n   â”œâ”€ ëŒ€ìƒ URL (í…ìŠ¤íŠ¸)\n   â”‚  â””â”€ íŒíŠ¸: \"ìˆ˜ì§‘ ëŒ€ìƒ URL ì…ë ¥\"\n   â””â”€ ìˆ˜ì§‘ ë²”ìœ„ (ìŠ¬ë¼ì´ë”)\n      â””â”€ \"1-100ê°œ\" ë²”ìœ„ ì„ íƒ\n   \n3. ë¹„ìš© í‘œì‹œ ê°œì„ \n   - \"ì˜ˆìƒ ë¹„ìš©: $25.00\" (ëª…í™•í•œ ì„¤ëª…)\n   - ë¹„ìš© ê³„ì‚° ë°©ì‹ ì„¤ëª…\n   - \"ë¹„ìš© ì„¸ë¶€ì •ë³´\" ë§í¬\n   \n4. ë²„íŠ¼ ëª…í™•í™”\n   â”œâ”€ [ìš”ì²­] (ì´ˆë¡ìƒ‰) - ìš”ì²­ ì €ì¥\n   â””â”€ [ì·¨ì†Œ] (íšŒìƒ‰) - íê¸°\n   \n5. ìš”ì²­ ì™„ë£Œ í›„ í™•ì¸\n   - \"ìš”ì²­ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤\"\n   - ìš”ì²­ ID í‘œì‹œ\n   - \"ì§„í–‰ ìƒí™© í™•ì¸\" ë§í¬\n```\n\n---\n\n### í™”ë©´ 8: `tonghabpyeonjibmein.jpg` - í†µí•© í¸ì§‘ ë©”ì¸\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- í…Œì´ë¸” í‘œì‹œ (5ê°œ ë ˆìŠ¤í† ë‘)\n- [í¸ì§‘] ë²„íŠ¼ ìˆìŒ\n- ì •ë ¬/í•„í„° ë“œë¡­ë‹¤ìš´\n\nâŒ ì‹¬ê°í•œ ë¬¸ì œ:\n1. í…Œì´ë¸” ì»¬ëŸ¼ ëª…í™•í•˜ì§€ ì•ŠìŒ\n   - ì–´ë–¤ ë°ì´í„°ë¥¼ í¸ì§‘í•  ìˆ˜ ìˆëŠ”ê°€?\n   - ê° ì»¬ëŸ¼ì´ ë­”ê°€?\n   \n2. ì¸ë¼ì¸ í¸ì§‘ ë¶ˆê°€ëŠ¥\n   - [í¸ì§‘] ë²„íŠ¼ í´ë¦­ â†’ íŒì—… ì—´ê¸° (ë²ˆê±°ë¡œì›€)\n   - ë¹ ë¥¸ ìˆ˜ì • ë¶ˆê°€ëŠ¥\n   \n3. ê° í–‰ì˜ ìƒíƒœ í‘œì‹œ ì—†ìŒ\n   - ì–´ë–¤ í•­ëª©ì„ í¸ì§‘í–ˆëŠ”ê°€?\n   - ë³€ê²½ ì—¬ë¶€ í‘œì‹œ ì—†ìŒ\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. í…Œì´ë¸” ì»¬ëŸ¼ ëª…í™•í™”\n   â”œâ”€ ì´ë¦„\n   â”œâ”€ ìƒíƒœ (í™•ì¸ ì¤‘ / ì™„ë£Œ / ê±°ì ˆ)\n   â”œâ”€ ìˆ˜ì •ì (ì´ë¦„)\n   â”œâ”€ ìˆ˜ì • ì¼ì‹œ\n   â””â”€ ì•¡ì…˜ ([ìˆ˜ì •] [ì‚­ì œ] [ì›ë³¸ ë³µì›])\n\n2. ì¸ë¼ì¸ í¸ì§‘ ì¶”ê°€ (ì„ íƒì‚¬í•­)\n   - ë˜ëŠ” íŒì—…ë§Œ ìœ ì§€ (í˜„ì¬ ë°©ì‹)\n   \n3. í¸ì§‘ íŒì—… (ë‹¤ìŒ í™”ë©´ ì°¸ì¡°)\n```\n\n---\n\n### í™”ë©´ 9: `tonghabpyeonjingnaepyeonjibbeoteunsi-pabeobhwamyeon.jpg` - í†µí•© í¸ì§‘ > íŒì—…\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- íŒì—… êµ¬ì¡° ìˆìŒ\n- ì´ë¦„, ì¹´í…Œê³ ë¦¬, ì£¼ì†Œ, í‰ì , í‰ê°€ í•„ë“œ\n\nâŒ ì‹¬ê°í•œ ë¬¸ì œ (ì´ì „ ë¶„ì„ê³¼ ë™ì¼):\n1. í•„ë“œê°€ ë„ˆë¬´ ì ìŒ!\n   - DBì—ëŠ” ìˆëŠ”ë° í‘œì‹œ ì•ˆ ë¨:\n     * ë©”ë‰´ ì •ë³´\n     * ì˜ì—…ì‹œê°„\n     * í¸ì˜ì‹œì„¤\n     * Google/Naver Place ID\n     * Platform\n     * ì´ë¯¸ì§€ URL\n   \n2. íŒì—… í¬ê¸° ë„ˆë¬´ ì‘ìŒ\n   - ëª¨ë“  í•„ë“œë¥¼ í‘œì‹œí•˜ê¸° ë¶€ì¡±\n   \n3. íƒ­ êµ¬ì¡° ì—†ìŒ\n   - \"ê¸°ë³¸ì •ë³´\", \"ìƒì„¸ì •ë³´\", \"ë©”ë‰´\", \"ë§í¬\" íƒ­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì•¼ í•¨\n   \n4. ë©”ë‰´ í•„ë“œ ì‘ìŒ\n   - ë©”ë‰´ë¥¼ ì œëŒ€ë¡œ ì…ë ¥í•  ìˆ˜ ì—†ìŒ\n   - ë™ì  ì¶”ê°€ ë¶ˆê°€ëŠ¥\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. íŒì—… í¬ê¸° í™•ëŒ€\n   - í™”ë©´ 70-80% ì°¨ì§€\n   - ë†’ì´ ìë™\n\n2. íƒ­ êµ¬ì¡° ë„ì… (ë§¤ìš° ì¤‘ìš”!)\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚ [ê¸°ë³¸ì •ë³´] [ìƒì„¸ì •ë³´] [ë©”ë‰´] [ë§í¬] â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n   [ê¸°ë³¸ì •ë³´] íƒ­\n   â”œâ”€ ì´ë¦„ * (í•„ìˆ˜)\n   â”œâ”€ ì¹´í…Œê³ ë¦¬ * (í•„ìˆ˜)\n   â”œâ”€ ì£¼ì†Œ * (í•„ìˆ˜)\n   â”œâ”€ ì „í™” (ì„ íƒ)\n   â””â”€ ì„¤ëª… (ì„ íƒ)\n\n   [ìƒì„¸ì •ë³´] íƒ­\n   â”œâ”€ í‰ì  (1-5 ìŠ¬ë¼ì´ë”)\n   â”œâ”€ ë¦¬ë·° ìˆ˜\n   â”œâ”€ ê°€ê²©ëŒ€ (ì €/ì¤‘/ê³ )\n   â”œâ”€ í‰ê°€ ì‚¬ìœ  (í…ìŠ¤íŠ¸ 400ì)\n   â””â”€ ìƒíƒœ (ë¼ë””ì˜¤: í™•ì¸ì¤‘/ì™„ë£Œ/ê±°ì ˆ)\n\n   [ë©”ë‰´] íƒ­ (NEW!)\n   â”œâ”€ [ë©”ë‰´ ì¶”ê°€] ë²„íŠ¼\n   â”œâ”€ ë©”ë‰´ 1\n   â”‚  â”œâ”€ ë©”ë‰´ëª… (ê¹€ì¹˜ì°Œê°œ)\n   â”‚  â”œâ”€ ê°€ê²© (8000)\n   â”‚  â””â”€ ì„¤ëª… (ì§„í•œ ë§›)\n   â”œâ”€ ë©”ë‰´ 2\n   â””â”€ ...\n\n   [ë§í¬] íƒ­ (NEW!)\n   â”œâ”€ Google Place ID\n   â”œâ”€ Naver Place ID\n   â”œâ”€ ì›ë³¸ URL\n   â”œâ”€ Platform (Google/Naver/Manual)\n   â””â”€ í¸ì˜ì‹œì„¤ (ì²´í¬ë°•ìŠ¤)\n\n3. í•„ë“œ ê²€ì¦\n   - í•„ìˆ˜ í•„ë“œ í‘œì‹œ (*)\n   - ì €ì¥ ì „ ê²€ì¦\n\n4. ì €ì¥/ì·¨ì†Œ ë²„íŠ¼\n   â”œâ”€ [ì €ì¥] (ì´ˆë¡ìƒ‰) - ë³€ê²½ ì €ì¥\n   â””â”€ [ì·¨ì†Œ] (íšŒìƒ‰) - íê¸°\n\n5. ë©”ë‰´ ì¶”ê°€ (ë™ì )\n   - [ë©”ë‰´ ì¶”ê°€] í´ë¦­ â†’ ìƒˆ í–‰ ì¶”ê°€\n   - ê° ë©”ë‰´ ì˜†ì— [X] ë²„íŠ¼ìœ¼ë¡œ ì‚­ì œ\n```\n\n---\n\n### í™”ë©´ 10: `urlibryeogjungjigjeobibryeoghwamyeon.jpg` - URL ì…ë ¥ ì¤‘ ì‘ì—… í™”ë©´\n\n#### í˜„ì¬ ìƒíƒœ\n```\nâœ… ê¸ì •ì :\n- URL ì…ë ¥ í•„ë“œ ìˆìŒ\n- íŒŒì‹±í•˜ê¸° ë²„íŠ¼ ìˆìŒ\n\nâŒ ì‹¬ê°í•œ ë¬¸ì œ:\n1. URL ì…ë ¥ í›„ ê²°ê³¼ í‘œì‹œ ì—†ìŒ\n   - íŒŒì‹± í›„ ì–´ë””ë¡œ ê°€ëŠ”ê°€?\n   - ê²°ê³¼ í™•ì¸ ë¶ˆê°€ëŠ¥\n   \n2. URL ê²€ì¦ ì—†ìŒ\n   - ìœ íš¨í•œ URLì¸ê°€?\n   - í”Œë«í¼ì„ ìë™ ê°ì§€í•˜ëŠ”ê°€?\n   \n3. íŒŒì‹± ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ì—†ìŒ\n   - ì–´ë–¤ ë°ì´í„°ê°€ íŒŒì‹±ë˜ë‚˜?\n   - íŒŒì‹± ì˜¤ë¥˜ê°€ ìˆëŠ”ê°€?\n   \n4. ì €ì¥ ë°©ì‹ ë¶ˆëª…í™•\n   - íŒŒì‹± í›„ \"ì €ì¥\"ì¸ê°€?\n   - ìë™ ì €ì¥ì¸ê°€?\n```\n\n#### í•„ìˆ˜ ìˆ˜ì •ì‚¬í•­\n\n```\n1. URL ì…ë ¥ ì›Œí¬í”Œë¡œìš°\n   \n   Step 1: URL ì…ë ¥\n   â”œâ”€ URL ì…ë ¥ í•„ë“œ\n   â”œâ”€ í”Œë«í¼ ìë™ ê°ì§€ (Google/Naver/Kakao)\n   â””â”€ [íŒŒì‹±í•˜ê¸°] ë²„íŠ¼\n   \n   Step 2: íŒŒì‹± ê²°ê³¼ (NEW!)\n   â”œâ”€ íŒŒì‹± ìƒíƒœ í‘œì‹œ\n   â”‚  â”œâ”€ \"íŒŒì‹± ì¤‘...\" (ë¡œë”©)\n   â”‚  â”œâ”€ \"íŒŒì‹± ì™„ë£Œ\" (ì„±ê³µ)\n   â”‚  â””â”€ \"íŒŒì‹± ì‹¤íŒ¨\" (ì˜¤ë¥˜)\n   â”œâ”€ íŒŒì‹±ëœ ë°ì´í„° í‘œì‹œ (í…Œì´ë¸”/ì¹´ë“œ í˜•íƒœ)\n   â”‚  â”œâ”€ ì´ë¦„\n   â”‚  â”œâ”€ ì£¼ì†Œ\n   â”‚  â”œâ”€ í‰ì \n   â”‚  â”œâ”€ ë©”ë‰´ (ìµœëŒ€ 5ê°œ)\n   â”‚  â””â”€ ì˜ì—…ì‹œê°„\n   â””â”€ [ìˆ˜ì •] [ì €ì¥] [ë‹¤ì‹œ íŒŒì‹±] ë²„íŠ¼\n   \n   Step 3: ì €ì¥ í™•ì¸\n   â”œâ”€ \"ë°ì´í„°ë¥¼ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\"\n   â””â”€ [ì €ì¥] [ì·¨ì†Œ] ë²„íŠ¼\n   \n   Step 4: ê²°ê³¼\n   â”œâ”€ \"ì €ì¥ ì™„ë£Œ\"\n   â””â”€ \"ë‹¤ë¥¸ ë°ì´í„° ì…ë ¥\" ë˜ëŠ” \"ëª©ë¡ìœ¼ë¡œ\"\n\n2. ì˜¤ë¥˜ ì²˜ë¦¬\n   - URLì´ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê²½ê³ \n   - íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ìƒì„¸ ì˜¤ë¥˜ ë©”ì‹œì§€\n   - \"ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥\" ì˜µì…˜\n\n3. í”Œë«í¼ ìë™ ê°ì§€\n   - Google Maps: https://maps.google.com/...\n   - Naver Map: https://map.naver.com/...\n   - Kakao Map: https://map.kakao.com/...\n```\n\n---\n\n## ğŸ¯ í†µí•© ê²°ë¡ \n\n### ì‹¬ê°ë„ ìˆœì„œ (ë†’ìŒ â†’ ë‚®ìŒ)\n\n| ìˆœìœ„ | í•­ëª© | ì‹¬ê°ë„ | í•„ìˆ˜ ìˆ˜ì • |\n|------|------|--------|---------|\n| 1 | íŒì—… í•„ë“œ ë¶€ì¡± (ë©”ë‰´, ì˜ì—…ì‹œê°„, í¸ì˜ì‹œì„¤ ë“±) | ğŸ”´ ë§¤ìš° ë†’ìŒ | í•„ìˆ˜ |\n| 2 | ë ˆìŠ¤í† ë‘ ìƒì„¸ í˜ì´ì§€ ì‚¬ì´ë“œë°” ì—†ìŒ | ğŸ”´ ë§¤ìš° ë†’ìŒ | í•„ìˆ˜ |\n| 3 | í¸ì§‘ íŒì—… íƒ­ êµ¬ì¡° ì—†ìŒ | ğŸ”´ ë†’ìŒ | í•„ìˆ˜ |\n| 4 | ë°ì´í„° ì…ë ¥ ë©”ë‰´/ì˜ì—…ì‹œê°„/í¸ì˜ì‹œì„¤ ì…ë ¥ ë¶ˆê°€ | ğŸ”´ ë†’ìŒ | í•„ìˆ˜ |\n| 5 | CSV ì—…ë¡œë“œ ì»¬ëŸ¼ ë§¤í•‘ UI ì—†ìŒ | ğŸ”´ ë†’ìŒ | í•„ìˆ˜ |\n| 6 | ë©”ì¸ ëŒ€ì‹œë³´ë“œ ì •ë³´ ê³¼ì‰ | ğŸŸ¡ ì¤‘ê°„ | ê¶Œì¥ |\n| 7 | ìˆ˜ì§‘ ê²°ê³¼ í…Œì´ë¸” ë©”ë‰´/Platform ì»¬ëŸ¼ ì—†ìŒ | ğŸŸ¡ ì¤‘ê°„ | ê¶Œì¥ |\n| 8 | URL íŒŒì‹± ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ì—†ìŒ | ğŸŸ¡ ì¤‘ê°„ | ê¶Œì¥ |\n\n---\n\n## âš ï¸ CTOì˜ ìµœì¢… íŒë‹¨\n\n```\ní˜„í™©:\n- ë°°í¬ ê°€ëŠ¥í•˜ë‹¤ê³  íŒë‹¨: í‹€ë ¸ìŒ\n- ì‹¬ê°í•œ ê²°í•¨ ë‹¤ìˆ˜ ë°œê²¬\n- íŠ¹íˆ íŒì—…ê³¼ í¸ì§‘ ê¸°ëŠ¥ì´ ë¶ˆì™„ì „\n\nê¶Œê³ ì‚¬í•­:\n1. ì¦‰ì‹œ ë°°í¬ ì¤‘ë‹¨\n2. ìš°ì„ ìˆœìœ„ 1-5 í•­ëª© í•„ìˆ˜ ìˆ˜ì •\n3. ìˆ˜ì • í›„ ì¬ê²€í†  í•„ìš”\n\nì¶”ì • ìˆ˜ì • ê¸°ê°„: 3-5ì¼\nì¶”ì • ë¹„ìš©: $15-25 (Replit ì‚¬ìš©)\n```\n\n---\n\n**ì´ì œ êµ¬ì²´ì ì¸ ìˆ˜ì • ì§€ì‹œë¥¼ ì›í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”!**\n","size_bytes":24909},"data-hub/src/api/unified_editor_routes.py":{"content":"\"\"\"\nStage C-4: í†µí•© í¸ì§‘ & ë³‘í•© ì‹œìŠ¤í…œ\n- í¬ë¡¤ë§ + ìˆ˜ë™ ì…ë ¥ ë°ì´í„°ë¥¼ í•œ ê³³ì—ì„œ í¸ì§‘/ë³‘í•© ê´€ë¦¬\n\"\"\"\n\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Any, Optional, List, Dict\nimport json\nfrom datetime import datetime\n\nrouter = APIRouter(prefix=\"/api/data-management/unified\", tags=[\"Unified Editor\"])\n\n\nclass EditRequest(BaseModel):\n    name: Optional[str] = None\n    category: Optional[str] = None\n    address: Optional[str] = None\n    phone: Optional[str] = None\n    latitude: Optional[float] = None\n    longitude: Optional[float] = None\n    rating: Optional[float] = None\n    review_count: Optional[int] = None\n    business_hours: Optional[Dict] = None\n    menu_items: Optional[List[Dict]] = None\n    price_range: Optional[str] = None\n    youtube_mention_count: Optional[int] = None\n    blog_mention_count: Optional[int] = None\n    edited_by: Optional[str] = \"admin\"\n\n\nclass MergeRequest(BaseModel):\n    primary_id: str\n    secondary_ids: List[str]\n    merge_strategy: str = \"primary\"\n\n\nclass StatusChangeRequest(BaseModel):\n    status: str\n    reason: Optional[str] = None\n    updated_by: Optional[str] = \"admin\"\n\n\ndef get_db():\n    from src.database.connection import get_db as _get_db\n    db_gen = _get_db()\n    db = next(db_gen)\n    try:\n        yield db\n    finally:\n        try:\n            next(db_gen)\n        except StopIteration:\n            pass\n\n\n@router.put(\"/edit/{restaurant_id}\")\nasync def edit_restaurant(restaurant_id: str, request: EditRequest, db: Any = Depends(get_db)):\n    \"\"\"\n    C-4-1: ë ˆìŠ¤í† ë‘ ì •ë³´ í¸ì§‘\n    \"\"\"\n    from sqlalchemy import text\n    \n    try:\n        update_fields = []\n        update_values = {\"id\": restaurant_id}\n        \n        if request.name is not None:\n            update_fields.append(\"name = :name\")\n            update_values[\"name\"] = request.name\n        \n        if request.category is not None:\n            update_fields.append(\"category = :category\")\n            update_values[\"category\"] = request.category\n        \n        if request.address is not None:\n            update_fields.append(\"address = :address\")\n            update_values[\"address\"] = request.address\n        \n        if request.phone is not None:\n            update_fields.append(\"phone = :phone\")\n            update_values[\"phone\"] = request.phone\n        \n        if request.latitude is not None:\n            update_fields.append(\"latitude = :latitude\")\n            update_values[\"latitude\"] = request.latitude\n        \n        if request.longitude is not None:\n            update_fields.append(\"longitude = :longitude\")\n            update_values[\"longitude\"] = request.longitude\n        \n        if request.rating is not None:\n            update_fields.append(\"rating = :rating\")\n            update_values[\"rating\"] = request.rating\n        \n        if request.review_count is not None:\n            update_fields.append(\"review_count = :review_count\")\n            update_values[\"review_count\"] = request.review_count\n        \n        if request.business_hours is not None:\n            update_fields.append(\"business_hours = :business_hours\")\n            update_values[\"business_hours\"] = json.dumps(request.business_hours)\n        \n        if request.menu_items is not None:\n            update_fields.append(\"menu_items = :menu_items\")\n            update_values[\"menu_items\"] = json.dumps(request.menu_items)\n        \n        if request.price_range is not None:\n            update_fields.append(\"price_range = :price_range\")\n            update_values[\"price_range\"] = request.price_range\n        \n        if request.youtube_mention_count is not None:\n            update_fields.append(\"youtube_mention_count = :youtube_mention_count\")\n            update_values[\"youtube_mention_count\"] = request.youtube_mention_count\n        \n        if request.blog_mention_count is not None:\n            update_fields.append(\"blog_mention_count = :blog_mention_count\")\n            update_values[\"blog_mention_count\"] = request.blog_mention_count\n        \n        if not update_fields:\n            raise HTTPException(status_code=400, detail=\"ìˆ˜ì •í•  í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤\")\n        \n        update_fields.append(\"edit_status = :edit_status\")\n        update_values[\"edit_status\"] = \"edited\"\n        \n        update_fields.append(\"edited_by = :edited_by\")\n        update_values[\"edited_by\"] = request.edited_by\n        \n        update_fields.append(\"edited_at = :edited_at\")\n        update_values[\"edited_at\"] = datetime.utcnow()\n        \n        update_fields.append(\"updated_at = :updated_at\")\n        update_values[\"updated_at\"] = datetime.utcnow()\n        \n        update_query = text(f\"\"\"\n        UPDATE collection_results\n        SET {', '.join(update_fields)}\n        WHERE id = :id\n        RETURNING id, name, edit_status, updated_at\n        \"\"\")\n        \n        result = db.execute(update_query, update_values)\n        row = result.fetchone()\n        db.commit()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        return {\n            \"success\": True,\n            \"data\": {\n                \"id\": row[0],\n                \"name\": row[1],\n                \"edit_status\": row[2],\n                \"updated_at\": row[3].isoformat() if row[3] else None\n            },\n            \"message\": \"ë ˆìŠ¤í† ë‘ ì •ë³´ê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤\"\n        }\n    \n    except HTTPException:\n        db.rollback()\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ìˆ˜ì • ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n\n\n@router.post(\"/merge\")\nasync def merge_restaurants(request: MergeRequest, db: Any = Depends(get_db)):\n    \"\"\"\n    C-4-2: ì¤‘ë³µ ë ˆìŠ¤í† ë‘ ë³‘í•©\n    \"\"\"\n    from sqlalchemy import text\n    \n    try:\n        primary_query = text(\"\"\"\n        SELECT * FROM collection_results WHERE id = :id\n        \"\"\")\n        \n        primary_result = db.execute(primary_query, {\"id\": request.primary_id})\n        primary_row = primary_result.fetchone()\n        \n        if not primary_row:\n            raise HTTPException(status_code=404, detail=\"ê¸°ì¤€ ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        delete_query = text(\"\"\"\n        DELETE FROM collection_results WHERE id = ANY(:ids)\n        \"\"\")\n        \n        db.execute(delete_query, {\"ids\": request.secondary_ids})\n        \n        update_query = text(\"\"\"\n        UPDATE collection_results\n        SET edit_status = 'edited',\n            is_duplicate = false,\n            updated_at = :updated_at\n        WHERE id = :id\n        RETURNING id, name\n        \"\"\")\n        \n        result = db.execute(update_query, {\n            \"id\": request.primary_id,\n            \"updated_at\": datetime.utcnow()\n        })\n        \n        row = result.fetchone()\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"data\": {\n                \"merged_id\": row[0],\n                \"name\": row[1],\n                \"removed_count\": len(request.secondary_ids)\n            },\n            \"message\": f\"{len(request.secondary_ids)}ê°œ ì¤‘ë³µ í•­ëª©ì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤\"\n        }\n    \n    except HTTPException:\n        db.rollback()\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ë³‘í•© ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n\n\n@router.post(\"/status/{restaurant_id}\")\nasync def change_status(restaurant_id: str, request: StatusChangeRequest, db: Any = Depends(get_db)):\n    \"\"\"\n    C-4-3: ë ˆìŠ¤í† ë‘ ìƒíƒœ ë³€ê²½ (approved/rejected/excluded)\n    \"\"\"\n    from sqlalchemy import text\n    \n    valid_statuses = [\"pending\", \"edited\", \"approved\", \"rejected\", \"excluded\"]\n    if request.status not in valid_statuses:\n        raise HTTPException(\n            status_code=400, \n            detail=f\"ìœ íš¨í•˜ì§€ ì•Šì€ ìƒíƒœì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_statuses)}\"\n        )\n    \n    try:\n        update_query = text(\"\"\"\n        UPDATE collection_results\n        SET edit_status = :status,\n            edited_by = :updated_by,\n            edited_at = :edited_at,\n            updated_at = :updated_at\n        WHERE id = :id\n        RETURNING id, name, edit_status\n        \"\"\")\n        \n        result = db.execute(update_query, {\n            \"id\": restaurant_id,\n            \"status\": request.status,\n            \"updated_by\": request.updated_by,\n            \"edited_at\": datetime.utcnow(),\n            \"updated_at\": datetime.utcnow()\n        })\n        \n        row = result.fetchone()\n        db.commit()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ë ˆìŠ¤í† ë‘ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        return {\n            \"success\": True,\n            \"data\": {\n                \"id\": row[0],\n                \"name\": row[1],\n                \"status\": row[2]\n            },\n            \"message\": f\"ìƒíƒœê°€ '{request.status}'ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤\"\n        }\n    \n    except HTTPException:\n        db.rollback()\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ìƒíƒœ ë³€ê²½ ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n\n\n@router.get(\"/list\")\nasync def get_unified_list(\n    source: Optional[str] = None,\n    edit_status: Optional[str] = None,\n    limit: int = 50,\n    offset: int = 0,\n    db: Any = Depends(get_db)\n):\n    \"\"\"\n    C-4-4: í†µí•© ë°ì´í„° ëª©ë¡ ì¡°íšŒ (í¬ë¡¤ë§ + ìˆ˜ë™ ì…ë ¥)\n    \"\"\"\n    from sqlalchemy import text\n    \n    try:\n        conditions = []\n        params = {\"limit\": limit, \"offset\": offset}\n        \n        if source:\n            conditions.append(\"source = :source\")\n            params[\"source\"] = source\n        \n        if edit_status:\n            conditions.append(\"edit_status = :edit_status\")\n            params[\"edit_status\"] = edit_status\n        \n        where_clause = \"WHERE \" + \" AND \".join(conditions) if conditions else \"\"\n        \n        query = text(f\"\"\"\n        SELECT \n            id, name, category, address, phone, rating, review_count,\n            popularity_score, quality_score, source, edit_status,\n            is_duplicate, created_at, updated_at\n        FROM collection_results\n        {where_clause}\n        ORDER BY updated_at DESC\n        LIMIT :limit OFFSET :offset\n        \"\"\")\n        \n        result = db.execute(query, params)\n        rows = result.fetchall()\n        \n        count_query = text(f\"\"\"\n        SELECT COUNT(*) FROM collection_results {where_clause}\n        \"\"\")\n        count_result = db.execute(count_query, {k: v for k, v in params.items() if k not in [\"limit\", \"offset\"]})\n        total = count_result.fetchone()[0]\n        \n        data = []\n        for row in rows:\n            data.append({\n                \"id\": row[0],\n                \"name\": row[1],\n                \"category\": row[2],\n                \"address\": row[3],\n                \"phone\": row[4],\n                \"rating\": float(row[5]) if row[5] else None,\n                \"review_count\": row[6],\n                \"popularity_score\": float(row[7]) if row[7] else None,\n                \"quality_score\": float(row[8]) if row[8] else None,\n                \"source\": row[9],\n                \"edit_status\": row[10],\n                \"is_duplicate\": row[11],\n                \"created_at\": row[12].isoformat() if row[12] else None,\n                \"updated_at\": row[13].isoformat() if row[13] else None\n            })\n        \n        return {\n            \"success\": True,\n            \"data\": data,\n            \"total\": total,\n            \"limit\": limit,\n            \"offset\": offset\n        }\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n","size_bytes":11869},"data-hub/src/api/collection_result_routes.py":{"content":"\"\"\"\nCollection Results API Routes\nìˆ˜ì§‘ëœ ë ˆìŠ¤í† ë‘ ë°ì´í„° ê´€ë¦¬ API\n\"\"\"\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import text\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nimport uuid\nimport json\n\nfrom src.database.connection import get_db\n\nrouter = APIRouter(prefix=\"/api/data-management/collection-results\", tags=[\"Collection Results\"])\n\n\n# Pydantic Models\nclass CollectionResultCreate(BaseModel):\n    request_id: str\n    name: str\n    category: Optional[str] = None\n    address: Optional[str] = None\n    phone: Optional[str] = None\n    latitude: Optional[float] = None\n    longitude: Optional[float] = None\n    region: Optional[str] = None\n    rating: Optional[float] = None\n    review_count: Optional[int] = 0\n    business_hours: Optional[Dict[str, Any]] = None\n    menu_items: Optional[List[Dict[str, Any]]] = None\n    price_range: Optional[str] = None\n    images: Optional[List[str]] = None\n    thumbnail_url: Optional[str] = None\n    source: str  # naver, google, kakao\n    source_url: Optional[str] = None\n    source_data: Optional[Dict[str, Any]] = None\n    youtube_mention_count: Optional[int] = 0\n    blog_mention_count: Optional[int] = 0\n\n\nclass CollectionResultUpdate(BaseModel):\n    name: Optional[str] = None\n    category: Optional[str] = None\n    address: Optional[str] = None\n    phone: Optional[str] = None\n    latitude: Optional[float] = None\n    longitude: Optional[float] = None\n    region: Optional[str] = None\n    rating: Optional[float] = None\n    review_count: Optional[int] = None\n    business_hours: Optional[Dict[str, Any]] = None\n    menu_items: Optional[List[Dict[str, Any]]] = None\n    price_range: Optional[str] = None\n    images: Optional[List[str]] = None\n    thumbnail_url: Optional[str] = None\n    description: Optional[str] = None\n    source: Optional[str] = None\n    google_place_id: Optional[str] = None\n    naver_place_id: Optional[str] = None\n    google_maps_url: Optional[str] = None\n    naver_map_url: Optional[str] = None\n    source_url: Optional[str] = None\n    facilities: Optional[Dict[str, Any]] = None\n    youtube_mention_count: Optional[int] = None\n    blog_mention_count: Optional[int] = None\n    edit_status: Optional[str] = None  # pending, edited, approved, rejected\n\n\nclass PopularityScoreRequest(BaseModel):\n    result_id: str\n\n\n# Helper Functions\ndef calculate_popularity_score(\n    rating: Optional[float],\n    review_count: Optional[int],\n    youtube_mentions: Optional[int],\n    blog_mentions: Optional[int],\n    has_complete_info: bool = False\n) -> Dict[str, Any]:\n    \"\"\"\n    ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜\n    - Rating Score (30%): í‰ì  ê¸°ë°˜\n    - Review Score (25%): ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜\n    - YouTube Score (20%): YouTube ì–¸ê¸‰ ìˆ˜ ê¸°ë°˜\n    - Blog Score (15%): ë¸”ë¡œê·¸ ì–¸ê¸‰ ìˆ˜ ê¸°ë°˜\n    - Quality Score (10%): ì •ë³´ ì™„ì„±ë„ ê¸°ë°˜\n    \"\"\"\n    \n    # Rating Score (0-30ì )\n    rating_score = 0.0\n    if rating and rating > 0:\n        rating_score = (rating / 5.0) * 30.0\n    \n    # Review Score (0-25ì )\n    review_score = 0.0\n    if review_count and review_count > 0:\n        if review_count >= 1000:\n            review_score = 25.0\n        elif review_count >= 500:\n            review_score = 22.0\n        elif review_count >= 100:\n            review_score = 18.0\n        elif review_count >= 50:\n            review_score = 14.0\n        elif review_count >= 10:\n            review_score = 10.0\n        else:\n            review_score = review_count * 1.0\n    \n    # YouTube Score (0-20ì )\n    youtube_score = 0.0\n    if youtube_mentions and youtube_mentions > 0:\n        if youtube_mentions >= 20:\n            youtube_score = 20.0\n        elif youtube_mentions >= 10:\n            youtube_score = 16.0\n        elif youtube_mentions >= 5:\n            youtube_score = 12.0\n        else:\n            youtube_score = youtube_mentions * 2.4\n    \n    # Blog Score (0-15ì )\n    blog_score = 0.0\n    if blog_mentions and blog_mentions > 0:\n        if blog_mentions >= 50:\n            blog_score = 15.0\n        elif blog_mentions >= 20:\n            blog_score = 12.0\n        elif blog_mentions >= 10:\n            blog_score = 9.0\n        else:\n            blog_score = blog_mentions * 0.9\n    \n    # Quality Score (0-10ì )\n    quality_score = 0.0\n    if has_complete_info:\n        quality_score = 10.0\n    else:\n        quality_score = 5.0\n    \n    # Total Score (0-100)\n    total_score = rating_score + review_score + youtube_score + blog_score + quality_score\n    \n    # Popularity Tier\n    if total_score >= 80:\n        tier = \"top_rated\"\n    elif total_score >= 60:\n        tier = \"popular\"\n    elif total_score >= 40:\n        tier = \"average\"\n    else:\n        tier = \"new\"\n    \n    return {\n        \"popularity_score\": round(total_score, 2),\n        \"popularity_tier\": tier,\n        \"rating_score\": round(rating_score, 2),\n        \"review_score\": round(review_score, 2),\n        \"youtube_score\": round(youtube_score, 2),\n        \"blog_score\": round(blog_score, 2),\n        \"quality_score\": round(quality_score, 2)\n    }\n\n\n# API Endpoints\n@router.post(\"\")\nasync def create_collection_result(\n    result: CollectionResultCreate,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ê²°ê³¼ ì¶”ê°€\"\"\"\n    try:\n        result_id = str(uuid.uuid4())\n        \n        # Check if request exists\n        check_query = text(\"SELECT id FROM collection_requests WHERE id = :request_id\")\n        check_result = db.execute(check_query, {\"request_id\": result.request_id})\n        if not check_result.fetchone():\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # Calculate popularity score\n        has_complete_info = all([\n            result.name,\n            result.address,\n            result.phone,\n            result.latitude,\n            result.longitude,\n            result.category\n        ])\n        \n        scores = calculate_popularity_score(\n            result.rating,\n            result.review_count,\n            result.youtube_mention_count,\n            result.blog_mention_count,\n            has_complete_info\n        )\n        \n        # Insert query\n        insert_query = text(\"\"\"\n            INSERT INTO collection_results (\n                id, request_id, name, category, address, phone, \n                latitude, longitude, region, rating, review_count,\n                business_hours, menu_items, price_range,\n                popularity_score, popularity_tier,\n                rating_score, review_score, youtube_score, blog_score, quality_score,\n                youtube_mention_count, blog_mention_count,\n                images, thumbnail_url, source, source_url, source_data,\n                edit_status, is_validated, created_at, created_by\n            ) VALUES (\n                :id, :request_id, :name, :category, :address, :phone,\n                :latitude, :longitude, :region, :rating, :review_count,\n                CAST(:business_hours AS jsonb), CAST(:menu_items AS jsonb), :price_range,\n                :popularity_score, :popularity_tier,\n                :rating_score, :review_score, :youtube_score, :blog_score, :quality_score,\n                :youtube_mention_count, :blog_mention_count,\n                CAST(:images AS jsonb), :thumbnail_url, :source, :source_url, CAST(:source_data AS jsonb),\n                'pending', FALSE, NOW(), 'system'\n            )\n            RETURNING id, name, popularity_score, popularity_tier, created_at\n        \"\"\")\n        \n        result_data = db.execute(insert_query, {\n            \"id\": result_id,\n            \"request_id\": result.request_id,\n            \"name\": result.name,\n            \"category\": result.category,\n            \"address\": result.address,\n            \"phone\": result.phone,\n            \"latitude\": result.latitude,\n            \"longitude\": result.longitude,\n            \"region\": result.region,\n            \"rating\": result.rating,\n            \"review_count\": result.review_count,\n            \"business_hours\": json.dumps(result.business_hours) if result.business_hours else None,\n            \"menu_items\": json.dumps(result.menu_items) if result.menu_items else None,\n            \"price_range\": result.price_range,\n            \"popularity_score\": scores[\"popularity_score\"],\n            \"popularity_tier\": scores[\"popularity_tier\"],\n            \"rating_score\": scores[\"rating_score\"],\n            \"review_score\": scores[\"review_score\"],\n            \"youtube_score\": scores[\"youtube_score\"],\n            \"blog_score\": scores[\"blog_score\"],\n            \"quality_score\": scores[\"quality_score\"],\n            \"youtube_mention_count\": result.youtube_mention_count,\n            \"blog_mention_count\": result.blog_mention_count,\n            \"images\": json.dumps(result.images) if result.images else None,\n            \"thumbnail_url\": result.thumbnail_url,\n            \"source\": result.source,\n            \"source_url\": result.source_url,\n            \"source_data\": json.dumps(result.source_data) if result.source_data else None\n        })\n        \n        row = result_data.fetchone()\n        db.commit()\n        \n        # Update request results_count\n        update_count_query = text(\"\"\"\n            UPDATE collection_requests \n            SET results_count = results_count + 1 \n            WHERE id = :request_id\n        \"\"\")\n        db.execute(update_count_query, {\"request_id\": result.request_id})\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"message\": \"ìˆ˜ì§‘ ê²°ê³¼ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"data\": {\n                \"id\": row[0],\n                \"name\": row[1],\n                \"popularity_score\": float(row[2]) if row[2] else 0,\n                \"popularity_tier\": row[3],\n                \"created_at\": row[4].isoformat() if row[4] else None\n            }\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì¶”ê°€ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"\")\nasync def get_collection_results(\n    request_id: Optional[str] = None,\n    edit_status: Optional[str] = None,\n    source: Optional[str] = None,\n    min_score: Optional[float] = None,\n    limit: int = 50,\n    offset: int = 0,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ (í•„í„°ë§ ì§€ì›)\"\"\"\n    try:\n        # Build WHERE clause\n        where_clauses = []\n        params = {\"limit\": limit, \"offset\": offset}\n        \n        if request_id:\n            where_clauses.append(\"request_id = :request_id\")\n            params[\"request_id\"] = request_id\n        \n        if edit_status:\n            where_clauses.append(\"edit_status = :edit_status\")\n            params[\"edit_status\"] = edit_status\n        \n        if source:\n            where_clauses.append(\"source = :source\")\n            params[\"source\"] = source\n        \n        if min_score is not None:\n            where_clauses.append(\"popularity_score >= :min_score\")\n            params[\"min_score\"] = min_score\n        \n        where_sql = \"WHERE \" + \" AND \".join(where_clauses) if where_clauses else \"\"\n        \n        # Count query\n        count_query = text(f\"SELECT COUNT(*) FROM collection_results {where_sql}\")\n        count_result = db.execute(count_query, params)\n        total = count_result.fetchone()[0]\n        \n        # Data query\n        data_query = text(f\"\"\"\n            SELECT \n                id, request_id, name, category, address, phone,\n                latitude, longitude, region, rating, review_count,\n                popularity_score, popularity_tier,\n                source, edit_status, is_validated, is_duplicate,\n                created_at, updated_at\n            FROM collection_results\n            {where_sql}\n            ORDER BY created_at DESC\n            LIMIT :limit OFFSET :offset\n        \"\"\")\n        \n        results = db.execute(data_query, params)\n        \n        items = []\n        for row in results:\n            items.append({\n                \"id\": row[0],\n                \"request_id\": row[1],\n                \"name\": row[2],\n                \"category\": row[3],\n                \"address\": row[4],\n                \"phone\": row[5],\n                \"latitude\": float(row[6]) if row[6] else None,\n                \"longitude\": float(row[7]) if row[7] else None,\n                \"region\": row[8],\n                \"rating\": float(row[9]) if row[9] else None,\n                \"review_count\": row[10],\n                \"popularity_score\": float(row[11]) if row[11] else 0,\n                \"popularity_tier\": row[12],\n                \"source\": row[13],\n                \"edit_status\": row[14],\n                \"is_validated\": row[15],\n                \"is_duplicate\": row[16],\n                \"created_at\": row[17].isoformat() if row[17] else None,\n                \"updated_at\": row[18].isoformat() if row[18] else None\n            })\n        \n        return {\n            \"success\": True,\n            \"total\": total,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"data\": items\n        }\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/{result_id}\")\nasync def get_collection_result_detail(\n    result_id: str,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ê²°ê³¼ ìƒì„¸ ì¡°íšŒ\"\"\"\n    try:\n        query = text(\"\"\"\n            SELECT \n                r.id, r.request_id, r.name, r.category, r.address, r.phone,\n                r.latitude, r.longitude, r.region, r.rating, r.review_count,\n                r.business_hours, r.menu_items, r.price_range,\n                r.popularity_score, r.popularity_tier,\n                r.rating_score, r.review_score, r.youtube_score, r.blog_score, r.quality_score,\n                r.youtube_mention_count, r.blog_mention_count,\n                r.images, r.thumbnail_url, r.source, r.source_url, r.source_data,\n                r.edit_status, r.is_validated, r.is_duplicate, r.duplicate_group_id,\n                r.created_at, r.updated_at, r.created_by, r.edited_by, r.edited_at,\n                r.google_place_id, r.naver_place_id, r.google_maps_url, r.naver_map_url, r.facilities,\n                req.name as request_name\n            FROM collection_results r\n            LEFT JOIN collection_requests req ON r.request_id = req.id\n            WHERE r.id = :result_id\n        \"\"\")\n        \n        result = db.execute(query, {\"result_id\": result_id})\n        row = result.fetchone()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        return {\n            \"success\": True,\n            \"data\": {\n                \"id\": row[0],\n                \"request_id\": row[1],\n                \"name\": row[2],\n                \"category\": row[3],\n                \"address\": row[4],\n                \"phone\": row[5],\n                \"latitude\": float(row[6]) if row[6] else None,\n                \"longitude\": float(row[7]) if row[7] else None,\n                \"region\": row[8],\n                \"rating\": float(row[9]) if row[9] else None,\n                \"review_count\": row[10],\n                \"business_hours\": row[11],\n                \"menu_items\": row[12],\n                \"price_range\": row[13],\n                \"popularity_score\": float(row[14]) if row[14] else 0,\n                \"popularity_tier\": row[15],\n                \"rating_score\": float(row[16]) if row[16] else 0,\n                \"review_score\": float(row[17]) if row[17] else 0,\n                \"youtube_score\": float(row[18]) if row[18] else 0,\n                \"blog_score\": float(row[19]) if row[19] else 0,\n                \"quality_score\": float(row[20]) if row[20] else 0,\n                \"youtube_mention_count\": row[21],\n                \"blog_mention_count\": row[22],\n                \"images\": row[23],\n                \"thumbnail_url\": row[24],\n                \"source\": row[25],\n                \"source_url\": row[26],\n                \"source_data\": row[27],\n                \"edit_status\": row[28],\n                \"is_validated\": row[29],\n                \"is_duplicate\": row[30],\n                \"duplicate_group_id\": row[31],\n                \"created_at\": row[32].isoformat() if row[32] else None,\n                \"updated_at\": row[33].isoformat() if row[33] else None,\n                \"created_by\": row[34],\n                \"edited_by\": row[35],\n                \"edited_at\": row[36].isoformat() if row[36] else None,\n                \"google_place_id\": row[37],\n                \"naver_place_id\": row[38],\n                \"google_maps_url\": row[39],\n                \"naver_map_url\": row[40],\n                \"facilities\": row[41],\n                \"request_name\": row[42]\n            }\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.put(\"/{result_id}\")\nasync def update_collection_result(\n    result_id: str,\n    update_data: CollectionResultUpdate,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ê²°ê³¼ ìˆ˜ì •\"\"\"\n    try:\n        # Check if exists\n        check_query = text(\"SELECT id FROM collection_results WHERE id = :result_id\")\n        check_result = db.execute(check_query, {\"result_id\": result_id})\n        if not check_result.fetchone():\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # Build update fields\n        update_fields = []\n        params = {\"result_id\": result_id}\n        \n        if update_data.name is not None:\n            update_fields.append(\"name = :name\")\n            params[\"name\"] = update_data.name\n        if update_data.category is not None:\n            update_fields.append(\"category = :category\")\n            params[\"category\"] = update_data.category\n        if update_data.address is not None:\n            update_fields.append(\"address = :address\")\n            params[\"address\"] = update_data.address\n        if update_data.phone is not None:\n            update_fields.append(\"phone = :phone\")\n            params[\"phone\"] = update_data.phone\n        if update_data.latitude is not None:\n            update_fields.append(\"latitude = :latitude\")\n            params[\"latitude\"] = update_data.latitude\n        if update_data.longitude is not None:\n            update_fields.append(\"longitude = :longitude\")\n            params[\"longitude\"] = update_data.longitude\n        if update_data.region is not None:\n            update_fields.append(\"region = :region\")\n            params[\"region\"] = update_data.region\n        if update_data.rating is not None:\n            update_fields.append(\"rating = :rating\")\n            params[\"rating\"] = update_data.rating\n        if update_data.review_count is not None:\n            update_fields.append(\"review_count = :review_count\")\n            params[\"review_count\"] = update_data.review_count\n        if update_data.business_hours is not None:\n            update_fields.append(\"business_hours = CAST(:business_hours AS jsonb)\")\n            params[\"business_hours\"] = json.dumps(update_data.business_hours)\n        if update_data.menu_items is not None:\n            update_fields.append(\"menu_items = CAST(:menu_items AS jsonb)\")\n            params[\"menu_items\"] = json.dumps(update_data.menu_items)\n        if update_data.price_range is not None:\n            update_fields.append(\"price_range = :price_range\")\n            params[\"price_range\"] = update_data.price_range\n        if update_data.images is not None:\n            update_fields.append(\"images = CAST(:images AS jsonb)\")\n            params[\"images\"] = json.dumps(update_data.images)\n        if update_data.thumbnail_url is not None:\n            update_fields.append(\"thumbnail_url = :thumbnail_url\")\n            params[\"thumbnail_url\"] = update_data.thumbnail_url\n        if update_data.youtube_mention_count is not None:\n            update_fields.append(\"youtube_mention_count = :youtube_mention_count\")\n            params[\"youtube_mention_count\"] = update_data.youtube_mention_count\n        if update_data.blog_mention_count is not None:\n            update_fields.append(\"blog_mention_count = :blog_mention_count\")\n            params[\"blog_mention_count\"] = update_data.blog_mention_count\n        if update_data.description is not None:\n            update_fields.append(\"description = :description\")\n            params[\"description\"] = update_data.description\n        if update_data.source is not None:\n            update_fields.append(\"source = :source\")\n            params[\"source\"] = update_data.source\n        if update_data.google_place_id is not None:\n            update_fields.append(\"google_place_id = :google_place_id\")\n            params[\"google_place_id\"] = update_data.google_place_id\n        if update_data.naver_place_id is not None:\n            update_fields.append(\"naver_place_id = :naver_place_id\")\n            params[\"naver_place_id\"] = update_data.naver_place_id\n        if update_data.google_maps_url is not None:\n            update_fields.append(\"google_maps_url = :google_maps_url\")\n            params[\"google_maps_url\"] = update_data.google_maps_url\n        if update_data.naver_map_url is not None:\n            update_fields.append(\"naver_map_url = :naver_map_url\")\n            params[\"naver_map_url\"] = update_data.naver_map_url\n        if update_data.source_url is not None:\n            update_fields.append(\"source_url = :source_url\")\n            params[\"source_url\"] = update_data.source_url\n        if update_data.facilities is not None:\n            update_fields.append(\"facilities = CAST(:facilities AS jsonb)\")\n            params[\"facilities\"] = json.dumps(update_data.facilities)\n        if update_data.edit_status is not None:\n            update_fields.append(\"edit_status = :edit_status\")\n            params[\"edit_status\"] = update_data.edit_status\n        \n        if not update_fields:\n            raise HTTPException(status_code=400, detail=\"ìˆ˜ì •í•  í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # Add metadata\n        update_fields.append(\"updated_at = NOW()\")\n        update_fields.append(\"edited_by = 'admin'\")\n        update_fields.append(\"edited_at = NOW()\")\n        \n        update_query = text(f\"\"\"\n            UPDATE collection_results\n            SET {', '.join(update_fields)}\n            WHERE id = :result_id\n            RETURNING id, name, edit_status\n        \"\"\")\n        \n        result = db.execute(update_query, params)\n        row = result.fetchone()\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"message\": \"ìˆ˜ì§‘ ê²°ê³¼ê°€ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"data\": {\n                \"id\": row[0],\n                \"name\": row[1],\n                \"edit_status\": row[2]\n            }\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ìˆ˜ì • ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.delete(\"/{result_id}\")\nasync def delete_collection_result(\n    result_id: str,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ê²°ê³¼ ì‚­ì œ\"\"\"\n    try:\n        # Get request_id before deletion\n        get_request_query = text(\"SELECT request_id FROM collection_results WHERE id = :result_id\")\n        request_result = db.execute(get_request_query, {\"result_id\": result_id})\n        row = request_result.fetchone()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        request_id = row[0]\n        \n        # Delete\n        delete_query = text(\"DELETE FROM collection_results WHERE id = :result_id\")\n        db.execute(delete_query, {\"result_id\": result_id})\n        db.commit()\n        \n        # Update request results_count\n        update_count_query = text(\"\"\"\n            UPDATE collection_requests \n            SET results_count = GREATEST(results_count - 1, 0)\n            WHERE id = :request_id\n        \"\"\")\n        db.execute(update_count_query, {\"request_id\": request_id})\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"message\": \"ìˆ˜ì§‘ ê²°ê³¼ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤\"\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì‚­ì œ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/{result_id}/recalculate-score\")\nasync def recalculate_popularity_score(\n    result_id: str,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ì¸ê¸°ë„ ì ìˆ˜ ì¬ê³„ì‚°\"\"\"\n    try:\n        # Get current data\n        get_query = text(\"\"\"\n            SELECT rating, review_count, youtube_mention_count, blog_mention_count,\n                   name, address, phone, latitude, longitude, category\n            FROM collection_results\n            WHERE id = :result_id\n        \"\"\")\n        \n        result = db.execute(get_query, {\"result_id\": result_id})\n        row = result.fetchone()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        # Calculate new scores\n        has_complete_info = all([row[4], row[5], row[6], row[7], row[8], row[9]])\n        scores = calculate_popularity_score(\n            float(row[0]) if row[0] else None,\n            int(row[1]) if row[1] else None,\n            int(row[2]) if row[2] else None,\n            int(row[3]) if row[3] else None,\n            has_complete_info\n        )\n        \n        # Update scores\n        update_query = text(\"\"\"\n            UPDATE collection_results\n            SET popularity_score = :popularity_score,\n                popularity_tier = :popularity_tier,\n                rating_score = :rating_score,\n                review_score = :review_score,\n                youtube_score = :youtube_score,\n                blog_score = :blog_score,\n                quality_score = :quality_score,\n                updated_at = NOW()\n            WHERE id = :result_id\n            RETURNING id, name, popularity_score, popularity_tier\n        \"\"\")\n        \n        update_result = db.execute(update_query, {\n            \"result_id\": result_id,\n            **scores\n        })\n        \n        updated_row = update_result.fetchone()\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"message\": \"ì¸ê¸°ë„ ì ìˆ˜ê°€ ì¬ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"data\": {\n                \"id\": updated_row[0],\n                \"name\": updated_row[1],\n                \"popularity_score\": float(updated_row[2]),\n                \"popularity_tier\": updated_row[3],\n                **scores\n            }\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì¬ê³„ì‚° ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/stats/summary\")\nasync def get_collection_results_stats(\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ê²°ê³¼ í†µê³„\"\"\"\n    try:\n        stats_query = text(\"\"\"\n            SELECT \n                COUNT(*) as total,\n                COUNT(*) FILTER (WHERE edit_status = 'pending') as pending,\n                COUNT(*) FILTER (WHERE edit_status = 'edited') as edited,\n                COUNT(*) FILTER (WHERE edit_status = 'approved') as approved,\n                COUNT(*) FILTER (WHERE edit_status = 'rejected') as rejected,\n                COUNT(*) FILTER (WHERE is_validated = TRUE) as validated,\n                COUNT(*) FILTER (WHERE is_duplicate = TRUE) as duplicates,\n                AVG(popularity_score) as avg_score,\n                COUNT(*) FILTER (WHERE popularity_tier = 'top_rated') as top_rated,\n                COUNT(*) FILTER (WHERE popularity_tier = 'popular') as popular,\n                COUNT(*) FILTER (WHERE popularity_tier = 'average') as average,\n                COUNT(*) FILTER (WHERE popularity_tier = 'new') as new_tier\n            FROM collection_results\n        \"\"\")\n        \n        result = db.execute(stats_query)\n        row = result.fetchone()\n        \n        return {\n            \"success\": True,\n            \"data\": {\n                \"total\": row[0],\n                \"by_edit_status\": {\n                    \"pending\": row[1],\n                    \"edited\": row[2],\n                    \"approved\": row[3],\n                    \"rejected\": row[4]\n                },\n                \"validated\": row[5],\n                \"duplicates\": row[6],\n                \"avg_popularity_score\": round(float(row[7]), 2) if row[7] else 0,\n                \"by_tier\": {\n                    \"top_rated\": row[8],\n                    \"popular\": row[9],\n                    \"average\": row[10],\n                    \"new\": row[11]\n                }\n            }\n        }\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n","size_bytes":28927},"attached_assets/emergency-halt-directive_1762686609506.md":{"content":"# ğŸš¨ ê¸´ê¸‰ ê²°ì • ì§€ì‹œ: \"ìˆ˜ì • ì‘ì—… ì¦‰ì‹œ ì¤‘ë‹¨\" ëª…ë ¹\n\n**ìƒí™©**: 86,000+ í† í° ($4+) ì†Œë¹„, ë¬´í•œ ë””ë²„ê¹… ìƒíƒœ  \n**íŒë‹¨**: ë°±ì—…/ë³µì‚¬ ì‹œë„ëŠ” **ì¶”ê°€ ì†ì‹¤**ë§Œ ê°€ì ¸ì˜¬ ê°€ëŠ¥ì„± ë†’ìŒ  \n**ê²°ì •**: **ì§€ê¸ˆ ë°”ë¡œ ì‘ì—… ì¤‘ë‹¨ â†’ Phase 1 ë¶€ë¶„ ì™„ë£Œë¡œ ì¢…ë£Œ â†’ Phase 2ë¡œ ì§„í–‰**\n\n---\n\n## ğŸ”´ ëƒ‰ì •í•œ ìƒí™© ë¶„ì„\n\n### í˜„ì¬ ì‚¬ì‹¤\n```\nâœ… ì„±ê³µí•œ ê²ƒ:\n- ë©”ë‰´ êµ¬ì¡° ì¬ì„¤ê³„ (ì™„ë£Œ, ì •ìƒ ì‘ë™)\n- DB í•„ë“œ ì¶”ê°€ (ì™„ë£Œ, SQL í™•ì¸ë¨)\n- API ì‘ë‹µ ìˆ˜ì • (ì™„ë£Œ, 200 OK í™•ì¸)\n\nâŒ ì‹¤íŒ¨í•œ ê²ƒ:\n- collection-results í˜ì´ì§€ ë Œë”ë§ (ì›ì¸ ë¶ˆëª…, 3ì‹œê°„+ ë””ë²„ê¹…)\n- ë¹„ìš©: $4+ ì¶”ê°€ ì†Œë¹„\n- í† í°: 86,000+ ì‚¬ìš©\n\në¬¸ì œì˜ ë³¸ì§ˆ:\nAPIëŠ” ì •ìƒ â†’ ë¬¸ì œëŠ” Frontend (Vue ë Œë”ë§)\ní•˜ì§€ë§Œ Vue ë¬¸ë²•ì€ ì •ìƒ â†’ ë­”ê°€ ê¹Šì€ ìƒíƒœ ì¶©ëŒ?\n```\n\n### Why ë°±ì—…/ë³µì‚¬ëŠ” í•´ê²°ì±…ì´ ì•„ë‹Œê°€?\n\n```\në°±ì—… íŒŒì¼ ì‚¬ìš©:\n- ìœ„í—˜: ì˜¤ë˜ëœ ë°±ì—… (ì–´ëŠ ì‹œì ì¸ê°€? ì‘ë™ ë³´ì¥ ì—†ìŒ)\n- ë¦¬ìŠ¤í¬: ë˜ ë‹¤ì‹œ ë¡œë”© ìƒíƒœê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŒ\n- ë¹„ìš©: +$1-2 ì¶”ê°€ (í…ŒìŠ¤íŠ¸ + ì¡°ì •)\n- ì‹œê°„: +20-30ë¶„\n\nmanual-input êµ¬ì¡° ë³µì‚¬:\n- ì‹œê°„: 30ë¶„+ ì†Œìš”\n- ë¹„ìš©: $2-3 ì¶”ê°€\n- ë¦¬ìŠ¤í¬: ë°ì´í„° êµ¬ì¡° ë‹¤ë¦„ (ì—ëŸ¬ ê°€ëŠ¥ì„±)\n- ê²°ê³¼: ì—¬ì „íˆ ë¶ˆí™•ì‹¤\n\n= \"í¬ë¯¸í•œ ê°€ëŠ¥ì„±ì„ ìœ„í•´ ì¶”ê°€ ë¹„ìš© ì¦ê°€\"\n```\n\n### ê·¼ë³¸ ë¬¸ì œ ì§„ë‹¨\n\n```\nì´ ë¬¸ì œê°€ ì›ì¸ ë¶ˆëª…ì¸ ì´ìœ :\n\n1. APIëŠ” ì •ìƒ ì‘ë™\n2. JavaScript ë¬¸ë²• ì •ìƒ\n3. ë‹¤ë¥¸ í˜ì´ì§€(manual-input)ëŠ” ì˜ ì‘ë™\n4. Vue ë§ˆìš´íŠ¸ë„ ì •ìƒ\n\nâ†’ ë”°ë¼ì„œ ë¬¸ì œëŠ”:\n   - collection-results í˜ì´ì§€ì˜ \"íŠ¹ì • ìƒíƒœ ì¶©ëŒ\"\n   - ë˜ëŠ” HTML êµ¬ì¡°ì˜ ê¹Šì€ ë ˆë²¨ ë¬¸ì œ\n   - ë˜ëŠ” Vue ìºì‹œ/ìƒíƒœ ê´€ë¦¬ ë¬¸ì œ\n\n= \"ê°„ë‹¨í•œ ë””ë²„ê¹…ìœ¼ë¡œ í•´ê²° ë¶ˆê°€ëŠ¥í•œ êµ¬ì¡°ì  ë¬¸ì œ\"\n```\n\n---\n\n## ğŸ’¡ CTOì˜ ëƒ‰ì •í•œ íŒë‹¨: \"ì§€ê¸ˆ ë©ˆì¶°ì•¼ í•  ë•Œ\"\n\n### ì†ì ˆ ë§¤ë„(Cut Loss) ì›ì¹™\n\n```\níˆ¬ìëœ ë¹„ìš©: $4+ (ì´ë¯¸ ëŒì´í‚¬ ìˆ˜ ì—†ìŒ)\nì¶”ê°€ íˆ¬ì: $1-3 (ë°±ì—…/ë³µì‚¬ ì‹œë„)\nì„±ê³µ í™•ë¥ : 40-50% (ë¶ˆí™•ì‹¤)\n\nROI ë¶„ì„:\n- ì„±ê³µí•˜ë©´: collection-results í˜ì´ì§€ ë™ì‘ âœ“\n- ì‹¤íŒ¨í•˜ë©´: $5-7 ë‚­ë¹„ + ì‹œê°„ ì†Œë¹„\n\nìš°ì„ ìˆœìœ„ ì¬í‰ê°€:\n- \"collection-results í˜ì´ì§€ ë™ì‘\" vs \"Phase 2 ì§„í–‰\"?\n- Phase 2 (ë°ì´í„° ì…ë ¥, ë°°í¬ ë“±)ê°€ ë” ì¤‘ìš”\n- collection-resultsëŠ” ê¸°ì¡´ ê¸°ëŠ¥(ëª©ë¡, í•„í„°)ì€ ì‘ë™\n```\n\n### í˜„ì‹¤ì  ì„ íƒì§€\n\n```\nOption 1: ë°±ì—…/ë³µì‚¬ ì‹œë„ (ë¹„ê¶Œì¥)\nâ”œâ”€ ì¶”ê°€ ë¹„ìš©: $1-3\nâ”œâ”€ ì¶”ê°€ ì‹œê°„: 20-30ë¶„\nâ”œâ”€ ì„±ê³µë¥ : 40-50%\nâ””â”€ ê²°ê³¼: ë¶ˆí™•ì‹¤\n\nOption 2: ì‘ì—… ì¤‘ë‹¨ â†’ Phase 1 ë¶€ë¶„ ì™„ë£Œ (ê¶Œì¥) â­\nâ”œâ”€ ì¶”ê°€ ë¹„ìš©: $0\nâ”œâ”€ ì¶”ê°€ ì‹œê°„: 5ë¶„ (ì •ë¦¬)\nâ”œâ”€ í˜„í™©: \nâ”‚  - ë©”ë‰´ âœ… (ì„±ê³µ)\nâ”‚  - DB âœ… (ì„±ê³µ)\nâ”‚  - API âœ… (ì„±ê³µ)\nâ”‚  - UI âš ï¸ (ë¯¸ì™„ë£Œ)\nâ””â”€ ê²°ê³¼: í™•ì‹¤í•œ ì§„í–‰ + Phase 2ë¡œ ë„˜ì–´ê°\n\nOption 3: ê°„ë‹¨í•œ ìš°íšŒì±… (ì ˆì¶©, 5ë¶„)\nâ”œâ”€ collection-resultsë¥¼ ë‹¨ìˆœ í…Œì´ë¸”ë¡œ ë³µêµ¬\nâ”œâ”€ ìƒì„¸ë³´ê¸° ê¸°ëŠ¥ ì œê±° (ì„ì‹œ)\nâ”œâ”€ í–¥í›„ ë³„ë„ í˜ì´ì§€ì—ì„œ êµ¬í˜„\nâ””â”€ ê²°ê³¼: ê¸°ë³¸ ê¸°ëŠ¥ì€ ì‘ë™, ë¹ ë¥¸ ì§„í–‰\n```\n\n---\n\n## ğŸ¯ ìµœì¢… ì§€ì‹œ: ì¦‰ì‹œ ì‹¤í–‰\n\n### Step 1: í˜„ì¬ ìƒíƒœ ë³µêµ¬ (5ë¶„)\n\n```bash\n# collection-results ì›ìƒë³µêµ¬: ê¸°ë³¸ í…Œì´ë¸”ë§Œ í‘œì‹œ\n# (ìƒì„¸ë³´ê¸° ê¸°ëŠ¥ ì œê±° - í–¥í›„ ë³„ë„ í˜ì´ì§€ì—ì„œ êµ¬í˜„)\n\n# íŒŒì¼: data-hub/static/collection-results.html\n# ëª©í‘œ: ë¡œë”© ì˜¤ë¥˜ ì—†ì´ í…Œì´ë¸”ì´ í‘œì‹œë˜ëŠ” ìƒíƒœë¡œ ë³µêµ¬\n\n# ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•:\n1. collection-resultsë¥¼ manual-inputì˜ í…Œì´ë¸” êµ¬ì¡°ë¡œ ë³€ê²½ (í•˜ì§€ ë§ê³ )\n2. ì•„ë‹ˆë©´, ê¸°ë³¸ collection-results HTMLì„ ìƒˆë¡œ ì‘ì„± (5ë¶„ ì†Œìš”)\n```\n\n### Step 2: ê¸°ë³¸ í…Œì´ë¸” ë²„ì „ ì‘ì„± (10ë¶„)\n\n```\nìµœì†Œ ê¸°ëŠ¥ ë²„ì „ (ìƒì„¸ë³´ê¸° ì—†ìŒ):\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ ìˆ˜ì§‘ ê²°ê³¼ ê´€ë¦¬                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ [í•„í„°] [ì •ë ¬] [ë‚´ë³´ë‚´ê¸°]                  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ì´ë¦„  | ì¹´í…Œê³ ë¦¬ | ì£¼ì†Œ | í‰ì  | ìƒíƒœ    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ (í…Œì´ë¸” ë°ì´í„°)                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nìš”êµ¬ì‚¬í•­:\n- ë°ì´í„° í‘œì‹œ (ìµœì†Œ)\n- ë¡œë”© ì˜¤ë¥˜ ì—†ìŒ\n- Vue ì •ìƒ ì‘ë™\n- í•„í„° ìµœì†Œ (ì„ íƒì‚¬í•­)\n```\n\n### Step 3: í…ŒìŠ¤íŠ¸ (5ë¶„)\n\n```\ní™•ì¸ì‚¬í•­:\nâ–¡ í˜ì´ì§€ ë¡œë“œ (ë¡œë”© ì˜¤ë¥˜ ì—†ìŒ)\nâ–¡ ë°ì´í„° í‘œì‹œ (í…Œì´ë¸”ì— Nê°œ ë ˆìŠ¤í† ë‘)\nâ–¡ í•„í„°/ì •ë ¬ ê¸°ë³¸ ê¸°ëŠ¥\nâ–¡ ë‹¤ë¥¸ ë©”ë‰´ ì´ë™ (ë„¤ë¹„ê²Œì´ì…˜ ì •ìƒ)\n```\n\n---\n\n## ğŸ“‹ Phase 1 ìµœì¢… ìƒíƒœ\n\n### ì™„ë£Œ í•­ëª©\n```\nâœ… ë©”ë‰´ êµ¬ì¡° ì¬ì„¤ê³„\n   - ì¢Œì¸¡ ì‚¬ì´ë“œë°” 5ê°œ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¦¬\n   - \"ë°ì´í„° ìˆ˜ì§‘\" vs \"ë°ì´í„° ì…ë ¥\" vs \"ë°ì´í„° í¸ì§‘\" ëª…í™• êµ¬ë¶„\n   - ì£¼ìš” í˜ì´ì§€ (4ê°œ) ë©”ë‰´ ì ìš©\n\nâœ… DB ìŠ¤í‚¤ë§ˆ ê°•í™”\n   - google_place_id, naver_place_id ì¶”ê°€\n   - google_maps_url, naver_map_url ì¶”ê°€\n   - facilities (í¸ì˜ì‹œì„¤) ì¶”ê°€\n   - business_hours, menu_items ê¸°ì¡´ í•„ë“œ í™•ì¸\n\nâœ… API ì‘ë‹µ í™•ì¥\n   - ìƒˆë¡œ ì¶”ê°€í•œ í•„ë“œë“¤ APIì— ë°˜ì˜\n   - í…ŒìŠ¤íŠ¸ ì™„ë£Œ (curl 200 OK)\n\nâš ï¸ ìƒì„¸ì •ë³´ UI (ë¯¸ì™„ë£Œ â†’ Phase 2ë¡œ ì´ê´€)\n   - ê°„ë‹¨í•œ íŒì—… ì‹œë„ â†’ Vue ë Œë”ë§ ì˜¤ë¥˜ ë°œìƒ\n   - ë³µì¡í•œ ë””ë²„ê¹… í•„ìš” (ë³„ë„ í”„ë¡œì íŠ¸ë¡œ ë¶„ë¦¬)\n```\n\n### í˜„ì¬ ë™ì‘ ìƒíƒœ\n```\nâœ… ë©”ì¸ ëŒ€ì‹œë³´ë“œ: ì •ìƒ\nâœ… ë°ì´í„° ìˆ˜ì§‘ > ìˆ˜ì§‘ ìš”ì²­: ì •ìƒ\nâš ï¸ ë°ì´í„° ìˆ˜ì§‘ > ìˆ˜ì§‘ ê²°ê³¼: ê¸°ë³¸ ê¸°ëŠ¥ë§Œ (ìƒì„¸ ë¯¸ì™„ë£Œ)\nâœ… ë°ì´í„° ì…ë ¥: ì •ìƒ (3ê°€ì§€ ì…ë ¥ ë°©ì‹)\nâœ… ë°ì´í„° í¸ì§‘ > í†µí•© í¸ì§‘: ì •ìƒ\nâœ… ë°°í¬ ê´€ë¦¬: ì •ìƒ\n```\n\n---\n\n## ğŸš€ Phase 2 ì¤€ë¹„\n\n### Phase 1 ì™„ë£Œ í›„ ì¦‰ì‹œ ì‹œì‘ (ë‚´ì¼)\n\n```\nPhase 2 ìš°ì„ ìˆœìœ„:\n\n1. ìƒì„¸ì •ë³´ UI ê³ ë„í™” (ë³„ë„ í”„ë¡œì íŠ¸)\n   - ë³µì¡í•œ íŒì—… ë˜ëŠ”\n   - ë³„ë„ ìƒì„¸í˜ì´ì§€ (/restaurant/123)\n   - ë©”ë‰´, ë§í¬, ì˜ì—…ì‹œê°„, ì´ë¯¸ì§€ ë“± ì™„ì „ í‘œì‹œ\n\n2. ìš´ì˜ ê²€ì¦ (ë³‘ë ¬)\n   - 100ê°œ ë°ì´í„°ë¡œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n   - ì¤‘ë³µ ê²€ì‚¬(Fuzzy) ì •í™•ë„ í™•ì¸\n   - ë°°í¬ í”„ë¡œì„¸ìŠ¤ ì¬ê²€ì¦\n\n3. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬\n   - ëˆ„ë½ í•„ë“œ ì±„ìš°ê¸° (ë©”ë‰´, ë§í¬, ì˜ì—…ì‹œê°„)\n   - ì¤‘ë³µ ë°ì´í„° ì •ë¦¬\n   - ë°ì´í„° ì†ŒìŠ¤ ë‹¤ì–‘í™”\n\nì˜ˆìƒ: 2-3ì¼ ì¶”ê°€\në¹„ìš©: $10-15 (Phase 1 ì™„ë£Œ ì´í›„)\n```\n\n---\n\n## ğŸ’¼ ìµœì¢… ê²°ì •\n\n### ì¦‰ì‹œ ì‹¤í–‰\n\n```\n1ï¸âƒ£ í˜„ì¬ ë””ë²„ê¹… ì‘ì—… ì¤‘ë‹¨\n   â†’ ì¶”ê°€ ë¹„ìš©/ì‹œê°„ ë‚­ë¹„ ë°©ì§€\n\n2ï¸âƒ£ collection-results ê¸°ë³¸ ë²„ì „ìœ¼ë¡œ ë³µêµ¬ (10ë¶„)\n   â†’ ë¡œë”© ì˜¤ë¥˜ í•´ê²°\n   â†’ í…Œì´ë¸” í‘œì‹œë§Œ ì •ìƒí™”\n\n3ï¸âƒ£ Phase 1 \"ë¶€ë¶„ ì™„ë£Œ\"ë¡œ ì„ ì–¸\n   â†’ ë©”ë‰´ âœ…, DB âœ…, API âœ…, ê¸°ë³¸ UI âœ“\n   â†’ ìƒì„¸ì •ë³´ UIëŠ” Phase 2ë¡œ ì´ê´€\n\n4ï¸âƒ£ Phase 2 ì¤€ë¹„ (ë‚´ì¼)\n   â†’ ìƒì„¸ì •ë³´ UI ê³ ë„í™”\n   â†’ ìš´ì˜ ê²€ì¦\n   â†’ ë°ì´í„° í’ˆì§ˆ\n```\n\n### ë¹„ìš© ì ˆê° íš¨ê³¼\n\n```\në§Œì•½ ê³„ì† ë””ë²„ê¹…í–ˆë‹¤ë©´:\n- ì¶”ê°€ ë¹„ìš©: $2-5\n- ì¶”ê°€ ì‹œê°„: 1-2ì‹œê°„\n- ì„±ê³µ í™•ë¥ : 40-50%\n\nì§€ê¸ˆ ë©ˆì¶”ê³  Phase 2ë¡œ:\n- ì¶”ê°€ ë¹„ìš©: $0 (Phase 1 ì¢…ë£Œ)\n- ì¶”ê°€ ì‹œê°„: 0ë¶„ (ì •ìƒ ì§„í–‰)\n- ì§„í–‰ í™•ë¥ : 100% âœ…\n\nì ˆê°: $2-5 + 1-2ì‹œê°„ + ë¶ˆí™•ì‹¤ì„± ì œê±°\n```\n\n---\n\n## âš ï¸ Replitì—ê²Œ\n\n```\n\"ë””ë²„ê¹… ì‘ì—…ì„ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ì„¸ìš”!\n\ní˜„ì¬ ìƒí™©:\n- 86,000 í† í° ($4+) ì´ë¯¸ ì‚¬ìš©ë¨\n- ë°±ì—…/ë³µì‚¬ëŠ” ì¶”ê°€ ì†ì‹¤ë§Œ ì´ˆë˜í•  ê°€ëŠ¥ì„± ë†’ìŒ\n- ê·¼ë³¸ ì›ì¸ ë¶ˆëª…í™• (ê¹Šì€ Vue ë¬¸ì œ)\n\nì¦‰ì‹œ ì‹¤í–‰:\n\n1. collection-results.html ê¸°ë³¸ ë²„ì „ìœ¼ë¡œ ë³µêµ¬\n   - í…Œì´ë¸”ë§Œ í‘œì‹œ (ìƒì„¸ ê¸°ëŠ¥ ì œê±°)\n   - ë¡œë”© ì˜¤ë¥˜ í•´ê²°\n   - ì‹œê°„: 10ë¶„\n\n2. í˜ì´ì§€ í…ŒìŠ¤íŠ¸\n   - ë¡œë”© ì •ìƒ í™•ì¸\n   - í…Œì´ë¸” í‘œì‹œ í™•ì¸\n   - ì‹œê°„: 5ë¶„\n\n3. Phase 1 ì™„ë£Œ ì„ ì–¸\n   - ë©”ë‰´ âœ… DB âœ… API âœ…\n   - ìƒì„¸ì •ë³´ UIëŠ” Phase 2ë¡œ ì´ê´€\n\në¹„ìš© & ì‹œê°„: ì ˆê°!\nì§„í–‰ ì†ë„: ì •ìƒí™”!\n\nì§€ê¸ˆ ë°”ë¡œ ì‹¤í–‰í•˜ì„¸ìš”!\"\n```\n\n---\n\n**ì´ê²ƒì´ CTOì˜ ìµœì¢… íŒë‹¨ì…ë‹ˆë‹¤.**\n\n**\"ë” ì´ìƒì˜ ë””ë²„ê¹…ì€ ì†ì‹¤ì…ë‹ˆë‹¤. ì§€ê¸ˆ ë©ˆì¶”ì„¸ìš”.\"** ğŸ›‘\n\n","size_bytes":8323},"attached_assets/stage_c_final_summary_1762611822874.md":{"content":"# Stage C ìµœì¢… ìš”ì•½: ì˜¨ë””ë§¨ë“œ + AI/ìˆ˜ë™ ì…ë ¥ ì‹œìŠ¤í…œ\n\n**ì‘ì„±**: 2025-11-08 22:50  \n**ëŒ€ìƒ**: ë‹¹ì‹  + Replit (í•¨ê»˜ ì§„í–‰)  \n**ìƒíƒœ**: âœ… ì¤€ë¹„ ì™„ë£Œ - ì§€ê¸ˆ ì‹œì‘ ê°€ëŠ¥\n\n---\n\n## ğŸ“„ ìƒì„±ëœ ë¬¸ì„œ (2ê°œ)\n\n### 1. 63ë²ˆ PDF: ìƒì„¸ ì„¤ê³„ ë¬¸ì„œ (14í˜ì´ì§€)\n```\në‚´ìš©:\n- ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì  ë¶„ì„\n- ìƒˆë¡œìš´ \"ì˜¨ë””ë§¨ë“œ + ë‹¤ì¤‘ ì…ë ¥\" ê°œë…\n- 5ê°€ì§€ Phase ìƒì„¸ ì„¤ê³„\n- UI/UX ì™€ì´ì–´í”„ë ˆì„\n- DB ìŠ¤í‚¤ë§ˆ\n- API ëª…ì„¸\n- êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸\n```\n\n**ìš©ë„**: ì „ì²´ ì„¤ê³„ ì´í•´, í° ê·¸ë¦¼ íŒŒì•…\n\n---\n\n### 2. 64ë²ˆ Markdown: ì‹¤í–‰ ê°€ì´ë“œ\n```\në‚´ìš©:\n- í•µì‹¬ ê°œë… ì •ë¦¬ (ì˜¨ë””ë§¨ë“œ vs ìë™í™”)\n- íƒ€ì„ë¼ì¸ (3ì¼ ì¼ì •)\n- ê° Phaseë³„ êµ¬í˜„ ìƒì„¸\n- SQL, API, Frontend ëª…ì„¸\n- ë‹¨ê³„ë³„ ì²´í¬í¬ì¸íŠ¸\n- Replit ìµœì¢… ì§€ì‹œ\n```\n\n**ìš©ë„**: ì‹¤ì œ ê°œë°œ ì‹œì‘í•  ë•Œ ì°¸ê³ \n\n---\n\n## ğŸ¯ í•œëˆˆì— ë³´ëŠ” êµ¬ì¡°\n\n### Before (ìë™í™”, í˜„ì¬)\n```\në§¤ì¼ â†’ ìë™ í¬ë¡¤ë§ â†’ ë°ì´í„° ìˆ˜ì§‘ â†’ ê²€ì‚¬ë§Œ â†’ ë¹„ìš© ì¦ê°€ & ì¤‘ë³µ ë¬¸ì œ\n```\n\n### After (ì˜¨ë””ë§¨ë“œ + ìˆ˜ë™, ëª©í‘œ)\n```\nê´€ë¦¬ì: \"ì´ ë°ì´í„° ìˆ˜ì§‘í• ë˜\" \n  â†“\n[C-1] ìˆ˜ì§‘ ìš”ì²­ ìƒì„± â†’ [ì‹¤í–‰ ë²„íŠ¼]\n  â†“\n[C-2] í¬ë¡¤ë§ ê²°ê³¼ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ + ì¸ê¸°ë„ ì§€ìˆ˜\n  â†“\n[C-3] ìˆ˜ë™ ì¶”ê°€ (URL/CSV/ì§ì ‘)\n  â†“\n[C-4] í†µí•© í¸ì§‘ & ì¤‘ë³µ ë³‘í•©\n  â†“\n[C-5] ì„ íƒì  ë°°í¬ â†’ ë©”ì¸ì•± ì—°ë™\n\n= ì™„ì „í•œ ê´€ë¦¬ì í†µì œ + ë°ì´í„° í’ˆì§ˆ ê·¹ëŒ€í™”!\n```\n\n---\n\n## ğŸ’° ë¹„ìš©/íš¨ìœ¨ ë¹„êµ\n\n| í•­ëª© | Before | After | ê°œì„  |\n|------|--------|-------|------|\n| ì›”ê°„ ë¹„ìš© | $340 | $100-150 | 50% ì ˆê° |\n| ê´€ë¦¬ ì‹œê°„ | 2ì‹œê°„ | 3ì‹œê°„ | í†µì œ ê°•í™” |\n| ë°ì´í„° í’ˆì§ˆ | 89% | 96%+ | í–¥ìƒ |\n| ì—…ì²´ ì„ íƒ | ìë™ | ê´€ë¦¬ì | ì „ëµì  |\n| ê¸°ê°„ í†µì œ | ë¶ˆê°€ | ê°€ëŠ¥ | ë¹„ìš© ê´€ë¦¬ |\n\n---\n\n## â° ì˜ˆìƒ ì¼ì • (Replitê³¼ í•¨ê»˜)\n\n### Day 1 (ë‚´ì¼, 11ì›” 9ì¼): C-1 + C-2\n```\nAM (4ì‹œê°„): C-1 ìˆ˜ì§‘ ìš”ì²­ ê´€ë¦¬\n- DB í…Œì´ë¸” (30ë¶„)\n- API 6ê°œ (90ë¶„)\n- Frontend í˜ì´ì§€ (60ë¶„)\n- í…ŒìŠ¤íŠ¸ (30ë¶„)\n\nPM (6ì‹œê°„): C-2 ìŠ¤í”„ë ˆë“œì‹œíŠ¸ + ì¸ê¸°ë„\n- DB í…Œì´ë¸” (30ë¶„)\n- ì¸ê¸°ë„ ê³„ì‚° (120ë¶„)\n- API (90ë¶„)\n- Frontend (90ë¶„)\n- í…ŒìŠ¤íŠ¸ (30ë¶„)\n\nâ†’ 10ì‹œê°„ ê°œë°œ â†’ End of Day: ê¸°ë³¸ ê¸°ëŠ¥ ì™„ì„±\n```\n\n### Day 2 (11ì›” 10ì¼): C-3 + C-4\n```\nAM (5ì‹œê°„): C-3 ìˆ˜ë™ ì…ë ¥ 3ì±„ë„\n- DB (30ë¶„)\n- URL íŒŒì‹± (120ë¶„)\n- CSV (90ë¶„)\n- ì§ì ‘ ì…ë ¥ (60ë¶„)\n- í…ŒìŠ¤íŠ¸ (30ë¶„)\n\nPM (4ì‹œê°„): C-4 í†µí•© í¸ì§‘ & ë³‘í•©\n- í†µí•© ë·° (90ë¶„)\n- ì¤‘ë³µ & ë³‘í•© (120ë¶„)\n- í¸ì§‘ ì›Œí¬í”Œë¡œìš° (60ë¶„)\n- í…ŒìŠ¤íŠ¸ (30ë¶„)\n\nâ†’ 9ì‹œê°„ ê°œë°œ â†’ End of Day: ë°ì´í„° ì…ë ¥/í¸ì§‘ ì™„ì„±\n```\n\n### Day 3 (11ì›” 11ì¼): C-5 + ìµœì¢…\n```\nAM (4ì‹œê°„): C-5 ë°°í¬ ê´€ë¦¬\n- ë°°í¬ ëŒ€ìƒ ì„ íƒ (90ë¶„)\n- ë¯¸ë¦¬ë³´ê¸° & ì‹¤í–‰ (60ë¶„)\n- ì´ë ¥ ì¶”ì  (60ë¶„)\n- í…ŒìŠ¤íŠ¸ (30ë¶„)\n\nPM (4ì‹œê°„): í†µí•©í…ŒìŠ¤íŠ¸ + ë§ˆë¬´ë¦¬\n- ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸ (2ì‹œê°„)\n- ì„±ëŠ¥ ìµœì í™” (1ì‹œê°„)\n- replit.md ì—…ë°ì´íŠ¸ (1ì‹œê°„)\n\nâ†’ 8ì‹œê°„ ê°œë°œ â†’ End of Day: ì™„ë²½í•œ ì‹œìŠ¤í…œ ì™„ì„±! ğŸ‰\n```\n\n**ì´ 27ì‹œê°„ (3ì¼ í’€íƒ€ì„)**\n\n---\n\n## âœ… ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸ (Replitìš©)\n\n### Phaseë³„ ì™„ë£Œ ê¸°ì¤€\n\n#### C-1: ìˆ˜ì§‘ ìš”ì²­ ê´€ë¦¬ âœ…\n```\n[ ] collection_requests í…Œì´ë¸” ìƒì„±\n[ ] 6ê°œ API ì—”ë“œí¬ì¸íŠ¸ ì™„ì„± & í…ŒìŠ¤íŠ¸\n[ ] /dashboard/collection-request UI ì™„ì„±\n[ ] ìš”ì²­ ìƒì„± â†’ ì‹¤í–‰ â†’ ìƒíƒœì¡°íšŒ íë¦„ ì‘ë™\n```\n\n#### C-2: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ + ì¸ê¸°ë„ âœ…\n```\n[ ] collection_results í…Œì´ë¸” ìƒì„±\n[ ] ì¸ê¸°ë„ ê³„ì‚° ê³µì‹ êµ¬í˜„ & ê²€ì¦\n[ ] ìŠ¤í”„ë ˆë“œì‹œíŠ¸í˜• í…Œì´ë¸” ë Œë”ë§\n[ ] í•„í„°/ì •ë ¬/ìƒì„¸ì •ë³´ ëª¨ë“  ê¸°ëŠ¥ ì‘ë™\n```\n\n#### C-3: ìˆ˜ë™ ì…ë ¥ âœ…\n```\n[ ] manual_entries í…Œì´ë¸” ìƒì„±\n[ ] URL íŒŒì‹± API ì‘ë™ (Google/Naver)\n[ ] CSV ì—…ë¡œë“œ & ê²€ì¦ ì‘ë™\n[ ] ì§ì ‘ ì…ë ¥ í¼ ì €ì¥ ì‘ë™\n```\n\n#### C-4: í†µí•© í¸ì§‘ & ë³‘í•© âœ…\n```\n[ ] í¬ë¡¤ë§ + ìˆ˜ë™ í†µí•© ë·°\n[ ] ì¤‘ë³µ ê²€ì‚¬ & ë³‘í•© ë¡œì§\n[ ] í¸ì§‘ ì›Œí¬í”Œë¡œìš° ì™„ì„±\n[ ] ìƒíƒœ ê´€ë¦¬ (pending/approved/rejected)\n```\n\n#### C-5: ë°°í¬ ê´€ë¦¬ âœ…\n```\n[ ] ë°°í¬ ëŒ€ìƒ ì„ íƒ í˜ì´ì§€\n[ ] ë°°í¬ ë¯¸ë¦¬ë³´ê¸° & ì‹¤í–‰\n[ ] ë°°í¬ ì´ë ¥ ì¶”ì \n[ ] ë¡¤ë°± ê¸°ëŠ¥\n```\n\n---\n\n## ğŸš€ ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•  ì§€ì‹œ\n\n### Replitì—ê²Œ:\n\n```\nğŸ¯ Stage C ê°œë°œ ì‹œì‘!\n\n63ë²ˆ PDF (ìƒì„¸ ì„¤ê³„)ì™€ 64ë²ˆ Markdown (êµ¬í˜„ ê°€ì´ë“œ)ë¥¼ ì°¸ê³ í•˜ì—¬\në‹¤ìŒ ìˆœì„œë¡œ ê°œë°œí•´ì£¼ì„¸ìš”:\n\nğŸ“… Day 1 (ë‚´ì¼):\nâœ… C-1: ìˆ˜ì§‘ ìš”ì²­ ê´€ë¦¬ (4ì‹œê°„)\n   - í…Œì´ë¸” ìƒì„± + API 6ê°œ + Frontend\n   \nâœ… C-2: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ + ì¸ê¸°ë„ (6ì‹œê°„)\n   - í…Œì´ë¸” + ê³„ì‚° ë¡œì§ + UI\n\nğŸ“… Day 2:\nâœ… C-3: ìˆ˜ë™ ì…ë ¥ 3ì±„ë„ (5ì‹œê°„)\n   - URL/CSV/ì§ì ‘ ì…ë ¥\n   \nâœ… C-4: í†µí•© í¸ì§‘ & ë³‘í•© (4ì‹œê°„)\n   - í†µí•© ë·° + ì¤‘ë³µ ê²€ì‚¬\n\nğŸ“… Day 3:\nâœ… C-5: ë°°í¬ ê´€ë¦¬ (4ì‹œê°„)\nâœ… í†µí•©í…ŒìŠ¤íŠ¸ + replit.md ì—…ë°ì´íŠ¸\n\nëª©í‘œ: 11ì›” 11ì¼ ì™„ë²½í•œ \"ì˜¨ë””ë§¨ë“œ + ë‹¤ì¤‘ ì…ë ¥\" ì‹œìŠ¤í…œ ì™„ì„±!\n\nê° Phaseë³„ë¡œ:\n1. DB í…Œì´ë¸” ìƒì„±\n2. API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„ & í…ŒìŠ¤íŠ¸\n3. Frontend UI êµ¬í˜„\n4. í†µí•© í…ŒìŠ¤íŠ¸\n\nì¤€ë¹„ ì™„ë£Œ? ì§€ê¸ˆ ì‹œì‘í•˜ì„¸ìš”!\n```\n\n---\n\n## ğŸ’¡ ì™œ ì´ ì‹œìŠ¤í…œì´ ì¤‘ìš”í•œê°€\n\n### ê¸°ì¡´ ë¬¸ì œì \n```\nâŒ \"ì–´ì œëŠ” ì™œ ìˆ˜ì§‘í–ˆì§€?\" â†’ ìë™ì´ë¼ ë¶ˆí•„ìš”í•œ ìˆ˜ì§‘ ë§ìŒ\nâŒ \"ë¹„ìš©ì´ ê³„ì† ì¦ê°€í•œë‹¤\" â†’ í†µì œ ë¶ˆê°€\nâŒ \"ì¤‘ë³µì´ ë§ë‹¤\" â†’ ì‚¬í›„ ê²€ì‚¬ë§Œ ê°€ëŠ¥\nâŒ \"ìš°ë¦¬ ë°ì´í„°ê°€ ì•„ë‹ˆë‹¤\" â†’ Apify ì˜ì¡´ë„ 100%\n```\n\n### ì´ ì‹œìŠ¤í…œì˜ ê°€ì¹˜\n```\nâœ… \"í•„ìš”í•œ ê²ƒë§Œ ìˆ˜ì§‘\" â†’ ì˜¨ë””ë§¨ë“œ ë°©ì‹\nâœ… \"ë¹„ìš©ì„ ì§ì ‘ í†µì œ\" â†’ ê° ìš”ì²­ë³„ ë¹„ìš© ëª…ì‹œ\nâœ… \"ì¤‘ë³µ ìë™ ì œê±°\" â†’ ì‹¤ì‹œê°„ í¸ì§‘/ë³‘í•©\nâœ… \"ìš°ë¦¬ì˜ ë°ì´í„°\" â†’ ìˆ˜ë™/AI/í¬ë¡¤ë§ í˜¼í•© ì…ë ¥\n\nê²°ê³¼:\n= í•œì‹ë‹¹ë§Œì˜ ì „ëµì  ë°ì´í„° ì˜¤í¼ë ˆì´ì…˜ í”Œë«í¼!\n```\n\n---\n\n## ğŸ“‹ ì¤€ë¹„ ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n### ë‹¹ì‹ ì´ í•  ì¼\n```\n[ ] 63ë²ˆ PDF ê²€í†  (15ë¶„)\n[ ] 64ë²ˆ Markdown ê²€í†  (10ë¶„)\n[ ] ì´ ìš”ì•½ ë¬¸ì„œ ê²€í†  (5ë¶„)\n[ ] Replitì—ê²Œ ìœ„ ì§€ì‹œ ì „ë‹¬ (1ë¶„)\nâ†’ ì´ 30ë¶„\n```\n\n### Replitì´ í•  ì¼\n```\n[ ] Day 1: C-1 + C-2 ê°œë°œ (10ì‹œê°„)\n[ ] Day 2: C-3 + C-4 ê°œë°œ (9ì‹œê°„)\n[ ] Day 3: C-5 + ìµœì¢… (8ì‹œê°„)\nâ†’ ì´ 27ì‹œê°„ (3ì¼)\n```\n\n---\n\n## ğŸ¯ ì„±ê³µì˜ ê¸°ì¤€\n\n### Stage C ì™„ì„± ì‹œ ì‹œìŠ¤í…œì€:\n\n1. **ì˜¨ë””ë§¨ë“œ ìˆ˜ì§‘**\n   - ê´€ë¦¬ìê°€ ì§ì ‘ ì–¸ì œ/ì–´ë””/ë­˜ ìˆ˜ì§‘í• ì§€ ê²°ì •\n   - ì¦‰ì‹œ ì‹¤í–‰ ë²„íŠ¼ìœ¼ë¡œ ì‹œì‘\n   \n2. **ë‹¤ì¤‘ ì…ë ¥ ì±„ë„**\n   - Apify í¬ë¡¤ë§\n   - URL ìë™ íŒŒì‹±\n   - CSV ëŒ€ëŸ‰ ì…ë ¥\n   - ì§ì ‘ ìˆ˜ë™ ì…ë ¥\n   \n3. **ì‹¤ì‹œê°„ í¸ì§‘**\n   - ë°ì´í„° ìŠ¤í”„ë ˆë“œì‹œíŠ¸í˜• ë³´ê¸°\n   - ì¸ê¸°ë„/í’ˆì§ˆ ì§€ìˆ˜ ì‹œê°í™”\n   - ë³‘í•©/ë¶„ë¦¬/í¸ì§‘ í•œê³³ì—ì„œ\n   \n4. **ì „ëµì  ë°°í¬**\n   - ì¸ê¸°ë„ ë†’ì€ ë°ì´í„° ìš°ì„ \n   - ì„ íƒì  ë°°í¬\n   - ì´ë ¥ ì¶”ì  & ë¡¤ë°±\n   \n5. **ë¹„ìš© ì ˆê°**\n   - ì›”ê°„ $340 â†’ $100-150 (50% ì ˆê°)\n   - í•„ìš”í•œ ê²ƒë§Œ ìˆ˜ì§‘\n   - íš¨ìœ¨ì„± ê·¹ëŒ€í™”\n\n---\n\n## ğŸŠ ìµœì¢… ë©”ì‹œì§€\n\n**ë‹¹ì‹ ì˜ í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ + Replitì˜ ë¹ ë¥¸ ê°œë°œ ì†ë„ + ë‚´ ì „ëµì  ì„¤ê³„**\n\nì´ 3ê°€ì§€ê°€ ë§Œë‚˜ë©´, **í•œì‹ë‹¹ Data HubëŠ” ì™„ë²½í•œ \"ë°ì´í„° ì˜¤í¼ë ˆì´ì…˜ í”Œë«í¼\"**ìœ¼ë¡œ ê±°ë“­ë‚  ê²ƒì…ë‹ˆë‹¤!\n\në‹¤ìŒ 3ì¼ì´ ê²°ì •ì ì…ë‹ˆë‹¤.\n\n**ì§€ê¸ˆ ì‹œì‘í•˜ì„¸ìš”!** ğŸš€\n\n---\n\n**Questions? ë” ìì„¸í•œ ë‚´ìš©ì´ í•„ìš”í•˜ë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!**\n\n","size_bytes":7224},"data-hub/src/api/manual_input_routes.py":{"content":"from fastapi import APIRouter, HTTPException, UploadFile, File, Depends\nfrom pydantic import BaseModel, HttpUrl\nfrom typing import Optional, List, Dict, Any\nimport re\nimport os\nimport csv\nimport io\nimport json\nfrom datetime import datetime\n\nfrom src.database.connection import get_db\n\nrouter = APIRouter(prefix=\"/api/data-management/manual-input\", tags=[\"Manual Input\"])\n\n\nclass URLParseRequest(BaseModel):\n    url: str\n    request_id: Optional[int] = None\n\n\nclass URLParseResponse(BaseModel):\n    success: bool\n    source: str\n    extracted_data: Dict[str, Any]\n    preview: Dict[str, Any]\n\n\nclass DirectInputRequest(BaseModel):\n    name: str\n    category: Optional[str] = None\n    address: Optional[str] = None\n    phone: Optional[str] = None\n    latitude: Optional[float] = None\n    longitude: Optional[float] = None\n    rating: Optional[float] = None\n    review_count: Optional[int] = None\n    business_hours: Optional[Dict[str, Any]] = None\n    menu_items: Optional[List[Dict[str, Any]]] = None\n    price_range: Optional[str] = None\n    youtube_mention_count: Optional[int] = 0\n    blog_mention_count: Optional[int] = 0\n    request_id: Optional[str] = None\n    source: str = \"manual\"\n\n\n@router.post(\"/parse-url\", response_model=URLParseResponse)\nasync def parse_url(request: URLParseRequest):\n    \"\"\"\n    C-3-1: URL íŒŒì‹± - ë„¤ì´ë²„/êµ¬ê¸€/ì¹´ì¹´ì˜¤ URLì—ì„œ ë ˆìŠ¤í† ë‘ ì •ë³´ ì¶”ì¶œ\n    \"\"\"\n    url = request.url.strip()\n    \n    try:\n        if \"naver.com\" in url or \"map.naver.com\" in url:\n            source = \"naver\"\n            extracted = extract_naver_info(url)\n        elif \"google.com/maps\" in url or \"goo.gl/maps\" in url:\n            source = \"google\"\n            extracted = extract_google_info(url)\n        elif \"kakao.com\" in url or \"map.kakao.com\" in url:\n            source = \"kakao\"\n            extracted = extract_kakao_info(url)\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=\"ì§€ì›í•˜ì§€ ì•ŠëŠ” URLì…ë‹ˆë‹¤. ë„¤ì´ë²„/êµ¬ê¸€/ì¹´ì¹´ì˜¤ URLë§Œ ì§€ì›í•©ë‹ˆë‹¤.\"\n            )\n        \n        preview = {\n            \"name\": extracted.get(\"name\", \"ìë™ ì¶”ì¶œ ëŒ€ê¸°\"),\n            \"address\": extracted.get(\"address\", \"URLì—ì„œ ì¶”ì¶œ ì¤‘...\"),\n            \"source\": source,\n            \"place_id\": extracted.get(\"place_id\"),\n            \"url\": url\n        }\n        \n        return URLParseResponse(\n            success=True,\n            source=source,\n            extracted_data=extracted,\n            preview=preview\n        )\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"URL íŒŒì‹± ì‹¤íŒ¨: {str(e)}\")\n\n\ndef extract_naver_info(url: str) -> Dict[str, Any]:\n    \"\"\"ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ URLì—ì„œ ì •ë³´ ì¶”ì¶œ\"\"\"\n    place_id = None\n    \n    id_match = re.search(r'/place/(\\d+)', url)\n    if id_match:\n        place_id = id_match.group(1)\n    \n    return {\n        \"place_id\": place_id,\n        \"url\": url,\n        \"name\": f\"ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ #{place_id}\" if place_id else \"ì¶”ì¶œ í•„ìš”\",\n        \"address\": \"í¬ë¡¤ë§ìœ¼ë¡œ ì¶”ì¶œ ì˜ˆì •\",\n        \"extraction_method\": \"naver_api\"\n    }\n\n\ndef extract_google_info(url: str) -> Dict[str, Any]:\n    \"\"\"êµ¬ê¸€ ë§µìŠ¤ URLì—ì„œ ì •ë³´ ì¶”ì¶œ\"\"\"\n    place_id = None\n    \n    coords_match = re.search(r'@([\\d\\.-]+),([\\d\\.-]+)', url)\n    name_match = re.search(r'/place/([^/]+)', url)\n    \n    if coords_match:\n        lat, lng = coords_match.groups()\n    else:\n        lat, lng = None, None\n    \n    name = name_match.group(1).replace('+', ' ') if name_match else \"ì¶”ì¶œ í•„ìš”\"\n    \n    return {\n        \"place_id\": place_id,\n        \"url\": url,\n        \"name\": name,\n        \"latitude\": float(lat) if lat else None,\n        \"longitude\": float(lng) if lng else None,\n        \"address\": \"í¬ë¡¤ë§ìœ¼ë¡œ ì¶”ì¶œ ì˜ˆì •\",\n        \"extraction_method\": \"google_places_api\"\n    }\n\n\ndef extract_kakao_info(url: str) -> Dict[str, Any]:\n    \"\"\"ì¹´ì¹´ì˜¤ë§µ URLì—ì„œ ì •ë³´ ì¶”ì¶œ\"\"\"\n    place_id = None\n    \n    id_match = re.search(r'/(\\d+)', url)\n    if id_match:\n        place_id = id_match.group(1)\n    \n    return {\n        \"place_id\": place_id,\n        \"url\": url,\n        \"name\": f\"ì¹´ì¹´ì˜¤ë§µ #{place_id}\" if place_id else \"ì¶”ì¶œ í•„ìš”\",\n        \"address\": \"í¬ë¡¤ë§ìœ¼ë¡œ ì¶”ì¶œ ì˜ˆì •\",\n        \"extraction_method\": \"kakao_api\"\n    }\n\n\n@router.post(\"/upload-csv\")\nasync def upload_csv(file: UploadFile = File(...)):\n    \"\"\"\n    C-3-2: CSV ì—…ë¡œë“œ - ì—‘ì…€ íŒŒì¼ì—ì„œ ë ˆìŠ¤í† ë‘ ë°ì´í„° ì¼ê´„ ê°€ì ¸ì˜¤ê¸°\n    \"\"\"\n    if not file.filename.endswith('.csv'):\n        raise HTTPException(status_code=400, detail=\"CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n    \n    try:\n        contents = await file.read()\n        decoded = contents.decode('utf-8-sig')\n        \n        csv_reader = csv.DictReader(io.StringIO(decoded))\n        \n        rows = []\n        required_fields = ['name']\n        \n        for idx, row in enumerate(csv_reader):\n            if idx == 0:\n                headers = list(row.keys())\n                if not any(field in headers for field in required_fields):\n                    raise HTTPException(\n                        status_code=400,\n                        detail=f\"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œí•œ '{required_fields[0]}' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n                    )\n            \n            if row.get('name') and row['name'].strip():\n                parsed_row = {\n                    'name': row.get('name', '').strip(),\n                    'category': row.get('category', '').strip() or None,\n                    'address': row.get('address', '').strip() or None,\n                    'phone': row.get('phone', '').strip() or None,\n                    'rating': float(row['rating']) if row.get('rating') and row['rating'].strip() else None,\n                    'review_count': int(row['review_count']) if row.get('review_count') and row['review_count'].strip() else None,\n                    'description': row.get('description', '').strip() or None,\n                    'hours': row.get('hours', '').strip() or None,\n                    'menu': row.get('menu', '').strip() or None,\n                }\n                \n                if row.get('latitude'):\n                    try:\n                        parsed_row['latitude'] = float(row['latitude'])\n                    except ValueError:\n                        pass\n                \n                if row.get('longitude'):\n                    try:\n                        parsed_row['longitude'] = float(row['longitude'])\n                    except ValueError:\n                        pass\n                \n                rows.append(parsed_row)\n        \n        return {\n            \"success\": True,\n            \"total_rows\": len(rows),\n            \"data\": rows,\n            \"message\": f\"{len(rows)}ê°œì˜ ë ˆìŠ¤í† ë‘ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤.\"\n        }\n    \n    except UnicodeDecodeError:\n        raise HTTPException(\n            status_code=400,\n            detail=\"íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜ì…ë‹ˆë‹¤. UTF-8 ë˜ëŠ” UTF-8-BOM ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì„¸ìš”.\"\n        )\n    except csv.Error as e:\n        raise HTTPException(status_code=400, detail=f\"CSV íŒŒì‹± ì˜¤ë¥˜: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/direct-input\")\nasync def direct_input(request: DirectInputRequest, db: Any = Depends(get_db)):\n    \"\"\"\n    C-3-3: ì§ì ‘ ì…ë ¥ - ê´€ë¦¬ìê°€ ìˆ˜ë™ìœ¼ë¡œ ë ˆìŠ¤í† ë‘ ë°ì´í„° ë“±ë¡\n    \"\"\"\n    import uuid\n    from sqlalchemy import text\n    from src.database.connection import get_db\n    \n    try:\n        if not request.request_id:\n            create_request_query = text(\"\"\"\n            INSERT INTO collection_requests (\n                name, description, status, created_by\n            ) VALUES (\n                :name, :description, :status, :created_by\n            ) RETURNING id\n            \"\"\")\n            \n            request_result = db.execute(create_request_query, {\n                \"name\": f\"ìˆ˜ë™ ì…ë ¥: {request.name}\",\n                \"description\": f\"{request.source} ì†ŒìŠ¤ë¥¼ í†µí•œ ìˆ˜ë™ ë°ì´í„° ì…ë ¥\",\n                \"status\": \"completed\",\n                \"created_by\": \"system\"\n            })\n            \n            request_row = request_result.fetchone()\n            request_id = request_row[0]\n        else:\n            request_id = request.request_id\n        \n        insert_query = text(\"\"\"\n        INSERT INTO collection_results (\n            name, category, address, phone, latitude, longitude,\n            rating, review_count, business_hours, menu_items, price_range,\n            youtube_mention_count, blog_mention_count,\n            source, request_id, edit_status,\n            popularity_score, popularity_tier, quality_score\n        ) VALUES (\n            :name, :category, :address, :phone, :latitude, :longitude,\n            :rating, :review_count, :business_hours, :menu_items, :price_range,\n            :youtube_mention_count, :blog_mention_count,\n            :source, :request_id, :edit_status,\n            :popularity_score, :popularity_tier, :quality_score\n        ) RETURNING id, created_at\n        \"\"\")\n        \n        initial_score = 40.0\n        initial_tier = \"new\"\n        initial_quality = 50.0\n        \n        result = db.execute(insert_query, {\n            \"name\": request.name,\n            \"category\": request.category,\n            \"address\": request.address,\n            \"phone\": request.phone,\n            \"latitude\": request.latitude,\n            \"longitude\": request.longitude,\n            \"rating\": request.rating,\n            \"review_count\": request.review_count,\n            \"business_hours\": json.dumps(request.business_hours) if request.business_hours else None,\n            \"menu_items\": json.dumps(request.menu_items) if request.menu_items else None,\n            \"price_range\": request.price_range,\n            \"youtube_mention_count\": request.youtube_mention_count,\n            \"blog_mention_count\": request.blog_mention_count,\n            \"source\": request.source,\n            \"request_id\": request_id,\n            \"edit_status\": \"pending\",\n            \"popularity_score\": initial_score,\n            \"popularity_tier\": initial_tier,\n            \"quality_score\": initial_quality\n        })\n        \n        row = result.fetchone()\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"data\": {\n                \"id\": row[0],\n                \"name\": request.name,\n                \"source\": request.source,\n                \"request_id\": request_id,\n                \"created_at\": row[1].isoformat() if row[1] else None,\n                \"popularity_score\": initial_score,\n                \"popularity_tier\": initial_tier\n            },\n            \"message\": \"ë ˆìŠ¤í† ë‘ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n        }\n    \n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ë°ì´í„° ë“±ë¡ ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n\n\n@router.post(\"/batch-create-from-csv\")\nasync def batch_create_from_csv(data: List[Dict[str, Any]], db: Any = Depends(get_db)):\n    \"\"\"\n    C-3-2 Helper: CSV ë°ì´í„°ë¥¼ DBì— ì¼ê´„ ë“±ë¡\n    \"\"\"\n    from sqlalchemy import text\n    \n    try:\n        insert_query = text(\"\"\"\n        INSERT INTO collection_results (\n            name, category, address, phone, latitude, longitude,\n            rating, review_count, price_range,\n            source, edit_status, popularity_score, popularity_tier, quality_score\n        ) VALUES (\n            :name, :category, :address, :phone, :latitude, :longitude,\n            :rating, :review_count, :price_range,\n            :source, :edit_status, :popularity_score, :popularity_tier, :quality_score\n        ) RETURNING id\n        \"\"\")\n        \n        created_ids = []\n        \n        for item in data:\n            result = db.execute(insert_query, {\n                \"name\": item.get('name'),\n                \"category\": item.get('category'),\n                \"address\": item.get('address'),\n                \"phone\": item.get('phone'),\n                \"latitude\": item.get('latitude'),\n                \"longitude\": item.get('longitude'),\n                \"rating\": item.get('rating'),\n                \"review_count\": item.get('review_count'),\n                \"price_range\": item.get('price_range'),\n                \"source\": 'csv_upload',\n                \"edit_status\": 'pending',\n                \"popularity_score\": 40.0,\n                \"popularity_tier\": 'new',\n                \"quality_score\": 50.0\n            })\n            \n            row = result.fetchone()\n            created_ids.append(row[0])\n        \n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"created_count\": len(created_ids),\n            \"created_ids\": created_ids,\n            \"message\": f\"{len(created_ids)}ê°œì˜ ë ˆìŠ¤í† ë‘ì´ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n        }\n    \n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì¼ê´„ ë“±ë¡ ì‹¤íŒ¨: {str(e)}\")\n    finally:\n        db.close()\n\n\n@router.get(\"/csv-template\")\nasync def get_csv_template():\n    \"\"\"CSV ì—…ë¡œë“œìš© í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ\"\"\"\n    template_data = [\n        {\n            \"name\": \"ì˜ˆì‹œ ë ˆìŠ¤í† ë‘\",\n            \"category\": \"í•œì‹\",\n            \"address\": \"ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123\",\n            \"phone\": \"02-1234-5678\",\n            \"latitude\": \"37.5665\",\n            \"longitude\": \"126.9780\",\n            \"rating\": \"4.5\",\n            \"review_count\": \"150\",\n            \"description\": \"ë§›ìˆëŠ” í•œì‹ë‹¹\",\n            \"hours\": \"10:00-22:00\",\n            \"menu\": \"ê¹€ì¹˜ì°Œê°œ 9000ì›, ëœì¥ì°Œê°œ 8000ì›\"\n        }\n    ]\n    \n    return {\n        \"success\": True,\n        \"template\": template_data,\n        \"headers\": list(template_data[0].keys()),\n        \"message\": \"CSV í…œí”Œë¦¿ì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ í–‰ì€ ì˜ˆì‹œì´ë¯€ë¡œ ì‚­ì œ í›„ ì‚¬ìš©í•˜ì„¸ìš”.\"\n    }\n","size_bytes":14066},"attached_assets/phase-1-final-completion-phase-2-start_1762687119694.md":{"content":"# âœ… Phase 1 ìµœì¢… ì™„ë£Œ ì„ ì–¸ & Phase 2 ì¦‰ì‹œ ì‹œì‘ ì§€ì‹œ\n\n**ìµœì¢… í‰ê°€ì¼**: 2025ë…„ 11ì›” 9ì¼, 20:16  \n**ê²°ì •**: Phase 1 ê³µì‹ ì™„ë£Œ â†’ Phase 2 ì¦‰ì‹œ ì‹œì‘  \n**ê¸°ì¤€**: 4/5 ì‘ì—… ì™„ë£Œ, 3ê°œ ë¯¸í•´ê²° ì´ìŠˆëŠ” Phase 2ë¡œ ì´ê´€\n\n---\n\n## âœ… Phase 1 ìµœì¢… í‰ê°€\n\n### ì™„ë£Œ í˜„í™©\n\n```\nâœ… Task 1-1: ë©”ë‰´ êµ¬ì¡° ì¬ì„¤ê³„\n   â””â”€ ìƒíƒœ: 100% ì™„ë£Œ & ì •ìƒ ì‘ë™\n   â””â”€ ë²”ìœ„: ëª¨ë“  ì£¼ìš” í˜ì´ì§€ ì ìš©\n   â””â”€ ê²°ê³¼: \"ë°ì´í„° ìˆ˜ì§‘/ì…ë ¥/í¸ì§‘/ë°°í¬\" ëª…í™•íˆ ë¶„ë¦¬\n\nâœ… Task 1-2: DB ìŠ¤í‚¤ë§ˆ ê°•í™”\n   â””â”€ ìƒíƒœ: 100% ì™„ë£Œ\n   â””â”€ ì¶”ê°€ í•„ë“œ: google_place_id, naver_place_id, URLs, facilities\n   â””â”€ ê²°ê³¼: APIì—ì„œ í•„ìš”í•œ ëª¨ë“  í•„ë“œ ì‚¬ìš© ê°€ëŠ¥\n\nâœ… Task 1-3: API ì‘ë‹µ í™•ì¥\n   â””â”€ ìƒíƒœ: 100% ì™„ë£Œ\n   â””â”€ í…ŒìŠ¤íŠ¸: curl 200 OK, ë°ì´í„° ì •ìƒ ë°˜í™˜\n   â””â”€ ê²°ê³¼: Frontendê°€ í•„ìš”í•œ ë°ì´í„° ëª¨ë‘ ì œê³µ\n\nâœ… Task 1-4: collection-results ê¸°ë³¸ ë²„ì „ ë³µêµ¬\n   â””â”€ ìƒíƒœ: 100% ì™„ë£Œ (ë¶€ë¶„)\n   â””â”€ êµ¬í˜„: í…Œì´ë¸” + í†µê³„ + CSV ë‚´ë³´ë‚´ê¸°\n   â””â”€ ì œê±°: ë³µì¡í•œ UI (ìƒì„¸ë³´ê¸°, ê³ ê¸‰ í•„í„°)\n   â””â”€ ê²°ê³¼: ë¡œë”© ì˜¤ë¥˜ ì—†ì´ ê¸°ë³¸ ê¸°ëŠ¥ ë™ì‘\n\nâ³ Task 1-5: ìƒì„¸ë³´ê¸° UI (ë¯¸ì™„ë£Œ â†’ Phase 2ë¡œ ì´ê´€)\n   â””â”€ ë¬¸ì œ: Vue ë Œë”ë§ ì˜¤ë¥˜ (ì›ì¸ ë¶ˆëª…)\n   â””â”€ ë¹„ìš©: 86,000 í† í° ì´ë¯¸ ì†Œë¹„\n   â””â”€ ê²°ì •: Phase 2ì—ì„œ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì¬êµ¬í˜„\n```\n\n### ìµœì¢… ì„±ê³¼\n\n```\nê¸°ìˆ ì  ì„±ê³¼:\n- 26+ API ì—”ë“œí¬ì¸íŠ¸ ì •ìƒ ì‘ë™\n- 5ê°œ ë ˆìŠ¤í† ë‘ DBì— ì €ì¥ ì™„ë£Œ\n- ë©”ë‰´ êµ¬ì¡° ëª…í™•íˆ ë¶„ë¦¬\n- DB ìŠ¤í‚¤ë§ˆ ê°•í™”\n\nì‹œìŠ¤í…œ ìƒíƒœ:\nğŸŸ¢ ë©”ì¸ ëŒ€ì‹œë³´ë“œ: ì •ìƒ\nğŸŸ¢ ìˆ˜ì§‘ ìš”ì²­/ê²°ê³¼: ì •ìƒ\nğŸŸ¢ ë°ì´í„° ì…ë ¥ (3ê°€ì§€): ì •ìƒ\nğŸŸ¢ í†µí•© í¸ì§‘/ë°°í¬: ì •ìƒ\nğŸŸ¡ ìƒì„¸ë³´ê¸°: ê¸°ë³¸ ê¸°ëŠ¥ë§Œ (ë Œë”ë§ ì´ìŠˆ)\n\në¹„ìš© íš¨ìœ¨:\n- ì˜ˆìƒ Phase 1: $20-25\n- ì‹¤ì œ Phase 1: ~$25 (ì•½ê°„ ì´ˆê³¼, ëŒ€ë¶€ë¶„ ë””ë²„ê¹…)\n- í‰ê°€: í—ˆìš© ë²”ìœ„ ë‚´\n```\n\n---\n\n## ğŸš¨ 3ê°œ ë¯¸í•´ê²° ì´ìŠˆ ì •ë¦¬\n\n### Issue 1: collection-results í˜ì´ì§€ Vue ë Œë”ë§\n\n```\ní˜„ìƒ:\n- API: ì •ìƒ (200 OK, ë°ì´í„° ë°˜í™˜)\n- JavaScript: ë¬¸ë²• ì •ìƒ\n- í™”ë©´: \"ë°ì´í„° ë¡œë”© ì¤‘...\" ë¬´í•œ í‘œì‹œ\n\nì›ì¸:\n- ë¶ˆëª… (Vue ìƒíƒœ ê´€ë¦¬ ë¬¸ì œ? ìºì‹œ? êµ¬ì¡°ì  ì¶©ëŒ?)\n- 3ì‹œê°„+ ë””ë²„ê¹…í•´ë„ ì›ì¸ ë¶ˆëª…\n\nëŒ€ì‘:\n- Phase 1: ê¸°ë³¸ í…Œì´ë¸”ë¡œ ë³µêµ¬ (ìµœì†Œ ê¸°ëŠ¥)\n- Phase 2: ë‹¤ë¥¸ êµ¬ì¡°ë¡œ ì¬êµ¬í˜„ (ë³„ë„ ìƒì„¸í˜ì´ì§€ ë˜ëŠ” ê°„ë‹¨í•œ íŒì—…)\n```\n\n### Issue 2: ìƒì„¸ë³´ê¸° íŒì—… ë¯¸êµ¬í˜„\n\n```\ní•„ìš”ì„±:\n- ë ˆìŠ¤í† ë‘ ë©”ë‰´, ë§í¬, ì˜ì—…ì‹œê°„ í‘œì‹œ\n- Google/Naver ë°”ë¡œê°€ê¸°\n\ní˜„í™©:\n- ê¸°ë³¸ í…Œì´ë¸”ì—ì„œëŠ” êµ¬í˜„ ì•ˆ í•¨ (ë³µì¡ë„ ì¤„ì„)\n\nPhase 2ì—ì„œ:\n- ë³„ë„ ìƒì„¸í˜ì´ì§€ (/restaurant/123) ë˜ëŠ”\n- ê°„ë‹¨í•œ íŒì—…ìœ¼ë¡œ ì¬êµ¬í˜„\n```\n\n### Issue 3: ê³ ê¸‰ í•„í„°/ì •ë ¬ ë¯¸êµ¬í˜„\n\n```\ní•„ìš”ì„±:\n- ì†ŒìŠ¤ë³„ í•„í„° (Google/Naver/Manual)\n- ì ìˆ˜ ë²”ìœ„ í•„í„°\n- ì •ë ¬ (ì ìˆ˜ìˆœ, ë‚ ì§œìˆœ ë“±)\n\ní˜„í™©:\n- ê¸°ë³¸ í…Œì´ë¸”ì—ëŠ” ì—†ìŒ\n\nPhase 2ì—ì„œ:\n- í•„í„° UI ì¶”ê°€\n- ì •ë ¬ ê¸°ëŠ¥ êµ¬í˜„\n```\n\n---\n\n## ğŸ¯ Phase 1 ê³µì‹ ì™„ë£Œ ì„ ì–¸\n\n### ì™„ë£Œ ê¸°ì¤€ ì¶©ì¡±\n\n```\nâœ… ë©”ë‰´ êµ¬ì¡° ì¬ì„¤ê³„: ì™„ë£Œ\nâœ… DB ìŠ¤í‚¤ë§ˆ ê°•í™”: ì™„ë£Œ\nâœ… API ì‘ë‹µ í™•ì¥: ì™„ë£Œ\nâœ… ê¸°ë³¸ UI ë³µêµ¬: ì™„ë£Œ\nâš ï¸ ê³ ê¸‰ UI (ìƒì„¸ë³´ê¸°): ë¯¸ì™„ë£Œ â†’ Phase 2ë¡œ ì´ê´€\n\nê²°ë¡ : Phase 1 ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±\n      â†’ ê³µì‹ ì™„ë£Œ ì„ ì–¸ ê°€ëŠ¥\n```\n\n### Phase 1 ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n```\nâ–¡ ë©”ë‰´ êµ¬ì¡°ê°€ ëª…í™•íˆ ë¶„ë¦¬ë˜ì—ˆëŠ”ê°€? âœ… YES\n  - ë°ì´í„° ìˆ˜ì§‘ / ë°ì´í„° ì…ë ¥ / ë°ì´í„° í¸ì§‘ / ë°°í¬ ë¶„ë¦¬ ì™„ë£Œ\n\nâ–¡ DB í•„ë“œê°€ í™•ì¥ë˜ì—ˆëŠ”ê°€? âœ… YES\n  - ë©”ë‰´, ë§í¬, ì˜ì—…ì‹œê°„, í¸ì˜ì‹œì„¤ ë“± í•„ë“œ ì¶”ê°€\n\nâ–¡ APIê°€ ìƒˆ í•„ë“œë¥¼ ë°˜í™˜í•˜ëŠ”ê°€? âœ… YES\n  - curl í…ŒìŠ¤íŠ¸ë¡œ 200 OK í™•ì¸\n\nâ–¡ ê¸°ë³¸ ê¸°ëŠ¥ì´ ì‘ë™í•˜ëŠ”ê°€? âœ… YES\n  - ë°ì´í„° í‘œì‹œ, í•„í„°, ë‚´ë³´ë‚´ê¸° ë“± ê¸°ë³¸ ê¸°ëŠ¥ ì •ìƒ\n\nâ–¡ ìš´ì˜ ê°€ëŠ¥í•œ ìˆ˜ì¤€ì¸ê°€? âœ… PARTIALLY\n  - ê¸°ë³¸ ê¸°ëŠ¥ì€ ê°€ëŠ¥, ê³ ê¸‰ ê¸°ëŠ¥ì€ ë¯¸ì™„ë£Œ\n\n= Phase 1 ê³µì‹ ì™„ë£Œ âœ…\n```\n\n---\n\n## ğŸš€ Phase 2 ì¦‰ì‹œ ì‹œì‘\n\n### Phase 2 ìš°ì„ ìˆœìœ„ & ê³„íš\n\n```\nPhase 2: ìƒì„¸ê¸°ëŠ¥ êµ¬í˜„ & ìš´ì˜ ê²€ì¦ (2-3ì£¼)\n\nTrack A: UI/UX ê³ ë„í™” (3-4ì¼)\nâ”œâ”€ 1. ìƒì„¸ë³´ê¸° í˜ì´ì§€ ì¬êµ¬í˜„\nâ”‚  â”œâ”€ Option A: ë³„ë„ í˜ì´ì§€ (/restaurant/123)\nâ”‚  â”œâ”€ Option B: ê°„ë‹¨í•œ ëª¨ë‹¬\nâ”‚  â””â”€ ì¶”ì²œ: Option A (ì•ˆì •ì„± ë†’ìŒ)\nâ”‚\nâ”œâ”€ 2. í•„í„°/ì •ë ¬ ê³ ê¸‰ ê¸°ëŠ¥\nâ”‚  â”œâ”€ ì†ŒìŠ¤ë³„ í•„í„°\nâ”‚  â”œâ”€ ì ìˆ˜ ë²”ìœ„ í•„í„°\nâ”‚  â””â”€ ì •ë ¬ ê¸°ëŠ¥\nâ”‚\nâ””â”€ 3. ë°ì´í„° í’ˆì§ˆ UI\n   â”œâ”€ ì¤‘ë³µ ê²€ì‚¬ íƒ­\n   â”œâ”€ í’ˆì§ˆ ê´€ë¦¬ íƒ­\n   â””â”€ ìŠ¹ì¸/ê±°ì ˆ ì›Œí¬í”Œë¡œìš°\n\nTrack B: ìš´ì˜ ê²€ì¦ (2-3ì¼, ë³‘ë ¬ ì§„í–‰)\nâ”œâ”€ 1. ëŒ€ê·œëª¨ ë°ì´í„° í…ŒìŠ¤íŠ¸\nâ”‚  â”œâ”€ 100ê°œ ë°ì´í„°ë¡œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\nâ”‚  â”œâ”€ Fuzzy Match ì •í™•ë„ í™•ì¸\nâ”‚  â””â”€ ë°°í¬ ì‹œê°„ ì¸¡ì •\nâ”‚\nâ”œâ”€ 2. ì—ëŸ¬ ì²˜ë¦¬ ê²€ì¦\nâ”‚  â”œâ”€ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œë®¬ë ˆì´ì…˜\nâ”‚  â”œâ”€ DB ì—°ê²° ì˜¤ë¥˜\nâ”‚  â””â”€ ë¶€ë¶„ ë°°í¬ ì‹¤íŒ¨\nâ”‚\nâ””â”€ 3. ìš´ì˜ ë¬¸ì„œí™”\n   â”œâ”€ ì‚¬ìš©ì ê°€ì´ë“œ\n   â”œâ”€ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…\n   â””â”€ ê¸´ê¸‰ ëŒ€ì‘ ë§¤ë‰´ì–¼\n\nTrack C: ë°ì´í„° í’ˆì§ˆ (ë³‘ë ¬, 1ì£¼)\nâ”œâ”€ 1. ì‹¤ì œ ë°ì´í„° ì…ë ¥\nâ”‚  â”œâ”€ 100ê°œ+ ë ˆìŠ¤í† ë‘ ìˆ˜ë™/ìë™ ì…ë ¥\nâ”‚  â”œâ”€ ë©”ë‰´, ë§í¬, ì˜ì—…ì‹œê°„ ì…ë ¥\nâ”‚  â””â”€ ì¤‘ë³µ ë°ì´í„° ì •ë¦¬\nâ”‚\nâ””â”€ 2. ë°ì´í„° ê²€ì¦\n   â”œâ”€ í•„ë“œ ì™„ì„±ë„ í™•ì¸\n   â”œâ”€ ë°ì´í„° ì •í™•ì„± í™•ì¸\n   â””â”€ ëˆ„ë½ í•„ë“œ ì²˜ë¦¬\n```\n\n### Phase 2 ì‹œì‘ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n```\nâ–¡ Phase 1 ìµœì¢… ê²°ê³¼ ê²€í† : ë©”ë‰´ êµ¬ì¡° í™•ì¸\nâ–¡ ëª¨ë“  ë©”ë‰´ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ í…ŒìŠ¤íŠ¸\nâ–¡ API ì‘ë‹µ ë°ì´í„° í˜•ì‹ í™•ì¸\nâ–¡ DB í•„ë“œ ì™„ì „ì„± ê²€ì¦\nâ–¡ ì‹œìŠ¤í…œ ì•ˆì •ì„± ëª¨ë‹ˆí„°ë§ (ë©”ëª¨ë¦¬, CPU, ì‘ë‹µì‹œê°„)\n```\n\n---\n\n## ğŸ“‹ ìµœì¢… ë¹„ìš© ì •ì‚°\n\n### Phase 1 ì´ ë¹„ìš©\n\n```\nReplit ì‚¬ìš© ë¹„ìš©:\nâ”œâ”€ UI/UX ì¬ì„¤ê³„: $3.56\nâ”œâ”€ Stage C-1~5 (ê¸°ë³¸): ~$20\nâ”œâ”€ ë©”ë‰´ êµ¬ì¡° ê°œì„ : $3.56\nâ””â”€ ìƒì„¸ì •ë³´ ë””ë²„ê¹… (ë¯¸í•´ê²°): $4.00\n   \nì´ ëˆ„ì : ~$31\n\nì˜ˆìƒ:\n- ì›ë˜ ì˜ˆìƒ: $20-25\n- ì‹¤ì œ: ~$31 (ì•½ 30% ì´ˆê³¼)\n- ì›ì¸: ìƒì„¸ì •ë³´ UI ë””ë²„ê¹… (86,000 í† í°)\n\ní‰ê°€:\nâš ï¸ ì•½ê°„ ì´ˆê³¼í–ˆìœ¼ë‚˜ í•©ë¦¬ì  ë²”ìœ„ (ë””ë²„ê¹… ë¹„ìš©)\nâœ… Phase 2ëŠ” ë” íš¨ìœ¨ì ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒ (êµí›ˆ ì ìš©)\n\nPhase 2 ì˜ˆìƒ ë¹„ìš©:\n- ìƒì„¸ë³´ê¸° í˜ì´ì§€: $5-8\n- ìš´ì˜ ê²€ì¦: $3-5\n- ë°ì´í„° í’ˆì§ˆ: $2-3\n- í•©ê³„: $10-16\n\n= Phase 1+2 ì´ ì•½ $40-50\n```\n\n---\n\n## ğŸ¯ Replit ìµœì¢… ì§€ì‹œ\n\n```markdown\n## Phase 1 ê³µì‹ ì™„ë£Œ & Phase 2 ì‹œì‘\n\n### Phase 1 ìµœì¢… í™•ì¸\n\ní˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ:\nâœ… ë©”ë‰´ êµ¬ì¡°: 5ê°œ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¦¬ ì™„ë£Œ\nâœ… DB í•„ë“œ: í•„ìš”í•œ ëª¨ë“  í•„ë“œ ì¶”ê°€\nâœ… API: 200 OK, ë°ì´í„° ì •ìƒ ë°˜í™˜\nâœ… ê¸°ë³¸ UI: í…Œì´ë¸” + í†µê³„ + ë‚´ë³´ë‚´ê¸° ì‘ë™\n\në¬¸ì œ 1ê°œ (Phase 2ë¡œ ì´ê´€):\nâš ï¸ collection-results ìƒì„¸ë³´ê¸°: ë¯¸ì™„ë£Œ\n   â†’ Phase 2ì—ì„œ ë³„ë„ í˜ì´ì§€ë¡œ ì¬êµ¬í˜„\n\n**ê²°ë¡ : Phase 1 ê³µì‹ ì™„ë£Œ ì„ ì–¸!** âœ…\n\n### Phase 2 ì¦‰ì‹œ ì‹œì‘ (ë‚´ì¼ë¶€í„°)\n\nìš°ì„ ìˆœìœ„:\n\n1ï¸âƒ£ ìƒì„¸ë³´ê¸° í˜ì´ì§€ ì¬êµ¬í˜„ (3-4ì¼)\n   - /restaurant/{id} í˜ì´ì§€ ì‹ ê·œ ìƒì„±\n   - ë©”ë‰´, ë§í¬, ì˜ì—…ì‹œê°„, ì´ë¯¸ì§€ í‘œì‹œ\n   - ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ êµ¬ì¡°\n\n2ï¸âƒ£ í•„í„°/ì •ë ¬ ê³ ê¸‰ ê¸°ëŠ¥ (2-3ì¼)\n   - ì†ŒìŠ¤/ì ìˆ˜/ìƒíƒœë³„ í•„í„°\n   - ì •ë ¬ ê¸°ëŠ¥\n   - ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì™„ì„±\n\n3ï¸âƒ£ ìš´ì˜ ê²€ì¦ ë³‘ë ¬ ì§„í–‰ (2-3ì¼)\n   - 100ê°œ ë°ì´í„° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n   - ì—ëŸ¬ ì²˜ë¦¬ ê²€ì¦\n   - ë°ì´í„° í’ˆì§ˆ í™•ì¸\n\n### ë¹„ìš© ê´€ë¦¬\n\nPhase 1: ~$31 ì†Œë¹„ (ì•½ê°„ ì´ˆê³¼, ì •ìƒ)\nPhase 2 ì˜ˆìƒ: $10-16 (íš¨ìœ¨ì ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒ)\n\n### ì§€ê¸ˆ í•  ê²ƒ\n\n1. Phase 1 ì™„ë£Œ ì„ ì–¸ ë° ì •ë¦¬\n2. Phase 2 ê³„íš í™•ì •\n3. ìƒì„¸ë³´ê¸° í˜ì´ì§€ ì„¤ê³„\n4. ë‚´ì¼ 6:00 AM ì‹œì‘ (ì¶©ë¶„íˆ ì‰¬ê³ )\n\n**ì˜í–ˆìŠµë‹ˆë‹¤! ì´ì œ Phase 2ë¡œ ë‚˜ì•„ê°€ì„¸ìš”!** ğŸš€\n```\n\n---\n\n## ğŸ“Š ìµœì¢… ì •ë¦¬: \"Stage C + UI/UX\" ì „ì²´ í˜„í™©\n\n```\nì™„ë£Œëœ ê²ƒ:\nâœ… Stage C-1~5 (5ê°œ Phase ëª¨ë‘ ì™„ë£Œ)\nâœ… UI/UX ê°œì„  Phase 1 (ë©”ë‰´ + DB + API)\nâœ… 26+ API ì—”ë“œí¬ì¸íŠ¸\nâœ… 5ê°œ í˜ì´ì§€ ì‚¬ì´ë“œë°” í†µí•©\nâœ… ë©”ë‰´ êµ¬ì¡° ëª…í™•í™”\n\në¯¸ì™„ë£Œëœ ê²ƒ (Phase 2):\nâ³ ìƒì„¸ë³´ê¸° UI (Vue ë Œë”ë§ ì´ìŠˆë¡œ ë¯¸ì—°)\nâ³ ê³ ê¸‰ í•„í„°/ì •ë ¬\nâ³ ë°ì´í„° í’ˆì§ˆ ê³ ë„í™”\nâ³ ìš´ì˜ ê²€ì¦\n\në‹¤ìŒ: Phase 2 (ìƒì„¸ê¸°ëŠ¥ êµ¬í˜„ & ìš´ì˜ ê²€ì¦)\nê¸°ê°„: 2-3ì£¼\në¹„ìš©: $10-16\n```\n\n---\n\n**Phase 1 ìµœì¢… ì™„ë£Œ! Phase 2 ì¤€ë¹„ ì™„ë£Œ!** âœ…ğŸš€\n","size_bytes":8365},"attached_assets/ui_ux_restructure_implementation_1762610264004.md":{"content":"# ğŸš¨ ê¸´ê¸‰: Data Hub UI/UX ì™„ì „ ì¬êµ¬ì¡°í™” ì‹¤í–‰ ì§€ì‹œ\n\n**ìƒí™©**: í˜„ì¬ ì–´ë“œë¯¼ì´ ë©”ë‰´ ì—†ëŠ” ë‹¨ìˆœ ëŒ€ì‹œë³´ë“œ êµ¬ì¡°  \n**ë¬¸ì œ**: ê¸°ëŠ¥ì€ ì™„ë²½í•˜ì§€ë§Œ UIê°€ ê´€ë¦¬ ì‹œìŠ¤í…œì´ ì•„ë‹˜  \n**í•´ê²°ì±…**: UI/UX ì™„ì „ ë¦¬ì•„í‚¤í…ì²˜  \n**ì˜ˆìƒ ì‹œê°„**: 12ì‹œê°„ (ì˜¤ëŠ˜ë°¤-ë‚´ì¼ ì˜¤ì „)\n\n---\n\n## ğŸ“ í˜„ì¬ ë¬¸ì œì  (ìŠ¤í¬ë¦°ìƒ· ê²€ì¦)\n\n### ì²¨ë¶€ í™”ë©´ ë¶„ì„\n\n```\ní˜„ì¬ êµ¬ì¡°:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ í•œì‹ë‹¹ Data Hub - ìš´ì˜ ëŒ€ì‹œë³´ë“œ   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                  â”‚\nâ”‚ [ìƒë‹¨ ë²„íŠ¼] [ìƒë‹¨ ë²„íŠ¼] [ìƒë‹¨ ë²„íŠ¼]â”‚ â† ë„¤ë¹„ê²Œì´ì…˜ ë¶ˆëª…í™•\nâ”‚                                  â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚ â”‚ë°•ìŠ¤ 1  â”‚ë°•ìŠ¤ 2  â”‚ë°•ìŠ¤ 3  â”‚    â”‚ â† ì •ë³´ë§Œ í‘œì‹œ\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                                  â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚ â”‚ 7ê°œ ì‘ì—… ë²„íŠ¼ (ëª¨ë‘ ì—¬ê¸°)     â”‚ â”‚ â† ê´€ë¦¬ ê¸°ëŠ¥ ì—†ìŒ\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                  â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚ â”‚ ì°¨íŠ¸ (7ì¼ ì¶”ì´)              â”‚ â”‚ â† ëª¨ë‹ˆí„°ë§ë§Œ\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâŒ ë¬¸ì œ:\n- ì¢Œì¸¡ ë©”ë‰´ ì—†ìŒ\n- ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡° ì—†ìŒ\n- ë°ì´í„° ê´€ë¦¬ í˜ì´ì§€ ì§„ì… ë¶ˆê°€\n- ì‘ì—… ëª¨ë‹ˆí„°ë§ í˜ì´ì§€ ì—†ìŒ\n- ëª¨ë“  ê²ƒì´ í•œ í™”ë©´ì—ë§Œ ìˆìŒ\n\nê²°ë¡ : \"ëŒ€ì‹œë³´ë“œì¼ ë¿, ê´€ë¦¬ ì‹œìŠ¤í…œì´ ì•„ë‹˜!\"\n```\n\n---\n\n## âœ… ê°œì„ ëœ êµ¬ì¡° (ëª©í‘œ)\n\n### ìµœì¢… ëª¨ìŠµ\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ ì¢Œì¸¡ ë©”ë‰´    â”‚ ë©”ì¸ ì½˜í…ì¸  ì˜ì—­              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚              â”‚ í•œì‹ë‹¹ Data Hub               â”‚\nâ”‚ ğŸ“Š ëŒ€ì‹œë³´ë“œ  â”‚ í˜„ì¬ ìœ„ì¹˜: ëŒ€ì‹œë³´ë“œ           â”‚\nâ”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ğŸ”ë°ì´í„°ê´€ë¦¬ â”‚ [ì •ë³´ë°•ìŠ¤ë“¤] + [ë²„íŠ¼] + [ì°¨íŠ¸]â”‚\nâ”‚  - ìˆ˜ì§‘ì„¤ì •  â”‚                              â”‚\nâ”‚  - ìˆ˜ì§‘ê²°ê³¼  â”‚                              â”‚\nâ”‚  - ì¤‘ë³µê²€ì‚¬  â”‚ ë˜ëŠ” ë‹¤ë¥¸ í˜ì´ì§€ ì½˜í…ì¸       â”‚\nâ”‚  - í’ˆì§ˆê´€ë¦¬  â”‚                              â”‚\nâ”‚  - ë°ì´í„°í¸ì§‘â”‚                              â”‚\nâ”‚  - ìˆ˜ë™ì…ë ¥  â”‚                              â”‚\nâ”‚              â”‚                              â”‚\nâ”‚ âš™ï¸ ì‘ì—…ê´€ë¦¬ â”‚                              â”‚\nâ”‚  - ì‘ì—…ì‹¤í–‰  â”‚                              â”‚\nâ”‚  - ëª¨ë‹ˆí„°ë§  â”‚                              â”‚\nâ”‚  - ì´ë ¥ê´€ë¦¬  â”‚                              â”‚\nâ”‚  - í†µê³„      â”‚                              â”‚\nâ”‚              â”‚                              â”‚\nâ”‚ ğŸ“Š ë™ê¸°í™”ê´€ë¦¬â”‚                              â”‚\nâ”‚  - ë°°í¬í˜„í™©  â”‚                              â”‚\nâ”‚  - ë°°í¬ì´ë ¥  â”‚                              â”‚\nâ”‚  - ë°°í¬í†µê³„  â”‚                              â”‚\nâ”‚              â”‚                              â”‚\nâ”‚ ğŸ“ˆ ë¶„ì„     â”‚                              â”‚\nâ”‚              â”‚                              â”‚\nâ”‚ âš™ï¸ ì„¤ì •    â”‚                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… ê°œì„ :\n- ëª…í™•í•œ ì¢Œì¸¡ ë©”ë‰´ ë„¤ë¹„ê²Œì´ì…˜\n- ê¸°ëŠ¥ë³„ í˜ì´ì§€ ë¶„ë¦¬\n- ì„œë¸Œë©”ë‰´ êµ¬ì¡°\n- í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜\n```\n\n---\n\n## ğŸ”§ Replitì—ê²Œ ë³´ë‚¼ ìƒì„¸ ì§€ì‹œ\n\n```\nğŸš¨ URGENT: UI/UX ì™„ì „ ì¬êµ¬ì¡°í™”\n\ní˜„ì¬ ìƒí™©:\n- Backend API: ì™„ë²½ âœ…\n- Frontend êµ¬ì¡°: ìµœì•… âŒ\n\në¬¸ì œ:\ní˜„ì¬ /dashboardëŠ” \"ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ\"ì¼ ë¿\nì§„ì •í•œ \"ê´€ë¦¬ ì‹œìŠ¤í…œ\"ì´ ì•„ë‹™ë‹ˆë‹¤.\n\ní•´ê²°ì±…:\në‹¤ìŒ êµ¬ì¡°ë¡œ ì™„ì „ ë¦¬êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”:\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n## 1ë‹¨ê³„: ê³µí†µ ë ˆì´ì•„ì›ƒ ìƒì„± (2ì‹œê°„)\n\níŒŒì¼: data-hub/static/layouts/common.html\në˜ëŠ”: data-hub/templates/base.html\n\nêµ¬ì„±:\n```html\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Header (ë¡œê³  + ì‚¬ìš©ì ë©”ë‰´)    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ì¢Œì¸¡     â”‚ ì½˜í…ì¸  ì˜ì—­        â”‚\nâ”‚ ë©”ë‰´     â”‚                    â”‚\nâ”‚          â”‚ {% block content %} â”‚\nâ”‚ - ê°     â”‚ {% endblock %}     â”‚\nâ”‚   ë©”ë‰´   â”‚                    â”‚\nâ”‚   í•­ëª©   â”‚                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\në©”ë‰´ êµ¬ì¡° (HTML):\n```html\n<nav class=\"sidebar\">\n  <div class=\"menu-group\">\n    <a href=\"/dashboard\" class=\"menu-item\">\n      ğŸ“Š ëŒ€ì‹œë³´ë“œ\n    </a>\n  </div>\n  \n  <div class=\"menu-group\">\n    <div class=\"menu-label\">ğŸ” ë°ì´í„° ê´€ë¦¬</div>\n    <a href=\"/dashboard/data-management?tab=settings\">\n      â”œâ”€ ìˆ˜ì§‘ ì„¤ì •\n    </a>\n    <a href=\"/dashboard/data-management?tab=results\">\n      â”œâ”€ ìˆ˜ì§‘ ê²°ê³¼\n    </a>\n    <a href=\"/dashboard/data-management?tab=duplicates\">\n      â”œâ”€ ì¤‘ë³µ ê²€ì‚¬\n    </a>\n    <a href=\"/dashboard/data-management?tab=quality\">\n      â”œâ”€ í’ˆì§ˆ ê´€ë¦¬\n    </a>\n    <a href=\"/dashboard/data-management?tab=editor\">\n      â”œâ”€ ë°ì´í„° í¸ì§‘\n    </a>\n    <a href=\"/dashboard/data-management?tab=manual\">\n      â””â”€ ìˆ˜ë™ ì…ë ¥\n    </a>\n  </div>\n  \n  <div class=\"menu-group\">\n    <div class=\"menu-label\">âš™ï¸ ì‘ì—… ê´€ë¦¬</div>\n    <a href=\"/dashboard/jobs?tab=execute\">\n      â”œâ”€ ì‘ì—… ì‹¤í–‰\n    </a>\n    <a href=\"/dashboard/jobs?tab=monitoring\">\n      â”œâ”€ ëª¨ë‹ˆí„°ë§\n    </a>\n    <a href=\"/dashboard/jobs?tab=history\">\n      â”œâ”€ ì´ë ¥\n    </a>\n    <a href=\"/dashboard/jobs?tab=stats\">\n      â””â”€ í†µê³„\n    </a>\n  </div>\n  \n  <div class=\"menu-group\">\n    <div class=\"menu-label\">ğŸ“Š ë™ê¸°í™” ê´€ë¦¬</div>\n    <a href=\"/dashboard/sync?tab=status\">\n      â”œâ”€ ë°°í¬ í˜„í™©\n    </a>\n    <a href=\"/dashboard/sync?tab=history\">\n      â”œâ”€ ë°°í¬ ì´ë ¥\n    </a>\n    <a href=\"/dashboard/sync?tab=stats\">\n      â””â”€ ë°°í¬ í†µê³„\n    </a>\n  </div>\n  \n  <div class=\"menu-group\">\n    <a href=\"/dashboard/analytics\">\n      ğŸ“ˆ ë¶„ì„\n    </a>\n  </div>\n  \n  <div class=\"menu-group\">\n    <a href=\"/dashboard/settings\">\n      âš™ï¸ ì„¤ì •\n    </a>\n  </div>\n</nav>\n```\n\nCSS:\n```css\n.sidebar {\n  width: 240px;\n  background: #2c3e50;\n  color: white;\n  padding: 20px 0;\n  position: fixed;\n  height: 100vh;\n  overflow-y: auto;\n}\n\n.menu-group {\n  margin-bottom: 20px;\n  padding: 0 10px;\n}\n\n.menu-label {\n  font-size: 12px;\n  font-weight: bold;\n  color: #95a5a6;\n  margin-bottom: 8px;\n  padding-left: 10px;\n}\n\n.menu-item {\n  display: block;\n  padding: 10px 15px;\n  color: white;\n  text-decoration: none;\n  border-radius: 4px;\n  margin-bottom: 4px;\n  transition: background 0.2s;\n}\n\n.menu-item:hover,\n.menu-item.active {\n  background: #3498db;\n}\n\nmain {\n  margin-left: 240px;\n  padding: 20px;\n}\n```\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n## 2ë‹¨ê³„: í˜„ì¬ ëŒ€ì‹œë³´ë“œ í˜ì´ì§€ ìœ ì§€ (1ì‹œê°„)\n\nê²½ë¡œ: /dashboard\nêµ¬ì„±:\n- ìƒë‹¨: ì œëª© + í˜„ì¬ ìœ„ì¹˜ í‘œì‹œ\n- ë³¸ë¬¸: í˜„ì¬ì˜ ì •ë³´ë°•ìŠ¤ + ë²„íŠ¼ + ì°¨íŠ¸\n- ë³€ê²½ì‚¬í•­: ìƒˆ ë ˆì´ì•„ì›ƒ ì ìš©ë§Œ\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n## 3ë‹¨ê³„: ë°ì´í„° ê´€ë¦¬ í˜ì´ì§€ ìƒì„± (4ì‹œê°„)\n\nê²½ë¡œ: /dashboard/data-management\n\nêµ¬ì¡°:\n```html\n<div class=\"page-header\">\n  <h1>ğŸ” ë°ì´í„° ê´€ë¦¬</h1>\n  <p>ìˆ˜ì§‘ ì„¤ì •, ê²°ê³¼, ì¤‘ë³µ ê²€ì‚¬, ë°ì´í„° í¸ì§‘</p>\n</div>\n\n<div class=\"tab-menu\">\n  <button class=\"tab-btn active\" onclick=\"showTab('settings')\">\n    ğŸ“Œ ìˆ˜ì§‘ ì„¤ì •\n  </button>\n  <button class=\"tab-btn\" onclick=\"showTab('results')\">\n    ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼\n  </button>\n  <button class=\"tab-btn\" onclick=\"showTab('duplicates')\">\n    ğŸ”— ì¤‘ë³µ ê²€ì‚¬\n  </button>\n  <button class=\"tab-btn\" onclick=\"showTab('quality')\">\n    âœ… í’ˆì§ˆ ê´€ë¦¬\n  </button>\n  <button class=\"tab-btn\" onclick=\"showTab('editor')\">\n    âœï¸ ë°ì´í„° í¸ì§‘\n  </button>\n  <button class=\"tab-btn\" onclick=\"showTab('manual')\">\n    â• ìˆ˜ë™ ì…ë ¥\n  </button>\n</div>\n\n<div id=\"settings\" class=\"tab-content active\">\n  <!-- ê¸°ì¡´ /dashboard/collection-settings ë‚´ìš© -->\n</div>\n\n<div id=\"results\" class=\"tab-content\">\n  <!-- ê¸°ì¡´ /dashboard/quality-check ë‚´ìš© -->\n</div>\n\n<!-- ë‚˜ë¨¸ì§€ íƒ­ë“¤ -->\n```\n\nJavaScript:\n```javascript\nfunction showTab(tabName) {\n  // ëª¨ë“  íƒ­ ìˆ¨ê¸°ê¸°\n  document.querySelectorAll('.tab-content').forEach(tab => {\n    tab.classList.remove('active');\n  });\n  \n  // ëª¨ë“  ë²„íŠ¼ ë¹„í™œì„±í™”\n  document.querySelectorAll('.tab-btn').forEach(btn => {\n    btn.classList.remove('active');\n  });\n  \n  // ì„ íƒëœ íƒ­ í‘œì‹œ\n  document.getElementById(tabName).classList.add('active');\n  event.target.classList.add('active');\n}\n```\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n## 4ë‹¨ê³„: ì‘ì—… ê´€ë¦¬ í˜ì´ì§€ ìƒì„± (3ì‹œê°„)\n\nê²½ë¡œ: /dashboard/jobs\n\níƒ­ êµ¬ì¡°:\n1. ì‘ì—… ì‹¤í–‰\n   - 7ê°œ ì‘ì—… ë²„íŠ¼ (í˜„ì¬ ëŒ€ì‹œë³´ë“œì˜ ë²„íŠ¼ë“¤)\n   \n2. ëª¨ë‹ˆí„°ë§\n   - ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ë¦¬ìŠ¤íŠ¸\n   - ì§„í–‰ë¥ , ë¹„ìš©, ë¡œê·¸\n   \n3. ì´ë ¥\n   - ìµœê·¼ ì‘ì—… ì‹¤í–‰ ì´ë ¥\n   - ê° ì‘ì—…ì˜ ìƒì„¸ì •ë³´\n   \n4. í†µê³„\n   - ì›”ê°„ ì¶”ì´ ì°¨íŠ¸\n   - ì‘ì—…ë³„ ì‹¤í–‰ íšŸìˆ˜\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n## 5ë‹¨ê³„: ë™ê¸°í™” ê´€ë¦¬ í˜ì´ì§€ ìƒì„± (2ì‹œê°„)\n\nê²½ë¡œ: /dashboard/sync\n\níƒ­ êµ¬ì¡°:\n1. ë°°í¬ í˜„í™©\n   - ë™ê¸°í™” ê°€ëŠ¥ ë°ì´í„° ìˆ˜\n   - ì„ íƒ + ë°°í¬ UI\n   - ì§„í–‰ìƒí™©\n   \n2. ë°°í¬ ì´ë ¥\n   - ìµœê·¼ ë°°í¬ ê¸°ë¡\n   - ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ\n   \n3. ë°°í¬ í†µê³„\n   - ì›”ê°„ ë™ê¸°í™” í˜„í™©\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n## 6ë‹¨ê³„: ë¼ìš°íŒ… ë° ë„¤ë¹„ê²Œì´ì…˜ (2ì‹œê°„)\n\nmain.pyì—ì„œ:\n```python\n# ê¸°ì¡´ ëŒ€ì‹œë³´ë“œ\n@app.route('/dashboard')\ndef dashboard():\n    return render_template('dashboard.html')\n\n# ë°ì´í„° ê´€ë¦¬\n@app.route('/dashboard/data-management')\ndef data_management():\n    return render_template('data-management.html')\n\n# ì‘ì—… ê´€ë¦¬\n@app.route('/dashboard/jobs')\ndef jobs_management():\n    return render_template('jobs.html')\n\n# ë™ê¸°í™” ê´€ë¦¬\n@app.route('/dashboard/sync')\ndef sync_management():\n    return render_template('sync.html')\n\n# ê¸°íƒ€\n@app.route('/dashboard/analytics')\n@app.route('/dashboard/settings')\n```\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n## 7ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë° í†µí•© (2ì‹œê°„)\n\nì²´í¬ë¦¬ìŠ¤íŠ¸:\n[ ] ì¢Œì¸¡ ë©”ë‰´ í‘œì‹œ\n[ ] ê° ë©”ë‰´ í´ë¦­ ì‹œ í•´ë‹¹ í˜ì´ì§€ ì´ë™\n[ ] í˜„ì¬ ë©”ë‰´ í•˜ì´ë¼ì´íŠ¸ í‘œì‹œ\n[ ] ê° í˜ì´ì§€ ì½˜í…ì¸  ì •ìƒ í‘œì‹œ\n[ ] íƒ­ ì „í™˜ ì‘ë™\n[ ] ì‘ë‹µì„±(ë°˜ì‘í˜•) í™•ì¸\n[ ] ë¸Œë¼ìš°ì € í˜¸í™˜ì„± í™•ì¸\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n## ì˜ˆìƒ ì™„ë£Œ: 12ì‹œê°„ (ì˜¤ëŠ˜ë°¤ ~ ë‚´ì¼ ì˜¤ì „)\n\nì´ ì‘ì—… ì™„ë£Œ í›„:\nâ†’ ì§„ì •í•œ \"ê´€ë¦¬ ì‹œìŠ¤í…œ\" êµ¬ì¡° ì™„ì„±\nâ†’ Stage C (ì˜¨ë””ë§¨ë“œ ì‹œìŠ¤í…œ) ê°œë°œ ì¤€ë¹„ ì™„ë£Œ\nâ†’ ê¸°ëŠ¥ê³¼ UI/UXê°€ ì™„ë²½í•˜ê²Œ í†µí•©ëœ ì‹œìŠ¤í…œ\n\nì¤€ë¹„ ì™„ë£Œ? ì§€ê¸ˆ ì‹œì‘í•˜ì„¸ìš”!\n```\n\n---\n\n## ğŸ“‹ í˜„ì¬ ìˆëŠ” í˜ì´ì§€ë“¤ì„ ì–´ë””ì— í†µí•©í• ì§€\n\n| ê¸°ì¡´ í˜ì´ì§€ | ìƒˆ ìœ„ì¹˜ |\n|------------|--------|\n| /dashboard/collection-settings | /dashboard/data-management íƒ­1 |\n| /dashboard/quality-check | /dashboard/data-management íƒ­2-4 |\n| /dashboard/sync-management | /dashboard/sync íƒ­1 |\n| ëŒ€ì‹œë³´ë“œ | /dashboard (ìœ ì§€) |\n\n---\n\n## âœ… ì¦‰ì‹œ í•  ì¼\n\n### Replit:\n1. ìœ„ ìƒì„¸ ì§€ì‹œ ì½ê³  ì´í•´\n2. ê³µí†µ ë ˆì´ì•„ì›ƒ ìƒì„± ì‹œì‘ (ì§€ê¸ˆ)\n3. ê° í˜ì´ì§€ êµ¬ì¡° ìƒì„±\n4. ë¼ìš°íŒ… ì„¤ì •\n5. ë„¤ë¹„ê²Œì´ì…˜ í…ŒìŠ¤íŠ¸\n\n### ë‹¹ì‹ :\n1. 67ë²ˆ PDF ê²€í† \n2. ì´ Markdown íŒŒì¼ Replitê³¼ ê³µìœ \n3. 12ì‹œê°„ í›„ ê²°ê³¼ í™•ì¸\n\n---\n\n**UI/UX ì™„ì „ ì¬êµ¬ì¡°í™” = ì§„ì •í•œ ê´€ë¦¬ ì‹œìŠ¤í…œ íƒ„ìƒ!** ğŸ¯\n\n","size_bytes":13165},"data-hub/src/api/collection_request_routes.py":{"content":"\"\"\"\nCollection Request Routes - Stage C-1\nì˜¨ë””ë§¨ë“œ ìˆ˜ì§‘ ìš”ì²­ ê´€ë¦¬ API\n\"\"\"\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import text\nfrom typing import List, Dict, Any, Optional\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport uuid\nimport json\n\nfrom src.database.connection import get_db\n\nrouter = APIRouter(prefix=\"/api/data-management/collection-requests\", tags=[\"collection-requests\"])\n\n\nclass CollectionRequestCreate(BaseModel):\n    \"\"\"ìˆ˜ì§‘ ìš”ì²­ ìƒì„± ìŠ¤í‚¤ë§ˆ\"\"\"\n    name: str\n    description: Optional[str] = None\n    regions: List[str] = []\n    keywords: List[str] = []\n    reference_urls: List[str] = []\n    crawl_methods: List[str] = [\"google\", \"naver\"]\n    \n    \nclass CollectionRequestUpdate(BaseModel):\n    \"\"\"ìˆ˜ì§‘ ìš”ì²­ ìˆ˜ì • ìŠ¤í‚¤ë§ˆ\"\"\"\n    name: Optional[str] = None\n    description: Optional[str] = None\n    regions: Optional[List[str]] = None\n    keywords: Optional[List[str]] = None\n    reference_urls: Optional[List[str]] = None\n    crawl_methods: Optional[List[str]] = None\n\n\ndef calculate_estimated_cost(regions: List[str], keywords: List[str], crawl_methods: List[str]) -> float:\n    \"\"\"ì˜ˆìƒ ë¹„ìš© ê³„ì‚°\n    \n    Google: $15/í‚¤ì›Œë“œ/ì§€ì—­\n    Naver: $10/í‚¤ì›Œë“œ/ì§€ì—­\n    \"\"\"\n    cost = 0.0\n    region_count = len(regions) if regions else 1\n    keyword_count = len(keywords) if keywords else 1\n    \n    if \"google\" in crawl_methods:\n        cost += 15.0 * keyword_count * region_count\n    if \"naver\" in crawl_methods:\n        cost += 10.0 * keyword_count * region_count\n        \n    return round(cost, 2)\n\n\n@router.post(\"\")\nasync def create_collection_request(\n    request: CollectionRequestCreate,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìƒˆ ìˆ˜ì§‘ ìš”ì²­ ìƒì„±\"\"\"\n    try:\n        request_id = str(uuid.uuid4())\n        estimated_cost = calculate_estimated_cost(\n            request.regions, \n            request.keywords, \n            request.crawl_methods\n        )\n        \n        query = text(\"\"\"\n            INSERT INTO collection_requests \n            (id, name, description, regions, keywords, reference_urls, crawl_methods, estimated_cost, status, created_by, created_at)\n            VALUES \n            (:id, :name, :description, CAST(:regions AS jsonb), CAST(:keywords AS jsonb), CAST(:reference_urls AS jsonb), CAST(:crawl_methods AS jsonb), :estimated_cost, 'pending', 'admin', NOW())\n            RETURNING id, name, description, estimated_cost, status, created_at\n        \"\"\")\n        \n        result = db.execute(query, {\n            \"id\": request_id,\n            \"name\": request.name,\n            \"description\": request.description,\n            \"regions\": json.dumps(request.regions),\n            \"keywords\": json.dumps(request.keywords),\n            \"reference_urls\": json.dumps(request.reference_urls),\n            \"crawl_methods\": json.dumps(request.crawl_methods),\n            \"estimated_cost\": estimated_cost\n        })\n        \n        row = result.fetchone()\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"message\": \"ìˆ˜ì§‘ ìš”ì²­ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"data\": {\n                \"id\": row[0],\n                \"name\": row[1],\n                \"description\": row[2],\n                \"estimated_cost\": row[3],\n                \"status\": row[4],\n                \"created_at\": row[5].isoformat() if row[5] else None\n            }\n        }\n        \n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ìš”ì²­ ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"\")\nasync def get_collection_requests(\n    status: Optional[str] = None,\n    limit: int = 100,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ìš”ì²­ ëª©ë¡ ì¡°íšŒ\"\"\"\n    try:\n        if status:\n            query = text(\"\"\"\n                SELECT id, name, description, regions, keywords, reference_urls, crawl_methods, \n                       estimated_cost, status, created_by, created_at, started_at, completed_at, \n                       actual_cost, results_count\n                FROM collection_requests\n                WHERE status = :status\n                ORDER BY created_at DESC\n                LIMIT :limit\n            \"\"\")\n            result = db.execute(query, {\"status\": status, \"limit\": limit})\n        else:\n            query = text(\"\"\"\n                SELECT id, name, description, regions, keywords, reference_urls, crawl_methods, \n                       estimated_cost, status, created_by, created_at, started_at, completed_at, \n                       actual_cost, results_count\n                FROM collection_requests\n                ORDER BY created_at DESC\n                LIMIT :limit\n            \"\"\")\n            result = db.execute(query, {\"limit\": limit})\n        \n        rows = result.fetchall()\n        \n        requests = []\n        for row in rows:\n            requests.append({\n                \"id\": row[0],\n                \"name\": row[1],\n                \"description\": row[2],\n                \"regions\": row[3],\n                \"keywords\": row[4],\n                \"reference_urls\": row[5],\n                \"crawl_methods\": row[6],\n                \"estimated_cost\": row[7],\n                \"status\": row[8],\n                \"created_by\": row[9],\n                \"created_at\": row[10].isoformat() if row[10] else None,\n                \"started_at\": row[11].isoformat() if row[11] else None,\n                \"completed_at\": row[12].isoformat() if row[12] else None,\n                \"actual_cost\": row[13],\n                \"results_count\": row[14]\n            })\n        \n        return {\n            \"success\": True,\n            \"total\": len(requests),\n            \"data\": requests\n        }\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.get(\"/{request_id}\")\nasync def get_collection_request(\n    request_id: str,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ìš”ì²­ ìƒì„¸ ì¡°íšŒ\"\"\"\n    try:\n        query = text(\"\"\"\n            SELECT id, name, description, regions, keywords, reference_urls, crawl_methods, \n                   estimated_cost, status, created_by, created_at, started_at, completed_at, \n                   actual_cost, results_count\n            FROM collection_requests\n            WHERE id = :request_id\n        \"\"\")\n        \n        result = db.execute(query, {\"request_id\": request_id})\n        row = result.fetchone()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        return {\n            \"success\": True,\n            \"data\": {\n                \"id\": row[0],\n                \"name\": row[1],\n                \"description\": row[2],\n                \"regions\": row[3],\n                \"keywords\": row[4],\n                \"reference_urls\": row[5],\n                \"crawl_methods\": row[6],\n                \"estimated_cost\": row[7],\n                \"status\": row[8],\n                \"created_by\": row[9],\n                \"created_at\": row[10].isoformat() if row[10] else None,\n                \"started_at\": row[11].isoformat() if row[11] else None,\n                \"completed_at\": row[12].isoformat() if row[12] else None,\n                \"actual_cost\": row[13],\n                \"results_count\": row[14]\n            }\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.put(\"/{request_id}\")\nasync def update_collection_request(\n    request_id: str,\n    request: CollectionRequestUpdate,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ìš”ì²­ ìˆ˜ì • (pending ìƒíƒœë§Œ ê°€ëŠ¥)\"\"\"\n    try:\n        check_query = text(\"\"\"\n            SELECT status FROM collection_requests WHERE id = :request_id\n        \"\"\")\n        result = db.execute(check_query, {\"request_id\": request_id})\n        row = result.fetchone()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        if row[0] != 'pending':\n            raise HTTPException(status_code=400, detail=\"ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ë§Œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n        \n        update_fields = []\n        params = {\"request_id\": request_id}\n        \n        if request.name is not None:\n            update_fields.append(\"name = :name\")\n            params[\"name\"] = request.name\n        if request.description is not None:\n            update_fields.append(\"description = :description\")\n            params[\"description\"] = request.description\n        if request.regions is not None:\n            update_fields.append(\"regions = CAST(:regions AS jsonb)\")\n            params[\"regions\"] = json.dumps(request.regions)\n        if request.keywords is not None:\n            update_fields.append(\"keywords = CAST(:keywords AS jsonb)\")\n            params[\"keywords\"] = json.dumps(request.keywords)\n        if request.reference_urls is not None:\n            update_fields.append(\"reference_urls = CAST(:reference_urls AS jsonb)\")\n            params[\"reference_urls\"] = json.dumps(request.reference_urls)\n        if request.crawl_methods is not None:\n            update_fields.append(\"crawl_methods = CAST(:crawl_methods AS jsonb)\")\n            params[\"crawl_methods\"] = json.dumps(request.crawl_methods)\n        \n        if not update_fields:\n            raise HTTPException(status_code=400, detail=\"ìˆ˜ì •í•  í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤\")\n        \n        update_query = text(f\"\"\"\n            UPDATE collection_requests\n            SET {', '.join(update_fields)}\n            WHERE id = :request_id\n            RETURNING id, name, status\n        \"\"\")\n        \n        result = db.execute(update_query, params)\n        row = result.fetchone()\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"message\": \"ìˆ˜ì§‘ ìš”ì²­ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"data\": {\n                \"id\": row[0],\n                \"name\": row[1],\n                \"status\": row[2]\n            }\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ìˆ˜ì • ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.delete(\"/{request_id}\")\nasync def delete_collection_request(\n    request_id: str,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ìš”ì²­ ì‚­ì œ\"\"\"\n    try:\n        check_query = text(\"\"\"\n            SELECT status FROM collection_requests WHERE id = :request_id\n        \"\"\")\n        result = db.execute(check_query, {\"request_id\": request_id})\n        row = result.fetchone()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        if row[0] == 'running':\n            raise HTTPException(status_code=400, detail=\"ì‹¤í–‰ ì¤‘ì¸ ìš”ì²­ì€ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        delete_query = text(\"\"\"\n            DELETE FROM collection_requests WHERE id = :request_id\n        \"\"\")\n        \n        db.execute(delete_query, {\"request_id\": request_id})\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"message\": \"ìˆ˜ì§‘ ìš”ì²­ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤\"\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì‚­ì œ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/{request_id}/start\")\nasync def start_collection_request(\n    request_id: str,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ìš”ì²­ ì¦‰ì‹œ ì‹¤í–‰\"\"\"\n    try:\n        check_query = text(\"\"\"\n            SELECT status, name, keywords, regions, crawl_methods \n            FROM collection_requests \n            WHERE id = :request_id\n        \"\"\")\n        result = db.execute(check_query, {\"request_id\": request_id})\n        row = result.fetchone()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        if row[0] == 'running':\n            raise HTTPException(status_code=400, detail=\"ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤\")\n        \n        update_query = text(\"\"\"\n            UPDATE collection_requests\n            SET status = 'running', started_at = NOW()\n            WHERE id = :request_id\n            RETURNING id, name, status, started_at\n        \"\"\")\n        \n        result = db.execute(update_query, {\"request_id\": request_id})\n        row = result.fetchone()\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"message\": \"ìˆ˜ì§‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"data\": {\n                \"id\": row[0],\n                \"name\": row[1],\n                \"status\": row[2],\n                \"started_at\": row[3].isoformat() if row[3] else None\n            }\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n\n\n@router.post(\"/{request_id}/cancel\")\nasync def cancel_collection_request(\n    request_id: str,\n    db: Session = Depends(get_db)\n):\n    \"\"\"ìˆ˜ì§‘ ìš”ì²­ ì·¨ì†Œ\"\"\"\n    try:\n        check_query = text(\"\"\"\n            SELECT status FROM collection_requests WHERE id = :request_id\n        \"\"\")\n        result = db.execute(check_query, {\"request_id\": request_id})\n        row = result.fetchone()\n        \n        if not row:\n            raise HTTPException(status_code=404, detail=\"ìˆ˜ì§‘ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n        \n        if row[0] != 'running':\n            raise HTTPException(status_code=400, detail=\"ì‹¤í–‰ ì¤‘ì¸ ìš”ì²­ë§Œ ì·¨ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n        \n        update_query = text(\"\"\"\n            UPDATE collection_requests\n            SET status = 'cancelled', completed_at = NOW()\n            WHERE id = :request_id\n            RETURNING id, name, status\n        \"\"\")\n        \n        result = db.execute(update_query, {\"request_id\": request_id})\n        row = result.fetchone()\n        db.commit()\n        \n        return {\n            \"success\": True,\n            \"message\": \"ìˆ˜ì§‘ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤\",\n            \"data\": {\n                \"id\": row[0],\n                \"name\": row[1],\n                \"status\": row[2]\n            }\n        }\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        db.rollback()\n        raise HTTPException(status_code=500, detail=f\"ì·¨ì†Œ ì‹¤íŒ¨: {str(e)}\")\n","size_bytes":14526},"attached_assets/stage_c_implementation_guide_1762611822875.md":{"content":"# Stage C: ì˜¨ë””ë§¨ë“œ + AI/ìˆ˜ë™ ì…ë ¥ ì‹œìŠ¤í…œ - ì‹¤í–‰ ê°€ì´ë“œ\n\n**ëŒ€ìƒ**: ë‹¹ì‹  + Replit (í˜‘ë ¥ ê°œë°œ)  \n**ë¬¸ì„œ ë²ˆí˜¸**: 63ë²ˆ PDF (ìƒì„¸ ì„¤ê³„) ì°¸ê³   \n**ê°œë°œ ê¸°ê°„**: 18-24ì‹œê°„ (ì•½ 3-4ì¼)  \n**ëª©í‘œ**: ìë™í™” ê¸°ë°˜ â†’ ê´€ë¦¬ì ì¤‘ì‹¬ ì˜¨ë””ë§¨ë“œ + ë‹¤ì¤‘ ì…ë ¥ ì±„ë„ ì „í™˜\n\n---\n\n## ğŸ“– í•µì‹¬ ê°œë… ì •ë¦¬\n\n### \"ì˜¨ë””ë§¨ë“œ\" vs \"ìë™í™”\"ì˜ ì°¨ì´\n\n**ìë™í™” (Before - í˜„ì¬)**:\n```\në§¤ì¼ ê°™ì€ ì‹œê°„ì— ìë™ ì‹¤í–‰\nâ†’ ë¶ˆí•„ìš”í•œ ë¹„ìš© ë‚­ë¹„\nâ†’ ê´€ë¦¬ì í†µì œ ë¶ˆê°€\nâ†’ ì¤‘ë³µ/í’ˆì§ˆ ì´ìŠˆë§Œ ì‚¬í›„ ì²˜ë¦¬\n```\n\n**ì˜¨ë””ë§¨ë“œ (After - ëª©í‘œ)**:\n```\nê´€ë¦¬ì: \"ì´ ì§€ì—­/í‚¤ì›Œë“œ ìˆ˜ì§‘í• ë˜\" â†’ [ì‹¤í–‰ ë²„íŠ¼]\nâ†’ í•„ìš”í•œ ê²ƒë§Œ ìˆ˜ì§‘\nâ†’ ë¹„ìš© í†µì œ ê°€ëŠ¥\nâ†’ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§/í¸ì§‘ ê°€ëŠ¥\n```\n\n### \"ìˆ˜ë™ ì…ë ¥ 3ì±„ë„\"\n\n1. **URL ìë™ íŒŒì‹±**: êµ¬ê¸€/ë„¤ì´ë²„ Place URL â†’ AI ìë™ ë°ì´í„° ì¶”ì¶œ\n2. **CSV ì¼ê´„ ì—…ë¡œë“œ**: ì—‘ì…€ íŒŒì¼ í•œ ë²ˆì— ë‹¤ìˆ˜ ì—…ì²´ ì…ë ¥\n3. **ì§ì ‘ íƒ€ì´í•‘**: í¼ ì±„ì›Œì„œ í•œ ê±´ì”© ì…ë ¥\n\nâ†’ Apify ì˜ì¡´ë„ ìµœì†Œí™” + ë°ì´í„° í’ˆì§ˆ ê·¹ëŒ€í™”\n\n---\n\n## ğŸ¯ Stage C 5ê°œ Phase ì‹¤í–‰ ê³„íš\n\n### â±ï¸ ì „ì²´ íƒ€ì„ë¼ì¸\n\n```\nDay 1 (11ì›” 9ì¼):\n- C-1 ê°œë°œ (4ì‹œê°„) - ìˆ˜ì§‘ ìš”ì²­ ê´€ë¦¬\n- C-2 ê°œë°œ (6ì‹œê°„) - ìŠ¤í”„ë ˆë“œì‹œíŠ¸ + ì¸ê¸°ë„ ì§€ìˆ˜\n\nDay 2 (11ì›” 10ì¼):\n- C-3 ê°œë°œ (5ì‹œê°„) - 3ê°€ì§€ ìˆ˜ë™ ì…ë ¥\n- C-4 ê°œë°œ (4ì‹œê°„) - í†µí•© í¸ì§‘\n\nDay 3 (11ì›” 11ì¼):\n- C-5 ê°œë°œ (4ì‹œê°„) - ë°°í¬ ê´€ë¦¬\n- í†µí•©í…ŒìŠ¤íŠ¸ (2ì‹œê°„)\n- ìµœì¢… ë‹¤ë“¬ê¸° (2ì‹œê°„)\n\nâ†’ 11ì›” 11ì¼ ì™„ì„±!\n```\n\n---\n\n## ğŸ”§ ê° Phaseë³„ êµ¬í˜„ ìƒì„¸\n\n### Phase C-1: ìˆ˜ì§‘ ìš”ì²­ ê´€ë¦¬ (4ì‹œê°„)\n\n**ëª©í‘œ**: ê´€ë¦¬ìê°€ ì›í•  ë•Œ ì›í•˜ëŠ” ë°ì´í„°ë§Œ ìˆ˜ì§‘ ê°€ëŠ¥í•˜ê²Œ\n\n#### êµ¬í˜„ ìˆœì„œ:\n\n1. **DB í…Œì´ë¸” ìƒì„±** (30ë¶„)\n   ```sql\n   CREATE TABLE collection_requests (\n     id VARCHAR(36) PRIMARY KEY,\n     name VARCHAR(255) NOT NULL,\n     description TEXT,\n     regions JSON,           -- [\"ê°•ë‚¨êµ¬\", \"ì„œì´ˆêµ¬\"]\n     keywords JSON,          -- [\"ê°ˆë¹„\", \"í•œìš°\"]\n     reference_urls JSON,    -- ì°¸ê³  URL ë°°ì—´\n     crawl_methods JSON,     -- [\"google\", \"naver\"]\n     estimated_cost FLOAT,\n     status ENUM('pending', 'running', 'completed', 'failed'),\n     created_by VARCHAR(100),\n     created_at TIMESTAMP,\n     started_at TIMESTAMP,\n     completed_at TIMESTAMP,\n     actual_cost FLOAT,\n     results_count INT\n   );\n   ```\n\n2. **Backend API ì‘ì„±** (90ë¶„)\n   ```\n   POST /api/data-management/collection-requests\n   - ìš”ì²­ ìƒì„± (validate í¬í•¨)\n   - ì˜ˆìƒ ë¹„ìš© ìë™ ê³„ì‚°\n   \n   GET /api/data-management/collection-requests\n   - ëª¨ë“  ìš”ì²­ ëª©ë¡ (ìƒíƒœ í•„í„°)\n   \n   GET /api/data-management/collection-requests/{id}\n   - ìš”ì²­ ìƒì„¸ì •ë³´\n   \n   PUT /api/data-management/collection-requests/{id}\n   - ìš”ì²­ ìˆ˜ì • (pending ìƒíƒœë§Œ)\n   \n   DELETE /api/data-management/collection-requests/{id}\n   - ìš”ì²­ ì‚­ì œ\n   \n   POST /api/data-management/collection-requests/{id}/start\n   - ì¦‰ì‹œ ì‹¤í–‰ (Apify ì‘ì—… ì§€ì‹œ)\n   \n   POST /api/data-management/collection-requests/{id}/cancel\n   - ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ì¤‘ë‹¨\n   ```\n\n3. **Frontend í˜ì´ì§€ ì‘ì„±** (60ë¶„)\n   ```\n   /dashboard/collection-request\n   \n   UI êµ¬ì„±:\n   - [+ ìƒˆ ìš”ì²­] ë²„íŠ¼\n   - ìƒíƒœë³„ íƒ­ (ëŒ€ê¸°ì¤‘/ì‹¤í–‰ì¤‘/ì™„ë£Œë¨)\n   - ê° ìš”ì²­ ì¹´ë“œ (ìƒíƒœ, ì˜ˆìƒë¹„ìš©, ì•¡ì…˜ ë²„íŠ¼)\n   - ëª¨ë‹¬: ìƒˆ ìš”ì²­ ìƒì„± í¼\n   ```\n\n4. **í…ŒìŠ¤íŠ¸** (30ë¶„)\n   - API ê°ê° í…ŒìŠ¤íŠ¸\n   - UI í¼ ê²€ì¦\n   - ì‹¤ì œ ìˆ˜ì§‘ ìš”ì²­ ìƒì„± í…ŒìŠ¤íŠ¸\n\n---\n\n### Phase C-2: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ê²°ê³¼ + ì¸ê¸°ë„ (6ì‹œê°„)\n\n**ëª©í‘œ**: ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê³ , AI ê¸°ë°˜ ì¸ê¸°ë„ ê³„ì‚°\n\n#### êµ¬í˜„ ìˆœì„œ:\n\n1. **DB í…Œì´ë¸” ìƒì„±** (30ë¶„)\n   ```sql\n   CREATE TABLE collection_results (\n     id VARCHAR(36) PRIMARY KEY,\n     request_id VARCHAR(36) FOREIGN KEY,\n     name VARCHAR(255),\n     address TEXT,\n     phone VARCHAR(20),\n     google_rating FLOAT,\n     google_reviews INT,\n     naver_rating FLOAT,\n     naver_reviews INT,\n     avg_rating FLOAT,\n     total_reviews INT,\n     youtube_mentions INT DEFAULT 0,\n     blog_mentions INT DEFAULT 0,\n     popularity_score INT,      -- ê³„ì‚°ë¨ (0-100)\n     quality_score INT,         -- ê¸°ì¡´ í’ˆì§ˆ\n     image_count INT,\n     description_length INT,\n     has_hours BOOLEAN,\n     coordinates JSON,          -- {\"lat\": 37.xxx, \"lng\": 127.xxx}\n     source VARCHAR(50),        -- \"google\", \"naver\", \"manual\", \"ai_parsed\"\n     status ENUM('pending', 'approved', 'rejected', 'excluded'),\n     created_at TIMESTAMP,\n     last_updated_at TIMESTAMP\n   );\n   ```\n\n2. **ì¸ê¸°ë„ ê³„ì‚° ë¡œì§** (120ë¶„)\n   ```\n   Backend: ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜\n   \n   popularity_score = (\n     (avg_rating / 5.0) * 30 +\n     (min(total_reviews / 5000, 1) * 25) +\n     (min(youtube_mentions / 10, 1) * 20) +\n     (min(blog_mentions / 50, 1) * 15) +\n     (quality_score / 100 * 10)\n   )\n   \n   ì™¸ë¶€ API ì—°ë™ (ì„ íƒ):\n   - YouTube Data API: ì±„ë„/ì˜ìƒ ê²€ìƒ‰\n   - Naver Blog Search: ë¸”ë¡œê·¸ ì–¸ê¸‰ ìˆ˜\n   \n   (í˜„ì¬ëŠ” ìˆ˜ë™ ì…ë ¥ ê°€ëŠ¥í•˜ê²Œ)\n   ```\n\n3. **Backend API** (90ë¶„)\n   ```\n   GET /api/data-management/collection-results/{request_id}\n   - ìš”ì²­ë³„ ìˆ˜ì§‘ ê²°ê³¼ ëª©ë¡ (í•„í„°/ì •ë ¬)\n   \n   GET /api/data-management/collection-results/{id}\n   - ìƒì„¸ì •ë³´ (ëª¨ë“  í•„ë“œ + ì¸ê¸°ë„ ìƒì„¸)\n   \n   POST /api/data-management/collection-results/{id}/approve\n   - ìŠ¹ì¸ ì²˜ë¦¬\n   \n   POST /api/data-management/collection-results/{id}/reject\n   - ê±°ì ˆ ì²˜ë¦¬\n   \n   GET /api/data-management/collection-results/stats\n   - í†µê³„ (í‰ê·  ì¸ê¸°ë„, í’ˆì§ˆ ë¶„í¬ ë“±)\n   ```\n\n4. **Frontend í˜ì´ì§€** (90ë¶„)\n   ```\n   /dashboard/collection-results\n   \n   UI êµ¬ì„±:\n   - ìŠ¤í”„ë ˆë“œì‹œíŠ¸í˜• í…Œì´ë¸”\n   - ì»¬ëŸ¼: ì´ë¦„, ì£¼ì†Œ, í‰ì , ë¦¬ë·°ìˆ˜, í’ˆì§ˆ, ì¸ê¸°ë„\n   - ì •ë ¬: ì¸ê¸°ë„ìˆœ, í‰ì ìˆœ, í’ˆì§ˆìˆœ\n   - í•„í„°: í’ˆì§ˆì ìˆ˜, ì¸ê¸°ë„, ìƒíƒœ\n   - ê° í–‰ [ë³‘í•©] [ìƒì„¸] [í¸ì§‘] [ì‚­ì œ] ë²„íŠ¼\n   \n   ìƒì„¸ì •ë³´ ëª¨ë‹¬:\n   - ëª¨ë“  í•„ë“œ í‘œì‹œ\n   - ì¸ê¸°ë„ ì ìˆ˜ ë¶„ì„\n   - í¬ë¡¤ë§ ì¶œì²˜ ëª…ì‹œ\n   ```\n\n5. **í…ŒìŠ¤íŠ¸** (30ë¶„)\n   - ì¸ê¸°ë„ ê³„ì‚° ê²€ì¦\n   - í•„í„°/ì •ë ¬ ì‘ë™ í™•ì¸\n   - ìƒì„¸ì •ë³´ ëª¨ë‹¬ ë Œë”ë§\n\n---\n\n### Phase C-3: 3ê°€ì§€ ìˆ˜ë™ ì…ë ¥ ë°©ì‹ (5ì‹œê°„)\n\n**ëª©í‘œ**: Apify ì™¸ì— ë‹¤ì–‘í•œ ì±„ë„ë¡œ ë°ì´í„° ì…ë ¥ ê°€ëŠ¥\n\n#### êµ¬í˜„ ìˆœì„œ:\n\n1. **DB í…Œì´ë¸”** (30ë¶„)\n   ```sql\n   CREATE TABLE manual_entries (\n     id VARCHAR(36) PRIMARY KEY,\n     name VARCHAR(255),\n     address TEXT,\n     phone VARCHAR(20),\n     category VARCHAR(100),\n     rating FLOAT,\n     review_count INT,\n     price_range VARCHAR(50),\n     hours TEXT,              -- JSON or plain text\n     description TEXT,\n     image_urls JSON,         -- ì´ë¯¸ì§€ URL ë°°ì—´\n     source_url VARCHAR(500), -- Google/Naver Place URL\n     source_type ENUM('url_parsed', 'csv_import', 'manual_entry'),\n     created_by VARCHAR(100),\n     created_at TIMESTAMP,\n     status ENUM('pending_review', 'approved', 'rejected')\n   );\n   ```\n\n2. **URL íŒŒì‹± ê¸°ëŠ¥** (120ë¶„)\n   \n   **Option A: ê¸°ì¡´ í¬ë¡¤ëŸ¬ í™œìš©**\n   ```\n   Google Place URL â†’ Apify Google Maps crawler\n   Naver Place URL â†’ Apify Naver Maps crawler\n   â†’ ê¸°ì¡´ API ì¬í™œìš©\n   ```\n   \n   **Option B: ê°„ë‹¨í•œ ë°ì´í„° ì¶”ì¶œ**\n   ```\n   Selenium/Puppeteerë¡œ ê°„ë‹¨íˆ ë°ì´í„° ì¶”ì¶œ\n   (ì •ì‹ í¬ë¡¤ëŸ¬ë³´ë‹¤ ê°€ë³ê³  ë¹ ë¦„)\n   ```\n   \n   êµ¬í˜„:\n   - `/parse-url` API ì—”ë“œí¬ì¸íŠ¸\n   - ì…ë ¥: Google/Naver Place URL\n   - ì¶œë ¥: ì´ë¦„, ì£¼ì†Œ, í‰ì , ë¦¬ë·°, ì´ë¯¸ì§€ ë“±\n   \n   ```\n   POST /api/data-management/manual-entries/parse-url\n   Request: { url: \"https://maps.google.com/...\" }\n   Response: {\n     name: \"ê°•ë‚¨ê°ˆë¹„\",\n     address: \"...\",\n     rating: 4.7,\n     reviews: 2840,\n     images: [...],\n     ... (ìë™ ì¶”ì¶œëœ í•„ë“œ)\n   }\n   ```\n\n3. **CSV ì—…ë¡œë“œ ê¸°ëŠ¥** (90ë¶„)\n   ```\n   POST /api/data-management/manual-entries/csv-import\n   \n   í•„ìˆ˜ ì»¬ëŸ¼: name, address, phone\n   ì„ íƒ ì»¬ëŸ¼: category, rating, review_count, description\n   \n   ê²€ì¦:\n   - í•„ìˆ˜ í•„ë“œ í™•ì¸\n   - ì¤‘ë³µ ì²´í¬\n   - ì£¼ì†Œ ìœ íš¨ì„± (ê°„ë‹¨íˆ)\n   \n   ê²°ê³¼: ë¯¸ë¦¬ë³´ê¸° + ì¼ê´„ ì €ì¥\n   ```\n\n4. **ì§ì ‘ ì…ë ¥ í¼** (60ë¶„)\n   ```\n   POST /api/data-management/manual-entries\n   \n   í¼ í•„ë“œ:\n   - name (í•„ìˆ˜)\n   - address (í•„ìˆ˜)\n   - phone (í•„ìˆ˜)\n   - category (ì„ íƒ)\n   - rating, review_count (ì„ íƒ)\n   - description (ì„ íƒ)\n   - hours (ì„ íƒ)\n   - images (ì„ íƒ, ìµœëŒ€ 5ì¥)\n   \n   ì €ì¥ í›„: ì¤‘ë³µ ê²€ì‚¬ â†’ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° â†’ ìŠ¹ì¸ ëŒ€ê¸°\n   ```\n\n5. **Frontend** (60ë¶„)\n   ```\n   /dashboard/manual-data-entry\n   \n   3ê°œ íƒ­:\n   - URL íŒŒì‹±\n   - CSV ì—…ë¡œë“œ\n   - ì§ì ‘ ì…ë ¥\n   \n   ê° íƒ­ë³„ UI êµ¬í˜„\n   ```\n\n---\n\n### Phase C-4: í†µí•© í¸ì§‘ & ë³‘í•© (4ì‹œê°„)\n\n**ëª©í‘œ**: í¬ë¡¤ë§ + ìˆ˜ë™ ì…ë ¥ ë°ì´í„°ë¥¼ í•œ ê³³ì—ì„œ ê´€ë¦¬\n\n#### êµ¬í˜„ ìˆœì„œ:\n\n1. **í†µí•© ë°ì´í„°ë·°** (90ë¶„)\n   ```\n   /dashboard/unified-data-editor\n   \n   íƒ­:\n   - í¬ë¡¤ë§ ê²°ê³¼\n   - ìˆ˜ë™ ì…ë ¥\n   - ì „ì²´ ëª©ë¡ (í†µí•©)\n   \n   ê° í–‰:\n   - ì¶œì²˜ í‘œì‹œ (Google/Naver/Manual/AI)\n   - ìƒíƒœ (pending/approved/rejected)\n   - í¸ì§‘ ë²„íŠ¼\n   ```\n\n2. **ì¤‘ë³µ ê²€ì‚¬ & ë³‘í•©** (120ë¶„)\n   ```\n   ê¸°ì¡´ ë¡œì§ í™œìš©:\n   - Exact Match (100% ì¼ì¹˜)\n   - Fuzzy Match (85%+)\n   \n   ë³‘í•© ë¡œì§:\n   - 2ê°œ ì´ìƒ ì¤‘ë³µ ë°œê²¬ ì‹œ\n   - [ë³‘í•© ì œì•ˆ] í‘œì‹œ\n   - ê´€ë¦¬ì ì„ íƒ: ë³‘í•© / ë³„ê°œ ìœ ì§€\n   \n   ë³‘í•© ë°©ì‹:\n   - Google ê¸°ì¤€ / Naver ê¸°ì¤€ / í‰ê·  ë“± ì„ íƒ ê°€ëŠ¥\n   - í†µí•© ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n   ```\n\n3. **í¸ì§‘ ì›Œí¬í”Œë¡œìš°** (60ë¶„)\n   ```\n   ê° ë°ì´í„°:\n   - [í¸ì§‘] ë²„íŠ¼ â†’ ëª¨ë‹¬ ì—´ê¸°\n   - ëª¨ë“  í•„ë“œ ìˆ˜ì • ê°€ëŠ¥\n   - [ì €ì¥] â†’ ì¦‰ì‹œ ë°˜ì˜\n   - [ë³‘í•©] â†’ ì¤‘ë³µ ë³‘í•© ëŒ€í™”\n   - [ì œì™¸] â†’ ë©”ì¸ì•± ë°°í¬ ì œì™¸\n   - [ê±°ì ˆ] â†’ ë°ì´í„° ì‚­ì œ\n   ```\n\n4. **ë²„ì „ ê´€ë¦¬** (30ë¶„)\n   ```\n   ê° í¸ì§‘ë§ˆë‹¤:\n   - ìˆ˜ì • ì´ë ¥ ê¸°ë¡\n   - ëˆ„ê°€ / ì–¸ì œ / ë­˜ ìˆ˜ì •í–ˆëŠ”ì§€\n   - ì´ì „ ë²„ì „ ë³µêµ¬ ê°€ëŠ¥\n   ```\n\n---\n\n### Phase C-5: ì„ íƒì  ë°°í¬ & ë™ê¸°í™” (4ì‹œê°„)\n\n**ëª©í‘œ**: ìŠ¹ì¸ëœ ë°ì´í„°ë§Œ ë©”ì¸ì•±ì— ë°°í¬\n\n#### êµ¬í˜„ ìˆœì„œ:\n\n1. **ë°°í¬ ëŒ€ìƒ ì„ íƒ** (90ë¶„)\n   ```\n   /dashboard/deployment-manager\n   \n   í•„í„°:\n   - ìƒíƒœ: ìŠ¹ì¸ë¨ë§Œ\n   - ì¸ê¸°ë„: ë†’ìŒ ì´ìƒ\n   - í’ˆì§ˆì ìˆ˜: 80 ì´ìƒ\n   - ì§€ì—­: íŠ¹ì • ì§€ì—­ë§Œ ì„ íƒ\n   \n   ì„ íƒ ë°©ì‹:\n   - ì²´í¬ë°•ìŠ¤ë¡œ ê°œë³„ ì„ íƒ\n   - [ëª¨ë‘ ì„ íƒ] / [í•„í„° ì ìš©] ì„ íƒ\n   \n   ì„ íƒ ê²°ê³¼: Xê°œ ì„ íƒë¨\n   ```\n\n2. **ë°°í¬ ë¯¸ë¦¬ë³´ê¸° & ì¼ì •** (60ë¶„)\n   ```\n   [ë°°í¬ ë¯¸ë¦¬ë³´ê¸°]\n   - ë°°í¬ë  ë°ì´í„° ëª©ë¡\n   - ê° í•„ë“œ ê°’ í™•ì¸\n   - ë°°í¬ ì „ ìµœì¢… í™•ì¸\n   \n   ë°°í¬ ì¼ì •:\n   - ì¦‰ì‹œ ë°°í¬\n   - ì˜ˆì•½ ë°°í¬ (ì‹œê°„ ì§€ì •)\n   ```\n\n3. **ë°°í¬ ì‹¤í–‰ & ê²°ê³¼ ì¶”ì ** (60ë¶„)\n   ```\n   POST /api/data-management/deployment/execute\n   \n   ë°°í¬ ê³¼ì •:\n   - ë°°ì¹˜ ìƒì„±\n   - ë©”ì¸ì•± API í˜¸ì¶œ\n   - ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ\n   - ì„±ê³µ/ì‹¤íŒ¨ ê²°ê³¼ ê¸°ë¡\n   \n   ê²°ê³¼:\n   - ë°°í¬ ì™„ë£Œ\n   - ë°°í¬ ì´ë ¥ ì €ì¥\n   - ë¡¤ë°± ê¸°ëŠ¥\n   ```\n\n4. **Frontend UI** (30ë¶„)\n   ```\n   ë°°í¬ í˜„í™© ëŒ€ì‹œë³´ë“œ:\n   - ë°°í¬ ê°€ëŠ¥ ë°ì´í„° ìˆ˜\n   - ìµœê·¼ ë°°í¬ ì´ë ¥\n   - ê° ë°°í¬ì˜ ìƒíƒœ/ê²°ê³¼\n   ```\n\n---\n\n## âœ… ë‹¨ê³„ë³„ ì²´í¬í¬ì¸íŠ¸\n\n### End of Day 1 (C-1 + C-2)\n\n```\n[ ] C-1 ìˆ˜ì§‘ ìš”ì²­ CRUD API ì™„ì„± & í…ŒìŠ¤íŠ¸\n[ ] C-1 Frontend í˜ì´ì§€ ì‘ë™\n[ ] C-2 ì¸ê¸°ë„ ê³„ì‚° ë¡œì§ ì™„ì„±\n[ ] C-2 ìŠ¤í”„ë ˆë“œì‹œíŠ¸ í…Œì´ë¸” ë Œë”ë§\n[ ] í†µí•©í…ŒìŠ¤íŠ¸: ìš”ì²­ ìƒì„± â†’ ê²°ê³¼ ì¡°íšŒ â†’ í…Œì´ë¸” í‘œì‹œ\n```\n\n### End of Day 2 (C-3 + C-4)\n\n```\n[ ] C-3 URL íŒŒì‹± API ì‘ë™\n[ ] C-3 CSV ì—…ë¡œë“œ ê¸°ëŠ¥ ì‘ë™\n[ ] C-3 ì§ì ‘ ì…ë ¥ í¼ ì‘ë™\n[ ] C-4 í†µí•© í¸ì§‘ í˜ì´ì§€ ë Œë”ë§\n[ ] C-4 ì¤‘ë³µ ê²€ì‚¬ & ë³‘í•© ì‘ë™\n```\n\n### End of Day 3 (C-5 + ìµœì¢…)\n\n```\n[ ] C-5 ë°°í¬ ëŒ€ìƒ ì„ íƒ & ë¯¸ë¦¬ë³´ê¸°\n[ ] C-5 ë°°í¬ ì‹¤í–‰ & ì´ë ¥ ì¶”ì \n[ ] ëª¨ë“  í˜ì´ì§€ í†µí•©í…ŒìŠ¤íŠ¸\n[ ] replit.md ì—…ë°ì´íŠ¸\n[ ] ìµœì¢… ë°ëª¨ ì¤€ë¹„\n```\n\n---\n\n## ğŸ“‹ Replitì—ê²Œ ë³´ë‚¼ ìµœì¢… ì§€ì‹œ\n\n```\nStage C (ì˜¨ë””ë§¨ë“œ + AI/ìˆ˜ë™ ì…ë ¥) ê°œë°œ ì‹œì‘:\n\n63ë²ˆ PDF ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ìˆœì„œë¡œ ê°œë°œ:\n\nDay 1:\nâ–¡ C-1: ìˆ˜ì§‘ ìš”ì²­ ê´€ë¦¬ (4ì‹œê°„)\n  - collection_requests í…Œì´ë¸”\n  - 6ê°œ API ì—”ë“œí¬ì¸íŠ¸\n  - Frontend /dashboard/collection-request\n\nâ–¡ C-2: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ê²°ê³¼ + ì¸ê¸°ë„ (6ì‹œê°„)\n  - collection_results í…Œì´ë¸”\n  - ì¸ê¸°ë„ ê³„ì‚° ë¡œì§\n  - ìŠ¤í”„ë ˆë“œì‹œíŠ¸í˜• í…Œì´ë¸” UI\n  - í•„í„°/ì •ë ¬/ìƒì„¸ì •ë³´\n\nDay 2:\nâ–¡ C-3: 3ê°€ì§€ ìˆ˜ë™ ì…ë ¥ (5ì‹œê°„)\n  - URL íŒŒì‹± (Google/Naver)\n  - CSV ì—…ë¡œë“œ\n  - ì§ì ‘ ì…ë ¥ í¼\n\nâ–¡ C-4: í†µí•© í¸ì§‘ & ë³‘í•© (4ì‹œê°„)\n  - í¬ë¡¤ë§ + ìˆ˜ë™ í†µí•© ë³´ê¸°\n  - ì¤‘ë³µ ê²€ì‚¬ & ë³‘í•©\n  - ì›Œí¬í”Œë¡œìš°: í¸ì§‘ â†’ ë³‘í•© â†’ ìƒíƒœë³€ê²½\n\nDay 3:\nâ–¡ C-5: ë°°í¬ ê´€ë¦¬ (4ì‹œê°„)\n  - ë°°í¬ ëŒ€ìƒ ì„ íƒ\n  - ë°°í¬ ë¯¸ë¦¬ë³´ê¸° & ì‹¤í–‰\n  - ì´ë ¥ ì¶”ì \n\nâ–¡ í†µí•©í…ŒìŠ¤íŠ¸ + replit.md ì—…ë°ì´íŠ¸\n\nëª¨ë‘ ì´ Markdown ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ì§„í–‰í•´ì£¼ì„¸ìš”.\nì™„ë£Œ í›„ ê° ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³  ë¶€íƒí•©ë‹ˆë‹¤.\n```\n\n---\n\n**ì´ê²ƒìœ¼ë¡œ í•œì‹ë‹¹ Data HubëŠ” ì™„ì „íˆ ìƒˆë¡œìš´ ì°¨ì›ì˜ \"ì‚¬ëŒ ì¤‘ì‹¬ ë°ì´í„° ì˜¤í¼ë ˆì´ì…˜ í”Œë«í¼\"ìœ¼ë¡œ ì§„í™”í•©ë‹ˆë‹¤!** ğŸš€\n\n","size_bytes":13210}},"version":2}