"""
CLI Tool for Restaurant Data Hub
ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤
"""
import asyncio
import click
from loguru import logger

from src.database.connection import init_db
from src.workflows.scraping import ScrapingWorkflow
from src.workflows.sync import SyncWorkflow
from src.processors.gemini import GeminiProcessor
from src.database.connection import db_session
from src.database.models import ScrapingTarget
import uuid


@click.group()
def cli():
    """Restaurant Data Hub CLI"""
    pass


@cli.command()
def init():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    click.echo("Initializing database...")
    init_db()
    click.echo("âœ… Database initialized!")


@cli.command()
def scrape():
    """ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
    click.echo("Starting scraping workflow...")
    
    async def run():
        workflow = ScrapingWorkflow()
        await workflow.run_daily_scraping()
    
    asyncio.run(run())
    click.echo("âœ… Scraping completed!")


@cli.command()
def process():
    """ì›ë³¸ ë°ì´í„° ì²˜ë¦¬"""
    click.echo("Processing raw data with Gemini AI...")
    
    async def run():
        from src.database.models import RawRestaurantData, ProcessedRestaurant
        
        with db_session() as db:
            # pending ìƒíƒœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            raw_data = db.query(RawRestaurantData).filter(
                RawRestaurantData.status == 'pending'
            ).limit(100).all()
            
            click.echo(f"Found {len(raw_data)} pending records")
            
            if not raw_data:
                click.echo("No pending data to process")
                return
            
            gemini = GeminiProcessor()
            processed_count = 0
            
            for idx, raw in enumerate(raw_data):
                try:
                    # Rate Limit ë°©ì§€ë¥¼ ìœ„í•œ delay (ë¶„ë‹¹ 10ê°œ ì œí•œ â†’ 8ê°œ/60ì´ˆë¡œ ì•ˆì „í•˜ê²Œ)
                    if idx > 0 and idx % 8 == 0:
                        click.echo(f"  â†’ Rate limit protection: waiting 60 seconds... ({idx}/{len(raw_data)} processed)")
                        await asyncio.sleep(60)
                    
                    # Geminië¡œ ì •ì œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                    max_retries = 3
                    retry_count = 0
                    refined = None
                    
                    while retry_count < max_retries and refined is None:
                        try:
                            refined = await gemini.refine_restaurant_data(raw.raw_data)
                        except Exception as retry_error:
                            error_msg = str(retry_error)
                            if "429" in error_msg or "quota" in error_msg.lower():
                                retry_count += 1
                                if retry_count < max_retries:
                                    wait_time = 60 * retry_count
                                    click.echo(f"  âš ï¸ Rate limit hit! Retrying in {wait_time}s... (attempt {retry_count}/{max_retries})")
                                    await asyncio.sleep(wait_time)
                                else:
                                    click.echo(f"  âŒ Max retries exceeded for: {raw.source_id}")
                                    raise
                            else:
                                raise
                    
                    if refined is None:
                        raise Exception("Failed to refine data after retries")
                    
                    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    quality = await gemini.calculate_quality_score(raw.raw_data)
                    
                    # DBì— ì €ì¥
                    new_restaurant = ProcessedRestaurant(
                        id=str(uuid.uuid4()),
                        mapping_id=raw.id,
                        name=refined.get('name', ''),
                        name_en=refined.get('nameEn', ''),
                        category=refined.get('category', 'í•œì‹'),
                        cuisine=refined.get('cuisine', ''),
                        district=refined.get('district', ''),
                        address=refined.get('address', ''),
                        address_en=refined.get('addressEn', ''),
                        latitude=raw.raw_data.get('lat') or raw.raw_data.get('geometry', {}).get('location', {}).get('lat'),
                        longitude=raw.raw_data.get('lng') or raw.raw_data.get('geometry', {}).get('location', {}).get('lng'),
                        description=refined.get('description', ''),
                        description_en=refined.get('descriptionEn', ''),
                        price_range=str(refined.get('priceRange', 2)),
                        phone=refined.get('phone', ''),
                        rating=raw.raw_data.get('rating'),
                        review_count=raw.raw_data.get('reviewCount') or raw.raw_data.get('user_ratings_total', 0),
                        image_url=refined.get('imageUrl', 'https://via.placeholder.com/400x300?text=Restaurant'),
                        open_hours=refined.get('openHours'),
                        quality_score=quality.get('quality_score', 0),
                        quality_details=quality.get('quality_details', {}),
                        sync_status='pending'
                    )
                    db.add(new_restaurant)
                    
                    # raw ë°ì´í„° ìƒíƒœ ì—…ë°ì´íŠ¸
                    raw.status = 'processed'
                    
                    processed_count += 1
                    click.echo(f"âœ“ Processed: {refined.get('name', 'Unknown')}")
                    
                except Exception as e:
                    raw.status = 'failed'
                    raw.error_message = str(e)
                    logger.error(f"Failed to process {raw.id}: {e}")
                    click.echo(f"âœ— Failed: {raw.source_id}")
            
            db.commit()
            click.echo(f"\nâœ… Processed {processed_count}/{len(raw_data)} records")
    
    asyncio.run(run())
    click.echo("âœ… Processing completed!")


@cli.command()
def sync():
    """í•œì‹ë‹¹ ë™ê¸°í™”"""
    click.echo("Syncing to í•œì‹ë‹¹...")
    
    async def run():
        workflow = SyncWorkflow()
        await workflow.sync_to_hansikdang()
    
    asyncio.run(run())
    click.echo("âœ… Sync completed!")


@cli.command()
@click.argument('keyword')
@click.option('--region', default=None, help='ì§€ì—­ (ì˜ˆ: ê°•ë‚¨êµ¬)')
@click.option('--priority', default=5, help='ìš°ì„ ìˆœìœ„ (1-10)')
def add_target(keyword, region, priority):
    """ìŠ¤í¬ë˜í•‘ íƒ€ê²Ÿ ì¶”ê°€"""
    with db_session() as db:
        target = ScrapingTarget(
            id=str(uuid.uuid4()),
            keyword=keyword,
            region=region,
            priority=priority,
            status='active',
            created_by='cli'
        )
        db.add(target)
        db.commit()
    
    click.echo(f"âœ… Target added: {keyword} ({region})")


@cli.command()
@click.option('--region', default='ê°•ë‚¨êµ¬', help='ì§€ì—­')
@click.option('--count', default=50, help='ìƒì„±í•  í‚¤ì›Œë“œ ìˆ˜')
def generate_targets(region, count):
    """AIë¡œ íƒ€ê²Ÿ í‚¤ì›Œë“œ ìë™ ìƒì„±"""
    click.echo(f"Generating {count} target keywords for {region}...")
    
    async def run():
        gemini = GeminiProcessor()
        keywords = await gemini.generate_target_keywords(region, count)
        
        with db_session() as db:
            for keyword in keywords:
                target = ScrapingTarget(
                    id=str(uuid.uuid4()),
                    keyword=keyword,
                    region=region,
                    priority=5,
                    status='active',
                    created_by='ai'
                )
                db.add(target)
            db.commit()
        
        click.echo(f"âœ… Generated {len(keywords)} targets")
    
    asyncio.run(run())


@cli.command()
def full_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìŠ¤í¬ë˜í•‘ â†’ ì²˜ë¦¬ â†’ ë™ê¸°í™”)"""
    click.echo("Running full pipeline...")
    
    async def run():
        # 1. ìŠ¤í¬ë˜í•‘
        click.echo("Step 1/3: Scraping...")
        scrape_workflow = ScrapingWorkflow()
        await scrape_workflow.run_daily_scraping()
        
        # 2. ì²˜ë¦¬
        click.echo("Step 2/3: Processing...")
        await scrape_workflow.process_raw_data(batch_size=100)
        
        # 3. ë™ê¸°í™”
        click.echo("Step 3/3: Syncing...")
        sync_workflow = SyncWorkflow()
        await sync_workflow.sync_to_hansikdang()
    
    asyncio.run(run())
    click.echo("âœ… Full pipeline completed!")


@cli.command()
@click.argument('json_file')
@click.option('--source', default='google_places', help='ë°ì´í„° ì†ŒìŠ¤')
def import_json(json_file, source):
    """JSON íŒŒì¼ì—ì„œ ë ˆìŠ¤í† ë‘ ë°ì´í„° ì„í¬íŠ¸"""
    import json
    from src.database.models import RawRestaurantData
    
    click.echo(f"Importing from {json_file}...")
    
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    click.echo(f"Found {len(data)} restaurants")
    
    with db_session() as db:
        count = 0
        for item in data:
            try:
                # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
                location = item.get('location', {})
                lat = location.get('lat')
                lng = location.get('lng')
                
                # raw_dataì— lat/lng ì¶”ê°€
                raw_data = {**item, 'lat': lat, 'lng': lng}
                
                raw = RawRestaurantData(
                    id=str(uuid.uuid4()),
                    source=source,
                    source_id=item.get('placeId', item.get('id', str(uuid.uuid4()))),
                    raw_data=raw_data,
                    status='pending'
                )
                db.add(raw)
                count += 1
            except Exception as e:
                logger.error(f"Failed to import: {e}")
        
        db.commit()
        click.echo(f"âœ… Imported {count}/{len(data)} restaurants")


@cli.command()
@click.option('--query', default='í™ëŒ€ í•œì‹', help='ê²€ìƒ‰ ì¿¼ë¦¬')
@click.option('--limit', default=100, help='ìˆ˜ì§‘í•  ìµœëŒ€ ê°œìˆ˜')
def scrape_naver(query, limit):
    """ë„¤ì´ë²„ Maps APIë¡œ ë ˆìŠ¤í† ë‘ ë°ì´í„° ìˆ˜ì§‘"""
    click.echo(f"ğŸ” Naver Maps API: {query} (limit={limit})")
    
    async def run():
        from src.scrapers.naver_maps_api import NaverMapsScraper
        
        scraper = NaverMapsScraper()
        result = await scraper.scrape(query=query, limit=limit)
        
        click.echo(f"\nâœ… ë„¤ì´ë²„ Maps API ìˆ˜ì§‘ ì™„ë£Œ!")
        click.echo(f"  - ê²€ìƒ‰ ì¿¼ë¦¬: {result['query']}")
        click.echo(f"  - ë°œê²¬: {result['total_found']}ê°œ")
        click.echo(f"  - ì €ì¥: {result['saved_count']}ê°œ")
        click.echo(f"  - ì¤‘ë³µ: {result['duplicate_count']}ê°œ")
    
    asyncio.run(run())


@cli.command()
def stats():
    """ì „ì²´ í†µê³„ í™•ì¸"""
    from src.database.models import RawRestaurantData, ProcessedRestaurant
    
    with db_session() as db:
        total_raw = db.query(RawRestaurantData).count()
        total_processed = db.query(ProcessedRestaurant).count()
        total_synced = db.query(ProcessedRestaurant).filter(
            ProcessedRestaurant.synced_to_hansikdang == True
        ).count()
        
        raw_pending = db.query(RawRestaurantData).filter(
            RawRestaurantData.status == 'pending'
        ).count()
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        naver_count = db.query(RawRestaurantData).filter(
            RawRestaurantData.source == 'naver'
        ).count()
        google_count = db.query(RawRestaurantData).filter(
            RawRestaurantData.source == 'google_places'
        ).count()
        
        click.echo("\nğŸ“Š Data Hub í†µê³„")
        click.echo("=" * 50)
        click.echo(f"Total raw: {total_raw}")
        click.echo(f"  - Naver: {naver_count}")
        click.echo(f"  - Google: {google_count}")
        click.echo(f"Total processed: {total_processed}")
        click.echo(f"Total synced: {total_synced}")
        click.echo(f"Pending processing: {raw_pending}")
        click.echo(f"Daily target: 333")
        click.echo("=" * 50)


if __name__ == '__main__':
    cli()
