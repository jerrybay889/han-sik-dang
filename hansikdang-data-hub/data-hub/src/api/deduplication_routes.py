from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from typing import Optional
from loguru import logger

from src.database.connection import get_db
from src.deduplication.service import DeduplicationService

router = APIRouter(prefix="/api/duplicates", tags=["duplicates"])


@router.post("/detect")
async def detect_duplicates(
    auto_merge: bool = Query(False, description="ìë™ ë³‘í•© ì—¬ë¶€"),
    name_threshold: float = Query(90.0, ge=0, le=100, description="ì´ë¦„ ìœ ì‚¬ë„ ì„ê³„ê°’"),
    address_threshold: float = Query(85.0, ge=0, le=100, description="ì£¼ì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’"),
    distance_threshold: float = Query(100.0, ge=0, description="ê±°ë¦¬ ì„ê³„ê°’ (ë¯¸í„°)"),
    db: Session = Depends(get_db)
):
    """
    ë ˆìŠ¤í† ë‘ ì¤‘ë³µ íƒì§€ ë° ìë™ ë³‘í•©
    
    - **auto_merge**: Trueì‹œ ì¤‘ë³µ ìë™ ë³‘í•© (ê¸°ë³¸ê°’: False)
    - **name_threshold**: ì´ë¦„ ìœ ì‚¬ë„ ì„ê³„ê°’ (0-100, ê¸°ë³¸ê°’: 90)
    - **address_threshold**: ì£¼ì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0-100, ê¸°ë³¸ê°’: 85)
    - **distance_threshold**: GPS ê±°ë¦¬ ì„ê³„ê°’ (ë¯¸í„°, ê¸°ë³¸ê°’: 100)
    """
    try:
        logger.info(f"ğŸ” ì¤‘ë³µ íƒì§€ ìš”ì²­: auto_merge={auto_merge}")
        
        service = DeduplicationService(
            db=db,
            name_threshold=name_threshold,
            address_threshold=address_threshold,
            distance_threshold_meters=distance_threshold
        )
        
        result = service.detect_and_merge_duplicates(
            auto_merge=auto_merge,
            merge_type='auto' if auto_merge else 'manual'
        )
        
        return {
            "status": "success",
            "data": result,
            "message": f"ì¤‘ë³µ íƒì§€ ì™„ë£Œ: {result['duplicate_groups_found']}ê°œ ê·¸ë£¹ ë°œê²¬"
        }
        
    except Exception as e:
        logger.error(f"âŒ ì¤‘ë³µ íƒì§€ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/groups")
async def get_duplicate_groups(
    status: Optional[str] = Query(None, description="ìƒíƒœ í•„í„° (detected, merged, ignored)"),
    limit: int = Query(100, ge=1, le=1000, description="ì¡°íšŒ ê°œìˆ˜"),
    db: Session = Depends(get_db)
):
    """
    ì¤‘ë³µ ê·¸ë£¹ ëª©ë¡ ì¡°íšŒ
    
    - **status**: ìƒíƒœ í•„í„° (detected: íƒì§€ë¨, merged: ë³‘í•©ë¨, ignored: ë¬´ì‹œë¨)
    - **limit**: ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)
    """
    try:
        service = DeduplicationService(db=db)
        groups = service.get_duplicate_groups(status=status, limit=limit)
        
        result = []
        for group in groups:
            result.append({
                "id": group.id,
                "master_id": group.master_id,
                "duplicate_ids": group.duplicate_ids,
                "total_duplicates": len(group.duplicate_ids) if group.duplicate_ids else 0,
                "similarity_scores": group.similarity_scores,
                "detection_method": group.detection_method,
                "status": group.status,
                "created_at": group.created_at.isoformat() if group.created_at else None,
                "merged_at": group.merged_at.isoformat() if group.merged_at else None
            })
        
        return {
            "status": "success",
            "data": {
                "total": len(result),
                "groups": result
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ì¤‘ë³µ ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/history")
async def get_merge_history(
    limit: int = Query(100, ge=1, le=1000, description="ì¡°íšŒ ê°œìˆ˜"),
    merge_type: Optional[str] = Query(None, description="ë³‘í•© íƒ€ì… (auto, manual)"),
    db: Session = Depends(get_db)
):
    """
    ë ˆìŠ¤í† ë‘ ë³‘í•© ì´ë ¥ ì¡°íšŒ
    
    - **limit**: ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)
    - **merge_type**: ë³‘í•© íƒ€ì… (auto: ìë™, manual: ìˆ˜ë™)
    """
    try:
        service = DeduplicationService(db=db)
        history = service.get_merge_history(limit=limit, merge_type=merge_type)
        
        result = []
        for record in history:
            result.append({
                "id": record.id,
                "duplicate_group_id": record.duplicate_group_id,
                "master_id": record.master_id,
                "merged_ids": record.merged_ids,
                "total_merged": len(record.merged_ids) if record.merged_ids else 0,
                "merge_reason": record.merge_reason,
                "similarity_details": record.similarity_details,
                "merge_type": record.merge_type,
                "merged_by": record.merged_by,
                "merged_at": record.merged_at.isoformat() if record.merged_at else None
            })
        
        return {
            "status": "success",
            "data": {
                "total": len(result),
                "history": result
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ë³‘í•© ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stats")
async def get_deduplication_stats(db: Session = Depends(get_db)):
    """
    ì¤‘ë³µ ì œê±° í†µê³„ ì¡°íšŒ
    
    ì „ì²´ ì¤‘ë³µ ê·¸ë£¹, ë³‘í•© ì´ë ¥, ì²˜ë¦¬ëœ ë ˆìŠ¤í† ë‘ ìˆ˜ ë“±
    """
    try:
        service = DeduplicationService(db=db)
        stats = service.get_deduplication_stats()
        
        return {
            "status": "success",
            "data": stats
        }
        
    except Exception as e:
        logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
