"""
ìŠ¤ë§ˆíŠ¸ íƒ€ê²ŸíŒ… API ì—”ë“œí¬ì¸íŠ¸
Smart Targeting System API Routes
"""

from fastapi import APIRouter, HTTPException, Depends
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from loguru import logger
from sqlalchemy import desc

from ..targeting.trends_analyzer import TrendsAnalyzer
from ..targeting.query_generator import QueryGenerator
from ..targeting.popularity_scorer import PopularityScorer
from ..database.connection import db_session
from ..database.models import ScrapingTarget

router = APIRouter(prefix="/api/targeting", tags=["targeting"])

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
trends_analyzer = TrendsAnalyzer()
query_generator = QueryGenerator()
popularity_scorer = PopularityScorer()


@router.get("/popularity")
async def get_regional_popularity(days: int = 7):
    """
    ì§€ì—­ë³„ ì™¸êµ­ì¸ ì¸ê¸°ë„ ì ìˆ˜ ì¡°íšŒ
    
    Args:
        days: ë¶„ì„ ê¸°ê°„ (ì¼ ë‹¨ìœ„, ê¸°ë³¸: 7ì¼)
        
    Returns:
        {ì§€ì—­ëª…: ì ìˆ˜(0-100)}
    """
    try:
        logger.info(f"ğŸ“Š ì§€ì—­ë³„ ì¸ê¸°ë„ ì¡°íšŒ ìš”ì²­ (ê¸°ê°„: {days}ì¼)")
        
        scores = await trends_analyzer.get_regional_popularity(days=days)
        
        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        for region, score in scores.items():
            popularity_scorer.update_history(region, score)
        
        # íŠ¸ë Œë“œ ë°©í–¥ ì¶”ê°€
        result = {}
        for region, score in scores.items():
            result[region] = {
                "score": score,
                "trend": popularity_scorer.get_trend_direction(region, days),
                "historical_avg": popularity_scorer.get_historical_avg(region, days)
            }
        
        return {
            "status": "success",
            "data": result,
            "analyzed_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ì¸ê¸°ë„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/top-regions")
async def get_top_regions(count: int = 7, days: int = 7):
    """
    ìƒìœ„ Nê°œ ì¸ê¸° ì§€ì—­ ì¡°íšŒ
    
    Args:
        count: ë°˜í™˜í•  ì§€ì—­ ê°œìˆ˜ (ê¸°ë³¸: 7)
        days: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸: 7ì¼)
        
    Returns:
        ìƒìœ„ ì§€ì—­ ë¦¬ìŠ¤íŠ¸
    """
    try:
        logger.info(f"ğŸ† ìƒìœ„ {count}ê°œ ì§€ì—­ ì¡°íšŒ")
        
        top_regions = await trends_analyzer.get_top_regions(count=count, days=days)
        
        result = [
            {
                "rank": i + 1,
                "region": region,
                "score": score,
                "trend": popularity_scorer.get_trend_direction(region, days)
            }
            for i, (region, score) in enumerate(top_regions)
        ]
        
        return {
            "status": "success",
            "data": result,
            "total": len(result)
        }
        
    except Exception as e:
        logger.error(f"âŒ ìƒìœ„ ì§€ì—­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/regenerate")
async def regenerate_queries(target_count: int = 33, days: int = 7):
    """
    ë™ì  ì¿¼ë¦¬ ìˆ˜ë™ ì¬ìƒì„±
    
    Args:
        target_count: ìƒì„±í•  ì¿¼ë¦¬ ê°œìˆ˜ (ê¸°ë³¸: 33)
        days: ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸: 7ì¼)
        
    Returns:
        ìƒì„±ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    try:
        logger.info(f"ğŸ”„ ì¿¼ë¦¬ ìˆ˜ë™ ì¬ìƒì„± ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)")
        
        # 1. ìƒìœ„ ì§€ì—­ ë¶„ì„
        top_regions = await trends_analyzer.get_top_regions(count=7, days=days)
        
        # 2. ë™ì  ì¿¼ë¦¬ ìƒì„±
        queries = await query_generator.generate_daily_queries(top_regions, target_count)
        
        # 3. DBì— ì €ì¥
        with db_session() as db:
            # ê¸°ì¡´ ìë™ ìƒì„± ì¿¼ë¦¬ ì‚­ì œ
            db.query(ScrapingTarget).filter_by(created_by='auto').delete()
            
            # ìƒˆ ì¿¼ë¦¬ ì €ì¥
            for query in queries:
                target = ScrapingTarget(
                    id=f"auto_{datetime.now().strftime('%Y%m%d')}_{queries.index(query)}",
                    keyword=query,
                    region=query.split()[0] if query.split() else "",
                    priority=5,
                    status='active',
                    created_by='auto'
                )
                db.add(target)
            
            db.commit()
        
        # 4. ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
        diversity = query_generator.get_query_diversity_score(queries)
        
        logger.info(f"âœ… ì¿¼ë¦¬ ì¬ìƒì„± ì™„ë£Œ: {len(queries)}ê°œ")
        
        return {
            "status": "success",
            "data": {
                "queries": queries,
                "top_regions": [
                    {"region": r, "score": s} for r, s in top_regions
                ],
                "diversity": diversity,
                "generated_at": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ì¿¼ë¦¬ ì¬ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stats")
async def get_targeting_stats():
    """
    íƒ€ê²ŸíŒ… ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ
    
    Returns:
        ì „ì²´ í†µê³„
    """
    try:
        # ì¸ê¸°ë„ í†µê³„
        popularity_stats = popularity_scorer.get_all_stats()
        
        # DB ì¿¼ë¦¬ í†µê³„
        with db_session() as db:
            total_targets = db.query(ScrapingTarget).count()
            active_targets = db.query(ScrapingTarget).filter_by(status='active').count()
            auto_targets = db.query(ScrapingTarget).filter_by(created_by='auto').count()
        
        return {
            "status": "success",
            "data": {
                "popularity": popularity_stats,
                "targets": {
                    "total": total_targets,
                    "active": active_targets,
                    "auto_generated": auto_targets
                },
                "updated_at": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/queries/today")
async def get_today_queries():
    """
    ì˜¤ëŠ˜ì˜ ë™ì  ì¿¼ë¦¬ ì¡°íšŒ
    
    Returns:
        ì˜¤ëŠ˜ ìƒì„±ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    try:
        with db_session() as db:
            today = datetime.now().date()
            
            # ì˜¤ëŠ˜ ìƒì„±ëœ ìë™ ì¿¼ë¦¬ ì¡°íšŒ
            targets = db.query(ScrapingTarget).filter(
                ScrapingTarget.created_by == 'auto',
                ScrapingTarget.status == 'active'
            ).order_by(desc(ScrapingTarget.created_at)).limit(33).all()
            
            queries = [
                {
                    "keyword": t.keyword,
                    "region": t.region,
                    "priority": t.priority,
                    "created_at": t.created_at.isoformat() if t.created_at else None
                }
                for t in targets
            ]
            
            return {
                "status": "success",
                "data": {
                    "queries": queries,
                    "total": len(queries),
                    "date": today.isoformat()
                }
            }
            
    except Exception as e:
        logger.error(f"âŒ ì˜¤ëŠ˜ì˜ ì¿¼ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/queries/history")
async def get_query_history(days: int = 7):
    """
    ì¿¼ë¦¬ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    
    Args:
        days: ì¡°íšŒ ê¸°ê°„ (ì¼ ë‹¨ìœ„, ê¸°ë³¸: 7ì¼)
        
    Returns:
        íˆìŠ¤í† ë¦¬ ë°ì´í„°
    """
    try:
        with db_session() as db:
            cutoff = datetime.now() - timedelta(days=days)
            
            targets = db.query(ScrapingTarget).filter(
                ScrapingTarget.created_by == 'auto',
                ScrapingTarget.created_at >= cutoff
            ).order_by(desc(ScrapingTarget.created_at)).all()
            
            # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
            history_by_date = {}
            for target in targets:
                if target.created_at:
                    date_key = target.created_at.date().isoformat()
                    if date_key not in history_by_date:
                        history_by_date[date_key] = []
                    history_by_date[date_key].append({
                        "keyword": target.keyword,
                        "region": target.region
                    })
            
            return {
                "status": "success",
                "data": {
                    "history": history_by_date,
                    "period_days": days,
                    "total_dates": len(history_by_date)
                }
            }
            
    except Exception as e:
        logger.error(f"âŒ ì¿¼ë¦¬ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/health")
async def targeting_health_check():
    """íƒ€ê²ŸíŒ… ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬"""
    try:
        # Google Trends ì—°ê²° í…ŒìŠ¤íŠ¸
        test_scores = await trends_analyzer.get_regional_popularity(
            regions=["ê°•ë‚¨êµ¬"],
            days=1
        )
        
        trends_ok = "ê°•ë‚¨êµ¬" in test_scores
        
        return {
            "status": "healthy" if trends_ok else "degraded",
            "components": {
                "google_trends": "ok" if trends_ok else "error",
                "query_generator": "ok",
                "popularity_scorer": "ok"
            },
            "checked_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "checked_at": datetime.now().isoformat()
        }
