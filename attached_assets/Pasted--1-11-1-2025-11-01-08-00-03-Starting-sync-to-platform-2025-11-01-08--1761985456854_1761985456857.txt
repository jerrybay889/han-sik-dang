분석 결과 보고
1️⃣ 11월 1일 로그 확인 ✅
2025-11-01 08:00:03 | 🔄 Starting sync to 한식당 platform
2025-11-01 08:00:04 | No restaurants to sync
2025-11-01 08:00:04 | ✅ Sync completed: None

발견: 오늘 08:00에 동기화 작업이 정상 실행되었으나, 동기화할 레스토랑이 없었음 (이미 모두 동기화 완료)

2️⃣ 쿼리 설정 확인 ⚠️ 문제 발견!
scheduler.py (41-56번 줄)
search_queries = [
    "홍대 한식",
    "강남 한식당",
    "명동 한식",
    "여의도 맛집",
    "이태원 한식",
    "서울 삼계탕",
    "서울 불고기",
    "서울 비빔밥",
    "서울 갈비",
    "서울 냉면",
    "서울 찌개",
]
total_saved = 0
for query in search_queries[:3]:  # ⚠️ 처음 3개만 사용!
    result = await scraper.scrape(query=query, limit=11)
    total_saved += result['saved_count']
    logger.info(f"  ✓ {query}: {result['saved_count']} saved")
    
    if total_saved >= 33:
        break

🚨 핵심 문제
11개 쿼리 보유: "홍대 한식" ~ "서울 찌개"
실제 사용: [:3] 슬라이싱으로 처음 3개만 사용
"홍대 한식"
"강남 한식당"
"명동 한식"
나머지 8개 쿼리 미사용: 여의도, 이태원, 삼계탕, 불고기, 비빔밥, 갈비, 냉면, 찌개
3️⃣ 중복 필터 로직 확인 ✅ 정상 작동
naver_maps_api.py (158-167번 줄)
# 중복 체크 (PlaceID 또는 link 기준)
source_id = place_id if place_id else link if link else title
existing = db.query(RawRestaurantData).filter(
    RawRestaurantData.source == "naver",
    RawRestaurantData.source_id == source_id
).first()
if existing:
    self.logger.debug(f"Skipping duplicate: {title}")
    continue

작동 방식
source_id 생성: PlaceID → link → title 우선순위
DB 검색: 같은 source와 source_id 조합 확인
중복 시: 건너뛰고 로그에 기록
결론: ✅ 중복 필터가 정상 작동 중이며, 이미 수집된 레스토랑을 효과적으로 걸러냄

📊 10월 31일 수집 부족 원인 (2개/33개)
원인 분석
제한된 쿼리 사용: 3개 쿼리만 사용
각 쿼리당 11개 제한: limit=11
최대 가능 수집: 3 × 11 = 33개
실제 수집: 2개
왜 2개만 수집되었나?
"홍대 한식", "강남 한식당", "명동 한식" 검색
반환된 대부분의 레스토랑이 이미 DB에 존재 (중복)
중복 필터링 후 새로운 레스토랑 2개만 발견
💡 해결 방안
Option 1: 전체 쿼리 활용 (권장)
# 변경 전
for query in search_queries[:3]:
# 변경 후
for query in search_queries:  # 11개 전체 사용

예상 효과: 11 × 11 = 최대 121개 검색 (중복 제외 후 33개 달성 가능)
Option 2: 더 다양한 지역 추가
search_queries = [
    # 기존 11개 +
    "건대 한식", "잠실 한식", "신촌 한식", "종로 한식",
    "광화문 한식", "청담 한식", "압구정 한식", "역삼 한식",
    "삼성 한식", "판교 한식", "분당 한식", "수원 한식",
    # ... 더 많은 지역
]

Option 3: 검색 개수 증가
# 변경 전
result = await scraper.scrape(query=query, limit=11)
# 변경 후
result = await scraper.scrape(query=query, limit=50)  # 또는 100

🎯 권장 조치
즉시 수정이 필요한 부분:

✅ search_queries[:3] → search_queries (전체 사용)
✅ 혹은 더 다양한 지역 키워드 추가
✅ 필요 시 limit=11 → limit=30 증가
기대 효과:

일일 33개 목표 달성 가능
월 990개 수집 경로 확보